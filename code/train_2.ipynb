{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "import mxnet as mx\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import OffsetSplitter\n",
    "from gluonts.time_feature import TimeFeature\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.transform.sampler import ValidationSplitSampler\n",
    "\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import Timer \n",
    "\n",
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 수치 계산 관련\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import psutil\n",
    "\n",
    "# 성능 평가 메트릭\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# GluonTS 관련\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.mx import DeepAREstimator, MQRNNEstimator, TemporalFusionTransformerEstimator, DeepStateEstimator, DeepFactorEstimator, WaveNetEstimator, MQCNNEstimator, NBEATSEstimator, TransformerEstimator\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gluonts.mx.distribution import GaussianOutput\n",
    "from gluonts.mx import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.loader import TrainDataLoader, InferenceDataLoader\n",
    "\n",
    "import h5py\n",
    "import gc\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gluonts.transform import UniformSplitSampler, InstanceSplitter\n",
    "from gluonts.mx.batchify import batchify\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The mean prediction is not stored in the forecast data*\")\n",
    "\n",
    "np.random.seed(0)\n",
    "mx.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mLoading dataset for level 1\u001b[0m\n",
      "\u001b[93mTraining model for level 1\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 43/100 [00:02<00:02, 19.09it/s, epoch=1/1, avg_epoch_loss=3.63e+3]\n",
      "c:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:484: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'epoch_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:420\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[1;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[0;32m    416\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Learning rate is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m )\n\u001b[1;32m--> 420\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_no\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_batches_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m should_continue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_epoch_end(\n\u001b[0;32m    427\u001b[0m     epoch_no\u001b[38;5;241m=\u001b[39mepoch_no,\n\u001b[0;32m    428\u001b[0m     epoch_loss\u001b[38;5;241m=\u001b[39mloss_value(epoch_loss),\n\u001b[0;32m    429\u001b[0m     training_network\u001b[38;5;241m=\u001b[39mnet,\n\u001b[0;32m    430\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    431\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:275\u001b[0m, in \u001b[0;36mTrainer.__call__.<locals>.loop\u001b[1;34m(epoch_no, batch_iter, num_batches_to_use, is_training)\u001b[0m\n\u001b[0;32m    273\u001b[0m any_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_no, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(it, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    276\u001b[0m     any_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:420\u001b[0m, in \u001b[0;36mIterableSlice.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:111\u001b[0m, in \u001b[0;36mTransformedDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[DataEntry]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataset, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\dataset\\loader.py:50\u001b[0m, in \u001b[0;36mBatch.__call__\u001b[1;34m(self, data, is_train)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, is_train):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m batcher(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:128\u001b[0m, in \u001b[0;36mbatcher.<locals>.get_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m():\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:186\u001b[0m, in \u001b[0;36mFlatMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    185\u001b[0m num_idle_transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    187\u001b[0m     num_idle_transforms \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:85\u001b[0m, in \u001b[0;36mCyclic.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable:\n\u001b[0;32m     86\u001b[0m         at_least_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:111\u001b[0m, in \u001b[0;36mTransformedDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[DataEntry]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataset, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:134\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_entry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\feature.py:366\u001b[0m, in \u001b[0;36mAddTimeFeatures.map_transform\u001b[1;34m(self, data, is_train)\u001b[0m\n\u001b[0;32m    364\u001b[0m index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mperiod_range(start, periods\u001b[38;5;241m=\u001b[39mlength, freq\u001b[38;5;241m=\u001b[39mstart\u001b[38;5;241m.\u001b[39mfreq)\n\u001b[1;32m--> 366\u001b[0m data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_field] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 203\u001b[0m\n\u001b[0;32m    191\u001b[0m         metrics_df\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(level_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# 하이퍼파라미터 저장\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# with open(os.path.join(save_dir, 'model_params.json'), 'w') as f:\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m#     model_params = {\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;66;03m#     }\u001b[39;00m\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;66;03m#     json.dump(model_params, f, indent=4)\u001b[39;00m\n\u001b[1;32m--> 203\u001b[0m \u001b[43mtrain_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../result/test/prac\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[28], line 157\u001b[0m, in \u001b[0;36mtrain_models\u001b[1;34m(epochs, learning_rate, batch_size, save_dir)\u001b[0m\n\u001b[0;32m    146\u001b[0m Transformer \u001b[38;5;241m=\u001b[39m TransformerEstimator(\n\u001b[0;32m    147\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    148\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    153\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    154\u001b[0m )\n\u001b[0;32m    156\u001b[0m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[1;32m--> 157\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mMQRNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# 모델 예측\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\model\\estimator.py:239\u001b[0m, in \u001b[0;36mGluonEstimator.train\u001b[1;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m     training_data: Dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Predictor:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\model\\estimator.py:216\u001b[0m, in \u001b[0;36mGluonEstimator.train_model\u001b[1;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     copy_parameters(from_predictor\u001b[38;5;241m.\u001b[39mnetwork, training_network)\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mctx:\n\u001b[0;32m    223\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_predictor(transformation, training_network)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:493\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[1;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# save model and epoch info\u001b[39;00m\n\u001b[0;32m    489\u001b[0m bp \u001b[38;5;241m=\u001b[39m base_path()\n\u001b[0;32m    490\u001b[0m epoch_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-0000.params\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_no\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_no,\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_value(\u001b[43mepoch_loss\u001b[49m),\n\u001b[0;32m    494\u001b[0m }\n\u001b[0;32m    496\u001b[0m net\u001b[38;5;241m.\u001b[39msave_parameters(epoch_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    497\u001b[0m save_epoch_info(bp, epoch_info)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'epoch_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "def train_models(epochs, learning_rate, batch_size, save_dir='../result/test/'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    level_estimators = {}\n",
    "    level_preds = {}\n",
    "    level_metrics = {}\n",
    "\n",
    "    trainer = Trainer(         \n",
    "        epochs=epochs,\n",
    "        learning_rate=learning_rate,\n",
    "        num_batches_per_epoch=100\n",
    "    )\n",
    "    \n",
    "    for level_idx in range(1, 13):\n",
    "        level_dir = os.path.join(save_dir, f'level_{level_idx}')\n",
    "        os.makedirs(level_dir, exist_ok=True)\n",
    "\n",
    "        highlight_print(f\"Loading dataset for level {level_idx}\")\n",
    "        with open(os.path.join('../dataset/else', f'dataset_level_{level_idx}.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "            train_dataset = dataset['train']\n",
    "            test_dataset = dataset['test']\n",
    "\n",
    "        highlight_print(f\"Training model for level {level_idx}\")\n",
    "\n",
    "        if level_idx == 1:\n",
    "            DeepAR = DeepAREstimator(\n",
    "                freq=\"D\",\n",
    "                context_length=28,\n",
    "                prediction_length=28,\n",
    "                use_feat_dynamic_real=True,  \n",
    "                use_feat_static_cat=False,    \n",
    "                trainer=trainer,\n",
    "            )\n",
    "        else:\n",
    "            DeepAR = DeepAREstimator(\n",
    "                freq=\"D\",\n",
    "                context_length=28,\n",
    "                prediction_length=28,\n",
    "                use_feat_dynamic_real=True,  \n",
    "                use_feat_static_cat=True,    \n",
    "                cardinality=[len(train_dataset)],\n",
    "                trainer=trainer,\n",
    "            )\n",
    "        MQRNN = MQRNNEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,         \n",
    "            trainer=trainer,\n",
    "        )\n",
    "        TFT = TemporalFusionTransformerEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,\n",
    "            dynamic_feature_dims = {\n",
    "                \"sales_mean\": 1,\n",
    "                \"sales_std\": 1,\n",
    "                \"sales_max\": 1,\n",
    "                \"sales_min\": 1,\n",
    "                \"sales_lag1\": 1,\n",
    "                \"sales_lag7\": 1,\n",
    "                \"sales_lag28\": 1,\n",
    "                \"sales_rolling7_mean\": 1,\n",
    "                \"sales_rolling28_mean\": 1,\n",
    "                \"sales_trend\": 1,\n",
    "                \"release_mean\": 1,\n",
    "                \"out_of_stock_mean\": 1,\n",
    "                \"sell_price_mean\": 1,\n",
    "                \"sell_price_std\": 1,\n",
    "                \"sell_price_max\": 1,\n",
    "                \"sell_price_min\": 1,\n",
    "                \"sell_price_diff\": 1,\n",
    "                \"sell_price_trend\": 1,\n",
    "                \"sell_price_in_store_mean\": 1,\n",
    "                \"year_delta\": 1,\n",
    "                \"quarter_sin\": 1,\n",
    "                \"quarter_cos\": 1,\n",
    "                \"month_sin\": 1,\n",
    "                \"month_cos\": 1,\n",
    "                \"day_sin\": 1,\n",
    "                \"day_cos\": 1,\n",
    "                \"weekday_sin\": 1,\n",
    "                \"weekday_cos\": 1,\n",
    "                \"event_count\": 1,\n",
    "            },\n",
    "            dynamic_cardinalities={\n",
    "                \"snap_CA\":2,\n",
    "                \"snap_TX\":2,\n",
    "                \"snap_WI\":2,\n",
    "                \"event_name_1_enc\":31,\n",
    "                \"event_name_2_enc\":31,\n",
    "                \"event_type_1_enc\":5,\n",
    "                \"event_type_2_enc\":5\n",
    "            },\n",
    "            static_cardinalities={\"id\":len(train_dataset)},\n",
    "            trainer=trainer\n",
    "        )\n",
    "        if level_idx == 1:\n",
    "            DeepState = DeepStateEstimator(\n",
    "                freq=\"D\",\n",
    "                past_length=28,\n",
    "                prediction_length=28,\n",
    "                use_feat_dynamic_real=True,  \n",
    "                use_feat_static_cat=False,\n",
    "                cardinality=[len(train_dataset)],\n",
    "                trainer=trainer,\n",
    "            )\n",
    "        else:\n",
    "            DeepState = DeepStateEstimator(\n",
    "                freq=\"D\",\n",
    "                past_length=28,\n",
    "                prediction_length=28,\n",
    "                use_feat_dynamic_real=True,  \n",
    "                use_feat_static_cat=True,    \n",
    "                cardinality=[len(train_dataset)],\n",
    "                trainer=trainer,\n",
    "            )\n",
    "        DeepFactor = DeepFactorEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,   \n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        WaveNet = WaveNetEstimator(\n",
    "            freq=\"D\",\n",
    "            prediction_length=28,   \n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        MQCNN = MQCNNEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,   \n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=True,  \n",
    "            cardinality=[len(train_dataset)],     \n",
    "            trainer=trainer,\n",
    "        )\n",
    "        NBEATS = NBEATSEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,   \n",
    "            trainer=trainer,\n",
    "        )\n",
    "        Transformer = TransformerEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,   \n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=True,  \n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "        predictor = MQRNN.train(train_dataset)\n",
    "\n",
    "        highlight_print(f\"Saving model for level {level_idx}\")\n",
    "        predictor.serialize(Path(\"/tmp/\"))\n",
    "\n",
    "        highlight_print(f\"Making predictions for level {level_idx}\")\n",
    "        forecast_it, test_it = make_evaluation_predictions(\n",
    "            dataset=test_dataset,\n",
    "            predictor=predictor,\n",
    "        )\n",
    "        forecasts = list(forecast_it)\n",
    "        tests = list(test_it)\n",
    "\n",
    "        forecast_entry = forecasts[0]\n",
    "        print(f\"Number of sample paths: {forecast_entry.num_samples}\")\n",
    "        print(f\"Dimension of samples: {forecast_entry.samples.shape}\")\n",
    "        print(f\"Start date of the forecast window: {forecast_entry.start_date}\")\n",
    "        print(f\"Frequency of the time series: {forecast_entry.freq}\")\n",
    "        print(f\"Mean of the future window:\\n {forecast_entry.mean}\")\n",
    "        print(f\"0.5-quantile (median) of the future window:\\n {forecast_entry.quantile(0.5)}\")\n",
    "\n",
    "        # # 예측 결과\n",
    "        # level_pred = {}\n",
    "        # for forecast in forecasts:\n",
    "        #     id_preds = {quantile: forecast.quantile(quantile) for quantile in quantiles}\n",
    "        #     level_pred[forecast.item_id] = id_preds\n",
    "\n",
    "        # # 모델 평가\n",
    "        # evaluator = Evaluator(quantiles=quantiles)\n",
    "        # agg_metrics, item_metrics = evaluator(tests, forecasts)\n",
    "\n",
    "        # # 결과 저장\n",
    "        # level_estimators[level_idx+1] = estimator\n",
    "        # level_preds[level_idx+1] = level_pred\n",
    "        # level_metrics[level_idx+1] = agg_metrics\n",
    "\n",
    "        # # 예측 결과 저장\n",
    "        # with open(os.path.join(level_dir, 'predictions.pkl'), 'wb') as f:\n",
    "        #     pickle.dump(level_preds[level_idx], f)\n",
    "        \n",
    "        # # 평가 지표 저장\n",
    "        # metrics_df = pd.DataFrame(level_metrics[level_idx]).round(4)\n",
    "        # metrics_df.to_csv(os.path.join(level_dir, 'metrics.csv'))\n",
    "    \n",
    "    # 하이퍼파라미터 저장\n",
    "    # with open(os.path.join(save_dir, 'model_params.json'), 'w') as f:\n",
    "    #     model_params = {\n",
    "    #         'epochs': epochs,\n",
    "    #         'learning_rate': lr,\n",
    "    #         'batch_size': batch_size\n",
    "    #     }\n",
    "    #     json.dump(model_params, f, indent=4)\n",
    "\n",
    "\n",
    "train_models(epochs=1, learning_rate=1e-3, batch_size=128, save_dir='../result/test/prac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "\n",
    "import mxnet as mx\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.dataset.pandas import PandasDataset\n",
    "from gluonts.dataset.split import OffsetSplitter\n",
    "from gluonts.time_feature import TimeFeature\n",
    "from gluonts.torch.model.tft import TemporalFusionTransformerEstimator\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gluonts.evaluation import make_evaluation_predictions, Evaluator\n",
    "from gluonts.transform.sampler import ValidationSplitSampler\n",
    "\n",
    "import torch\n",
    "from lightning.pytorch.callbacks import Timer \n",
    "\n",
    "# 기본 라이브러리\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 수치 계산 관련\n",
    "from scipy import linalg\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import psutil\n",
    "\n",
    "# 성능 평가 메트릭\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "# GluonTS 관련\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.mx import DeepAREstimator, MQRNNEstimator, TemporalFusionTransformerEstimator, DeepStateEstimator, DeepFactorEstimator, WaveNetEstimator, MQCNNEstimator, NBEATSEstimator, TransformerEstimator\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gluonts.mx.distribution import GaussianOutput\n",
    "from gluonts.mx import Trainer\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.loader import TrainDataLoader, InferenceDataLoader\n",
    "\n",
    "import h5py\n",
    "import gc\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from gluonts.transform import UniformSplitSampler, InstanceSplitter\n",
    "from gluonts.mx.batchify import batchify\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"The mean prediction is not stored in the forecast data*\")\n",
    "\n",
    "np.random.seed(0)\n",
    "mx.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mLoading dataset for level 10\u001b[0m\n",
      "\u001b[93mTraining model for level 10\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]WARNING:gluonts.trainer:Batch [1] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [2] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [3] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [4] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [5] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [6] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [7] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [8] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [9] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [10] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [11] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [12] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [13] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [14] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [15] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [16] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [17] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [18] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [19] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [20] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [21] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [22] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [23] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [24] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [25] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [26] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [27] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [28] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [29] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [30] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [31] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [32] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [33] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [34] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [35] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [36] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [37] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [38] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [39] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [40] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [41] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [42] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [43] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [44] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [45] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [46] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [47] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [48] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [49] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [50] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [51] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [52] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [53] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [54] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [55] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [56] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [57] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [58] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [59] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [60] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [61] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [62] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [63] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [64] of Epoch[0] gave NaN loss and it will be ignored\n",
      "WARNING:gluonts.trainer:Batch [65] of Epoch[0] gave NaN loss and it will be ignored\n",
      " 65%|██████▌   | 65/100 [00:06<00:03,  9.45it/s, epoch=1/3, avg_epoch_loss=nan]\n",
      "c:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:484: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'epoch_loss' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:420\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[1;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[0;32m    416\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    417\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_no\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] Learning rate is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurr_lr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    418\u001b[0m )\n\u001b[1;32m--> 420\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[43mloop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepoch_no\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_batches_to_use\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_batches_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m should_continue \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_train_epoch_end(\n\u001b[0;32m    427\u001b[0m     epoch_no\u001b[38;5;241m=\u001b[39mepoch_no,\n\u001b[0;32m    428\u001b[0m     epoch_loss\u001b[38;5;241m=\u001b[39mloss_value(epoch_loss),\n\u001b[0;32m    429\u001b[0m     training_network\u001b[38;5;241m=\u001b[39mnet,\n\u001b[0;32m    430\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    431\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:275\u001b[0m, in \u001b[0;36mTrainer.__call__.<locals>.loop\u001b[1;34m(epoch_no, batch_iter, num_batches_to_use, is_training)\u001b[0m\n\u001b[0;32m    273\u001b[0m any_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m--> 275\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_no, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(it, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m    276\u001b[0m     any_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:420\u001b[0m, in \u001b[0;36mIterableSlice.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 420\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m itertools\u001b[38;5;241m.\u001b[39mislice(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlength)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:111\u001b[0m, in \u001b[0;36mTransformedDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[DataEntry]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataset, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\dataset\\loader.py:50\u001b[0m, in \u001b[0;36mBatch.__call__\u001b[1;34m(self, data, is_train)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data, is_train):\n\u001b[1;32m---> 50\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m batcher(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:128\u001b[0m, in \u001b[0;36mbatcher.<locals>.get_batch\u001b[1;34m()\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_batch\u001b[39m():\n\u001b[1;32m--> 128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mislice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:186\u001b[0m, in \u001b[0;36mFlatMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    185\u001b[0m num_idle_transforms \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 186\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    187\u001b[0m     num_idle_transforms \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\itertools.py:85\u001b[0m, in \u001b[0;36mCyclic.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterable:\n\u001b[0;32m     86\u001b[0m         at_least_one \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:111\u001b[0m, in \u001b[0;36mTransformedDataset.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[DataEntry]:\n\u001b[1;32m--> 111\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformation(\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_dataset, is_train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train\n\u001b[0;32m    113\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:132\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    130\u001b[0m     \u001b[38;5;28mself\u001b[39m, data_it: Iterable[DataEntry], is_train: \u001b[38;5;28mbool\u001b[39m\n\u001b[0;32m    131\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator:\n\u001b[1;32m--> 132\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data_entry \u001b[38;5;129;01min\u001b[39;00m data_it:\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\_base.py:134\u001b[0m, in \u001b[0;36mMapTransformation.__call__\u001b[1;34m(self, data_it, is_train)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_entry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\transform\\feature.py:366\u001b[0m, in \u001b[0;36mAddTimeFeatures.map_transform\u001b[1;34m(self, data, is_train)\u001b[0m\n\u001b[0;32m    364\u001b[0m index \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mperiod_range(start, periods\u001b[38;5;241m=\u001b[39mlength, freq\u001b[38;5;241m=\u001b[39mstart\u001b[38;5;241m.\u001b[39mfreq)\n\u001b[1;32m--> 366\u001b[0m data[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_field] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdate_features\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\numpy\\core\\shape_base.py:296\u001b[0m, in \u001b[0;36mvstack\u001b[1;34m(tup, dtype, casting)\u001b[0m\n\u001b[0;32m    295\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m [arrs]\n\u001b[1;32m--> 296\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 160\u001b[0m\n\u001b[0;32m    144\u001b[0m NBEATS \u001b[38;5;241m=\u001b[39m NBEATSEstimator(\n\u001b[0;32m    145\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    146\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m,\n\u001b[0;32m    147\u001b[0m     prediction_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m,   \n\u001b[0;32m    148\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    149\u001b[0m )\n\u001b[0;32m    150\u001b[0m Transformer \u001b[38;5;241m=\u001b[39m TransformerEstimator(\n\u001b[0;32m    151\u001b[0m     freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    152\u001b[0m     context_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m28\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    157\u001b[0m     trainer\u001b[38;5;241m=\u001b[39mtrainer,\n\u001b[0;32m    158\u001b[0m )\n\u001b[1;32m--> 160\u001b[0m predictor \u001b[38;5;241m=\u001b[39m \u001b[43mDeepAR\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    162\u001b[0m highlight_print(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking predictions for level \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlevel_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    163\u001b[0m forecast_it, test_it \u001b[38;5;241m=\u001b[39m make_evaluation_predictions(\n\u001b[0;32m    164\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mtest_dataset,\n\u001b[0;32m    165\u001b[0m     predictor\u001b[38;5;241m=\u001b[39mpredictor,\n\u001b[0;32m    166\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\model\\estimator.py:239\u001b[0m, in \u001b[0;36mGluonEstimator.train\u001b[1;34m(self, training_data, validation_data, shuffle_buffer_length, cache_data, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[0;32m    232\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    233\u001b[0m     training_data: Dataset,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    238\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Predictor:\n\u001b[1;32m--> 239\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle_buffer_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredictor\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\model\\estimator.py:216\u001b[0m, in \u001b[0;36mGluonEstimator.train_model\u001b[1;34m(self, training_data, validation_data, from_predictor, shuffle_buffer_length, cache_data)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    214\u001b[0m     copy_parameters(from_predictor\u001b[38;5;241m.\u001b[39mnetwork, training_network)\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnet\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_network\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mctx:\n\u001b[0;32m    223\u001b[0m     predictor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_predictor(transformation, training_network)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\gluonts\\mx\\trainer\\_base.py:493\u001b[0m, in \u001b[0;36mTrainer.__call__\u001b[1;34m(self, net, train_iter, validation_iter)\u001b[0m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;66;03m# save model and epoch info\u001b[39;00m\n\u001b[0;32m    489\u001b[0m bp \u001b[38;5;241m=\u001b[39m base_path()\n\u001b[0;32m    490\u001b[0m epoch_info \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    491\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams_path\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-0000.params\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    492\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_no\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch_no,\n\u001b[1;32m--> 493\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_value(\u001b[43mepoch_loss\u001b[49m),\n\u001b[0;32m    494\u001b[0m }\n\u001b[0;32m    496\u001b[0m net\u001b[38;5;241m.\u001b[39msave_parameters(epoch_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparams_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    497\u001b[0m save_epoch_info(bp, epoch_info)\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'epoch_loss' referenced before assignment"
     ]
    }
   ],
   "source": [
    "epochs=3\n",
    "learning_rate=1e-4\n",
    "batch_size=128\n",
    "save_dir='../result/test/prac'\n",
    "\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "level_estimators = {}\n",
    "level_preds = {}\n",
    "level_metrics = {}\n",
    "\n",
    "trainer = Trainer(         \n",
    "    epochs=epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    num_batches_per_epoch=100,\n",
    "    clip_gradient=10.0 \n",
    ")\n",
    "\n",
    "for level_idx in range(10, 13):\n",
    "    level_dir = os.path.join(save_dir, f'level_{level_idx}')\n",
    "    os.makedirs(level_dir, exist_ok=True)\n",
    "\n",
    "    highlight_print(f\"Loading dataset for level {level_idx}\")\n",
    "    with open(os.path.join('../dataset/else', f'dataset_level_{level_idx}.pkl'), 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "        train_dataset = dataset['train']\n",
    "        test_dataset = dataset['test']\n",
    "\n",
    "    highlight_print(f\"Training model for level {level_idx}\")\n",
    "    if level_idx == 1:\n",
    "        DeepAR = DeepAREstimator( # 10 x\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,\n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=False,    \n",
    "            trainer=trainer,\n",
    "        )\n",
    "    else:\n",
    "        DeepAR = DeepAREstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,\n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=True,    \n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "    MQRNN = MQRNNEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,         \n",
    "        trainer=trainer,\n",
    "    )\n",
    "    TFT = TemporalFusionTransformerEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,\n",
    "        dynamic_feature_dims = {\n",
    "            \"sales_mean\": 1,\n",
    "            \"sales_std\": 1,\n",
    "            \"sales_max\": 1,\n",
    "            \"sales_min\": 1,\n",
    "            \"sales_lag1\": 1,\n",
    "            \"sales_lag7\": 1,\n",
    "            \"sales_lag28\": 1,\n",
    "            \"sales_rolling7_mean\": 1,\n",
    "            \"sales_rolling28_mean\": 1,\n",
    "            \"sales_trend\": 1,\n",
    "            \"release_mean\": 1,\n",
    "            \"out_of_stock_mean\": 1,\n",
    "            \"sell_price_mean\": 1,\n",
    "            \"sell_price_std\": 1,\n",
    "            \"sell_price_max\": 1,\n",
    "            \"sell_price_min\": 1,\n",
    "            \"sell_price_diff\": 1,\n",
    "            \"sell_price_trend\": 1,\n",
    "            \"sell_price_in_store_mean\": 1,\n",
    "            \"year_delta\": 1,\n",
    "            \"quarter_sin\": 1,\n",
    "            \"quarter_cos\": 1,\n",
    "            \"month_sin\": 1,\n",
    "            \"month_cos\": 1,\n",
    "            \"day_sin\": 1,\n",
    "            \"day_cos\": 1,\n",
    "            \"weekday_sin\": 1,\n",
    "            \"weekday_cos\": 1,\n",
    "            \"event_count\": 1,\n",
    "        },\n",
    "        dynamic_cardinalities={\n",
    "            \"snap_CA\":2,\n",
    "            \"snap_TX\":2,\n",
    "            \"snap_WI\":2,\n",
    "            \"event_name_1_enc\":31,\n",
    "            \"event_name_2_enc\":31,\n",
    "            \"event_type_1_enc\":5,\n",
    "            \"event_type_2_enc\":5\n",
    "        },\n",
    "        static_cardinalities={\"id\":len(train_dataset)},\n",
    "        trainer=trainer\n",
    "    )\n",
    "    if level_idx == 1:\n",
    "        DeepState = DeepStateEstimator(\n",
    "            freq=\"D\",\n",
    "            past_length=28,\n",
    "            prediction_length=28,\n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=False,\n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "    else:\n",
    "        DeepState = DeepStateEstimator(\n",
    "            freq=\"D\",\n",
    "            past_length=28,\n",
    "            prediction_length=28,\n",
    "            use_feat_dynamic_real=True,  \n",
    "            use_feat_static_cat=True,    \n",
    "            cardinality=[len(train_dataset)],\n",
    "            trainer=trainer,\n",
    "        )\n",
    "    DeepFactor = DeepFactorEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,   \n",
    "        cardinality=[len(train_dataset)],\n",
    "        trainer=trainer,\n",
    "    )\n",
    "    WaveNet = WaveNetEstimator(\n",
    "        freq=\"D\",\n",
    "        prediction_length=28,   \n",
    "        cardinality=[len(train_dataset)],\n",
    "        trainer=trainer,\n",
    "    )\n",
    "    MQCNN = MQCNNEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,   \n",
    "        use_feat_dynamic_real=True,  \n",
    "        use_feat_static_cat=True,  \n",
    "        cardinality=[len(train_dataset)],     \n",
    "        trainer=trainer,\n",
    "    )\n",
    "    NBEATS = NBEATSEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,   \n",
    "        trainer=trainer,\n",
    "    )\n",
    "    Transformer = TransformerEstimator(\n",
    "        freq=\"D\",\n",
    "        context_length=28,\n",
    "        prediction_length=28,   \n",
    "        use_feat_dynamic_real=True,  \n",
    "        use_feat_static_cat=True,  \n",
    "        cardinality=[len(train_dataset)],\n",
    "        trainer=trainer,\n",
    "    )\n",
    "\n",
    "    predictor = DeepAR.train(train_dataset)\n",
    "\n",
    "    highlight_print(f\"Making predictions for level {level_idx}\")\n",
    "    forecast_it, test_it = make_evaluation_predictions(\n",
    "        dataset=test_dataset,\n",
    "        predictor=predictor,\n",
    "    )\n",
    "    forecasts = list(forecast_it)\n",
    "    tests = list(test_it)\n",
    "\n",
    "    forecast_entry = forecasts[0]\n",
    "    test_entry = tests[0]\n",
    "    \n",
    "    for idx in range(len(forecasts)):\n",
    "        if idx == 3:\n",
    "            break\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(tests[idx][-100:].to_timestamp(), label=\"Actual\")\n",
    "        plt.plot(pd.Series(forecasts[idx].quantile(0.5), index=forecasts[idx].start_date.to_timestamp() + pd.to_timedelta(range(len(forecasts[idx].quantile(0.5))), unit='D')), label=\"Forecast\")\n",
    "        plt.title(f\"Forecast for Time Series {idx + 1}\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_datasets(save_dir='../dataset/'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    levels = [\n",
    "        [],                        # Level 1: Total\n",
    "        ['state_id'],              # Level 2: State\n",
    "        ['store_id'],              # Level 3: Store\n",
    "        ['cat_id'],                # Level 4: Category\n",
    "        ['dept_id'],               # Level 5: Department\n",
    "        ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "        ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "        ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "        ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "        ['item_id'],               # Level 10: Item\n",
    "        ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "        ['item_id', 'store_id']    # Level 12: Individual\n",
    "    ]\n",
    "\n",
    "    for level_idx, level in enumerate(levels[11:], start=11):\n",
    "        datasets = {'train': {}, 'test': {}}\n",
    "\n",
    "        highlight_print(f\"Preparing dataset for level {level_idx+1}\")\n",
    "\n",
    "        # 데이터 로드\n",
    "        agg_df = pd.read_csv(f'../data/preprocessed/agg_df_level_{level_idx+1}.csv')\n",
    "        calendar_df = pd.read_csv('../data/preprocessed/calendar_df.csv')\n",
    "\n",
    "        # 메모리 절약\n",
    "        agg_df = save_memory(agg_df)\n",
    "        calendar_df = save_memory(calendar_df)\n",
    "\n",
    "        # 날짜 처리\n",
    "        start_date = pd.to_datetime('2011-01-29')\n",
    "        agg_df['d'] = agg_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        agg_df['d'] = start_date + pd.to_timedelta(agg_df['d'], unit='D')\n",
    "        calendar_df['d'] = calendar_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        calendar_df['d'] = start_date + pd.to_timedelta(calendar_df['d'], unit='D')\n",
    "        \n",
    "        # ID 컬럼 생성\n",
    "        if len(level) == 0:\n",
    "            agg_df.insert(1, 'id', 'total')\n",
    "        elif len(level) == 1:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]])\n",
    "        elif len(level) > 1:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]] + '_' + agg_df[level[1]])\n",
    "        \n",
    "        # 그룹 생성\n",
    "        groups = agg_df['id'].unique()\n",
    "        group_encoder = {group: group_idx for group_idx, group in enumerate(groups)}\n",
    "        \n",
    "        # GluonTS dataset 생성\n",
    "        train_data = []\n",
    "        test_data = []\n",
    "        for idx,group in enumerate(groups):\n",
    "            print(f'{idx}/{len(groups)}')\n",
    "            group_df = agg_df[agg_df['id'] == group] # 1941 rows\n",
    "            group_df = group_df.merge(calendar_df, on=\"d\", how=\"left\")\n",
    "            group_df = save_memory(group_df)\n",
    "\n",
    "            train_data.append({\n",
    "                FieldName.ITEM_ID: str(group),\n",
    "                FieldName.TARGET: group_df['sales_sum'].values[:1913],\n",
    "                FieldName.START: start_date,\n",
    "                FieldName.FEAT_STATIC_CAT: [group_encoder[group]],\n",
    "                FieldName.FEAT_DYNAMIC_REAL: group_df[[\n",
    "                    \"sales_mean\", \"sales_std\", \"sales_max\", \"sales_min\", \n",
    "                    \"sales_lag1\", \"sales_lag7\", \"sales_lag28\",  \n",
    "                    \"sales_rolling7_mean\", \"sales_rolling28_mean\", \n",
    "                    \"sales_trend\", \n",
    "                    \"release_mean\", \"out_of_stock_mean\", \n",
    "                    \"sell_price_mean\", \"sell_price_std\", \"sell_price_max\", \"sell_price_min\", \n",
    "                    \"sell_price_diff\", \"sell_price_trend\", \"sell_price_in_store_mean\",\n",
    "                    \"snap_CA\", \"snap_TX\", \"snap_WI\",  \n",
    "                    \"year_delta\", \"quarter_sin\", \"quarter_cos\", \"month_sin\", \"month_cos\",  \n",
    "                    \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n",
    "                    'event_count'\n",
    "                ]].values[:1913].T,\n",
    "                FieldName.FEAT_DYNAMIC_CAT: group_df[[\n",
    "                    'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                    'event_name_1_enc', 'event_name_2_enc', 'event_type_1_enc', 'event_type_2_enc'\n",
    "                ]].values[:1913].T,\n",
    "            })\n",
    "\n",
    "            test_data.append({\n",
    "                FieldName.ITEM_ID: str(group),\n",
    "                FieldName.TARGET: group_df['sales_sum'].values,\n",
    "                FieldName.START: start_date,\n",
    "                FieldName.FEAT_STATIC_CAT: [group_encoder[group]],\n",
    "                FieldName.FEAT_DYNAMIC_REAL: group_df[[\n",
    "                    \"sales_mean\", \"sales_std\", \"sales_max\", \"sales_min\", \n",
    "                    \"sales_lag1\", \"sales_lag7\", \"sales_lag28\",  \n",
    "                    \"sales_rolling7_mean\", \"sales_rolling28_mean\", \n",
    "                    \"sales_trend\", \n",
    "                    \"release_mean\", \"out_of_stock_mean\", \n",
    "                    \"sell_price_mean\", \"sell_price_std\", \"sell_price_max\", \"sell_price_min\", \n",
    "                    \"sell_price_diff\", \"sell_price_trend\", \"sell_price_in_store_mean\",\n",
    "                    \"snap_CA\", \"snap_TX\", \"snap_WI\",  \n",
    "                    \"year_delta\", \"quarter_sin\", \"quarter_cos\", \"month_sin\", \"month_cos\",  \n",
    "                    \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n",
    "                    'event_count'\n",
    "                ]].values.T,\n",
    "                FieldName.FEAT_DYNAMIC_CAT: group_df[[\n",
    "                    'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                    'event_name_1_enc', 'event_name_2_enc', \n",
    "                    'event_type_1_enc', 'event_type_2_enc'\n",
    "                ]].values.T,\n",
    "            })\n",
    "\n",
    "        train_dataset = ListDataset(train_data, freq=\"D\")\n",
    "        test_dataset = ListDataset(test_data, freq=\"D\")\n",
    "        \n",
    "        datasets['train'] = train_dataset\n",
    "        datasets['test'] = test_dataset\n",
    "    \n",
    "        with open(os.path.join(save_dir, f'dataset_level_{level_idx+1}.pkl'), 'wb') as f:\n",
    "            pickle.dump(datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(epochs, lr, batch_size, quantiles=[0.005, 0.025, 0.165, 0.25, 0.5, 0.75, 0.835, 0.975, 0.995], save_dir='../result/test/'):\n",
    "    # 저장 경로\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # Train/Test 데이터셋 준비\n",
    "    datasets = {}\n",
    "    if not os.path.exists('../dataset/') or len(os.listdir('../dataset/')) != 12:\n",
    "        prepare_datasets('../dataset/')\n",
    "    for level_idx in range(1, 13): \n",
    "        highlight_print(f\"Loading dataset for level {level_idx+1}\")\n",
    "        with open(os.path.join('../dataset/', f'dataset_level_{level_idx+1}.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "            datasets['train'][level_idx+1] = dataset['train']\n",
    "            datasets['test'][level_idx+1] = dataset['test']\n",
    "    \n",
    "    # 각 레벨별로 TFT 모델 학습\n",
    "    level_estimators = {}\n",
    "    level_preds = {}\n",
    "    level_metrics = {}\n",
    "    \n",
    "    for level_idx in range(1, 13):\n",
    "        highlight_print(f\"Training model for level {level_idx+1}\")\n",
    "\n",
    "        # 저장 경로\n",
    "        level_dir = os.path.join(save_dir, f'level_{level_idx}')\n",
    "        os.makedirs(level_dir, exist_ok=True)\n",
    "\n",
    "        # 데이터셋 로드\n",
    "        train_dataset = datasets['train'][level_idx+1]\n",
    "        test_dataset = datasets['test'][level_idx+1]\n",
    "\n",
    "        # 모델 선언\n",
    "        estimator = TemporalFusionTransformerEstimator(\n",
    "            freq=\"D\",\n",
    "            context_length=28,\n",
    "            prediction_length=28,\n",
    "            quantiles=quantiles,\n",
    "\n",
    "            hidden_dim=64,\n",
    "            num_heads=4,\n",
    "            dropout_rate=0.1,\n",
    "\n",
    "            static_cardinalities=[len(groups)],\n",
    "            dynamic_dims=[32],\n",
    "            dynamic_cardinalities=[2, 2, 2, 31, 31, 5, 5],\n",
    "\n",
    "            trainer_kwargs={\n",
    "                \"max_epochs\": epochs,\n",
    "                \"accelerator\": \"auto\",\n",
    "                'callbacks': [\n",
    "                    EarlyStopping(\n",
    "                        monitor='val_loss', \n",
    "                        patience=5\n",
    "                    ),\n",
    "                    # 모델 저장\n",
    "                    ModelCheckpoint(\n",
    "                        dirpath=level_dir,\n",
    "                        filename='checkpoint-{epoch:02d}-{val_loss:.4f}',\n",
    "                        monitor='val_loss',\n",
    "                        mode='min',\n",
    "                        save_top_k=1,\n",
    "                        save_last=False \n",
    "                    ),\n",
    "                ]\n",
    "            },\n",
    "            lr=lr,\n",
    "            batch_size=batch_size,\n",
    "\n",
    "            validation_sampler = ValidationSplitSampler(min_future=28)\n",
    "        )\n",
    "        \n",
    "        # 모델 학습\n",
    "        predictor = estimator.train(train_dataset)\n",
    "\n",
    "        # 모델 예측\n",
    "        forecast_it, test_it = make_evaluation_predictions(\n",
    "            dataset=test_dataset,\n",
    "            predictor=predictor,\n",
    "        )\n",
    "        forecasts = list(forecast_it)\n",
    "        tests = list(test_it)\n",
    "\n",
    "        # 예측 결과\n",
    "        level_pred = {}\n",
    "        for forecast in forecasts:\n",
    "            id_preds = {quantile: forecast.quantile(quantile) for quantile in quantiles}\n",
    "            level_pred[forecast.item_id] = id_preds\n",
    "\n",
    "        # 모델 평가\n",
    "        evaluator = Evaluator(quantiles=quantiles)\n",
    "        agg_metrics, item_metrics = evaluator(tests, forecasts)\n",
    "\n",
    "        # 결과 저장\n",
    "        level_estimators[level_idx+1] = estimator\n",
    "        level_preds[level_idx+1] = level_pred\n",
    "        level_metrics[level_idx+1] = agg_metrics\n",
    "\n",
    "        # 예측 결과 저장\n",
    "        with open(os.path.join(level_dir, 'predictions.pkl'), 'wb') as f:\n",
    "            pickle.dump(level_preds[level_idx], f)\n",
    "        \n",
    "        # 평가 지표 저장\n",
    "        metrics_df = pd.DataFrame(level_metrics[level_idx]).round(4)\n",
    "        metrics_df.to_csv(os.path.join(level_dir, 'metrics.csv'))\n",
    "    \n",
    "    # 하이퍼파라미터 저장\n",
    "    with open(os.path.join(save_dir, 'model_params.json'), 'w') as f:\n",
    "        model_params = {\n",
    "            'epochs': epochs,\n",
    "            'learning_rate': lr,\n",
    "            'batch_size': batch_size\n",
    "        }\n",
    "        json.dump(model_params, f, indent=4)\n",
    "\n",
    "\n",
    "train_models(epochs=1, lr=1e-1, batch_size=128, save_dir='../result/test/prac')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InterpretableTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, fundamental_period, n_channels, n_auxiliary_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fundamental_period (int): 기본 주기 (예: 7일)\n",
    "            n_channels (int): 컨볼루션 출력 채널 수\n",
    "            n_auxiliary_features (int): 보조 특성 수 (주말여부, 이벤트, 가격변화 등)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.fundamental_period = fundamental_period\n",
    "        \n",
    "        # 단일 주기를 사용한 1D 컨볼루션\n",
    "        # Input: (batch_size, 1, sequence_length)\n",
    "        # Output: (batch_size, n_channels, sequence_length)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1, \n",
    "            out_channels=n_channels, \n",
    "            kernel_size=fundamental_period,\n",
    "            stride=1,  # 모든 시점의 패턴을 분석하기 위해 stride=1 사용\n",
    "            padding=fundamental_period-1  # 시퀀스 앞부분의 패턴도 캡쳐하기 위한 패딩\n",
    "        )\n",
    "        \n",
    "        # 컨볼루션 가중치 해석을 위한 어텐션\n",
    "        # Input: (batch_size, n_channels * sequence_length)\n",
    "        # Output: (batch_size, n_channels)\n",
    "        self.conv_attention = nn.Sequential(\n",
    "            nn.Linear(n_channels * fundamental_period, n_channels),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # 특성 중요도 계산 레이어\n",
    "        # Input: (batch_size, n_channels + n_auxiliary_features)\n",
    "        # Output: (batch_size, n_channels + n_auxiliary_features)\n",
    "        self.feature_importance = nn.Sequential(\n",
    "            nn.Linear(n_channels + n_auxiliary_features, \n",
    "                     n_channels + n_auxiliary_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 최종 예측 레이어\n",
    "        # Input: (batch_size, n_channels + n_auxiliary_features)\n",
    "        # Output: (batch_size, 1)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(n_channels + n_auxiliary_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, sales, period_strength, auxiliary_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sales (torch.Tensor): (batch_size, sequence_length)\n",
    "            period_strength (torch.Tensor): (batch_size, 1) 주기성 강도\n",
    "            auxiliary_features (torch.Tensor): (batch_size, n_auxiliary_features)\n",
    "            \n",
    "        Returns:\n",
    "            prediction (torch.Tensor): (batch_size, 1)\n",
    "            conv_weights (torch.Tensor): (batch_size, n_channels)\n",
    "            feature_weights (torch.Tensor): (batch_size, n_channels + n_auxiliary_features)\n",
    "        \"\"\"\n",
    "        batch_size = sales.shape[0]\n",
    "        \n",
    "        # 1D 컨볼루션 적용\n",
    "        conv_out = self.conv(sales.unsqueeze(1))  # (batch_size, n_channels, sequence_length)\n",
    "        \n",
    "        # 주기성 강도로 가중치 적용\n",
    "        conv_out = conv_out * period_strength.view(batch_size, 1, 1)\n",
    "        \n",
    "        # 컨볼루션 패턴의 중요도 계산\n",
    "        # 기본 주기만큼의 패턴만 분석 (최근 데이터)\n",
    "        recent_patterns = conv_out[:, :, -self.fundamental_period:]\n",
    "        conv_weights = self.conv_attention(recent_patterns.flatten(1))\n",
    "        weighted_conv = conv_out * conv_weights.unsqueeze(-1)\n",
    "        \n",
    "        # 시퀀스를 통합하여 각 채널의 최종 특성 추출\n",
    "        pooled_features = F.adaptive_avg_pool1d(weighted_conv, 1).squeeze(-1)\n",
    "        \n",
    "        # 보조 특성과 결합\n",
    "        combined_features = torch.cat([pooled_features, auxiliary_features], dim=1)\n",
    "        \n",
    "        # 특성 중요도 계산\n",
    "        feature_weights = self.feature_importance(combined_features)\n",
    "        weighted_features = combined_features * feature_weights\n",
    "        \n",
    "        # 최종 예측\n",
    "        prediction = self.predictor(weighted_features)\n",
    "        \n",
    "        return prediction, conv_weights, feature_weights\n",
    "\n",
    "class InterpretabilityAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def analyze_contribution(self, sales, period_strength, auxiliary_features):\n",
    "        \"\"\"\n",
    "        예측에 대한 각 컴포넌트의 기여도 분석\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'prediction': 예측값,\n",
    "                'conv_pattern_contributions': 컨볼루션 패턴별 기여도,\n",
    "                'auxiliary_contributions': 보조 특성별 기여도\n",
    "            }\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred, conv_weights, feature_weights = self.model(\n",
    "                sales, period_strength, auxiliary_features)\n",
    "            \n",
    "            # 컨볼루션 특성과 보조 특성의 중요도 분리\n",
    "            conv_importance = feature_weights[:, :self.model.n_channels]\n",
    "            aux_importance = feature_weights[:, self.model.n_channels:]\n",
    "            \n",
    "            # 컨볼루션 패턴의 기여도 분석\n",
    "            pattern_contributions = {}\n",
    "            for i, weight in enumerate(conv_weights[0]):\n",
    "                pattern_contributions[f'pattern_{i}'] = float(weight)\n",
    "            \n",
    "            # 보조 특성의 기여도 분석\n",
    "            aux_contributions = {\n",
    "                'is_weekend': float(aux_importance[0, 0]),\n",
    "                'cultural_event': float(aux_importance[0, 1]),\n",
    "                'national_event': float(aux_importance[0, 2]),\n",
    "                'religious_event': float(aux_importance[0, 3]),\n",
    "                'sporting_event': float(aux_importance[0, 4]),\n",
    "                'price_change': float(aux_importance[0, 5])\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'prediction': float(pred[0]),\n",
    "                'conv_pattern_contributions': pattern_contributions,\n",
    "                'auxiliary_contributions': aux_contributions\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class ProductSpecificConv1D(layers.Layer):\n",
    "    def __init__(self, n_products, fundamental_period, n_channels):\n",
    "        \"\"\"\n",
    "        각 제품별로 독립적인 1D 컨볼루션을 수행하는 레이어\n",
    "        \n",
    "        Args:\n",
    "            n_products: 전체 제품 수\n",
    "            fundamental_period: 기본 주기 (예: 7일)\n",
    "            n_channels: 컨볼루션 출력 채널 수\n",
    "        \"\"\"\n",
    "        super(ProductSpecificConv1D, self).__init__()\n",
    "        self.n_products = n_products\n",
    "        self.fundamental_period = fundamental_period\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # 각 제품별 독립적인 컨볼루션 레이어\n",
    "        self.conv_layers = [\n",
    "            layers.Conv1D(\n",
    "                filters=n_channels,\n",
    "                kernel_size=fundamental_period,\n",
    "                padding='same'\n",
    "            ) for _ in range(n_products)\n",
    "        ]\n",
    "    \n",
    "    def call(self, inputs, product_ids):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = []\n",
    "        \n",
    "        # 각 배치 항목에 대해 해당 제품의 컨볼루션 레이어 적용\n",
    "        for i in range(batch_size):\n",
    "            prod_id = product_ids[i]\n",
    "            conv_out = self.conv_layers[prod_id](tf.expand_dims(inputs[i], 0))\n",
    "            outputs.append(conv_out)\n",
    "            \n",
    "        return tf.concat(outputs, axis=0)\n",
    "\n",
    "class DomainKnowledgeWeights(layers.Layer):\n",
    "    def __init__(self, n_auxiliary_features):\n",
    "        \"\"\"도메인 지식 기반의 가중치 계산 레이어\"\"\"\n",
    "        super(DomainKnowledgeWeights, self).__init__()\n",
    "        self.n_auxiliary_features = n_auxiliary_features\n",
    "        \n",
    "        # 보조 특성의 베이스라인 중요도\n",
    "        self.base_importance = self.add_weight(\n",
    "            shape=(n_auxiliary_features,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='base_importance'\n",
    "        )\n",
    "    \n",
    "    def call(self, auxiliary_features, period_strength):\n",
    "        # 비-제로 비율 계산\n",
    "        non_zero_ratio = tf.reduce_mean(\n",
    "            tf.cast(auxiliary_features != 0, tf.float32),\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        # 기본 중요도와 비-제로 비율 결합\n",
    "        importance = self.base_importance * non_zero_ratio\n",
    "        \n",
    "        # Softmax로 정규화\n",
    "        importance = tf.nn.softmax(importance)\n",
    "        \n",
    "        return tf.tile(tf.expand_dims(importance, 0), [tf.shape(auxiliary_features)[0], 1])\n",
    "\n",
    "class InterpretableTimeSeriesModel(Model):\n",
    "    def __init__(self, n_products, fundamental_period, n_channels, n_auxiliary_features):\n",
    "        super(InterpretableTimeSeriesModel, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.fundamental_period = fundamental_period\n",
    "        \n",
    "        # 제품별 독립적인 1D 컨볼루션\n",
    "        self.conv = ProductSpecificConv1D(\n",
    "            n_products=n_products,\n",
    "            fundamental_period=fundamental_period,\n",
    "            n_channels=n_channels\n",
    "        )\n",
    "        \n",
    "        # 도메인 지식 기반 가중치\n",
    "        self.domain_weights = DomainKnowledgeWeights(n_auxiliary_features)\n",
    "        \n",
    "        # 예측 레이어\n",
    "        self.predictor = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # 컨볼루션 활성화 저장용\n",
    "        self.conv_activations = None\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        sales, product_ids, period_strength, auxiliary_features = inputs\n",
    "        \n",
    "        # 제품별 1D 컨볼루션 적용\n",
    "        conv_out = self.conv(sales, product_ids)\n",
    "        \n",
    "        # 주기성 강도 적용\n",
    "        conv_out = conv_out * tf.expand_dims(tf.expand_dims(period_strength, -1), -1)\n",
    "        \n",
    "        # 컨볼루션 활성화 저장 (해석용)\n",
    "        self.conv_activations = conv_out\n",
    "        \n",
    "        # 특성 추출 (시간 차원 보존)\n",
    "        feature_maps = conv_out  # (batch_size, sequence_length, n_channels)\n",
    "        \n",
    "        # 도메인 지식 기반 가중치 계산\n",
    "        feature_weights = self.domain_weights(auxiliary_features, period_strength)\n",
    "        \n",
    "        # 보조 특성 가중치 적용\n",
    "        weighted_aux_features = auxiliary_features * feature_weights\n",
    "        \n",
    "        # 시간 차원에 대한 평균\n",
    "        pooled_conv = tf.reduce_mean(feature_maps, axis=1)\n",
    "        \n",
    "        # 특성 결합\n",
    "        combined_features = tf.concat([pooled_conv, weighted_aux_features], axis=1)\n",
    "        \n",
    "        # 예측\n",
    "        prediction = self.predictor(combined_features)\n",
    "        \n",
    "        return prediction, feature_weights, feature_maps\n",
    "\n",
    "def weighted_loss(y_true, y_pred, sparse_mask, period_strength, alpha=0.5, beta=0.3):\n",
    "    \"\"\"\n",
    "    희소 데이터 발생 시점과 주기성 강도를 고려한 가중치 손실 함수\n",
    "    \n",
    "    Args:\n",
    "        y_true: 실제 값\n",
    "        y_pred: 예측 값\n",
    "        sparse_mask: 희소 데이터 발생 마스크\n",
    "        period_strength: 주기성 강도\n",
    "        alpha: 희소 데이터 가중치 계수\n",
    "        beta: 주기성 강도 가중치 계수\n",
    "    \"\"\"\n",
    "    # 기본 손실\n",
    "    base_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # 가중치 계산\n",
    "    weights = tf.ones_like(sparse_mask)\n",
    "    \n",
    "    # 희소 데이터 발생 시점 가중치\n",
    "    weights = weights + (sparse_mask * alpha)\n",
    "    \n",
    "    # 주기 강도 가중치\n",
    "    weights = weights * (1 + period_strength * beta)\n",
    "    \n",
    "    # 가중치 정규화\n",
    "    weights = weights / tf.reduce_mean(weights)\n",
    "    \n",
    "    # 가중치 적용된 손실\n",
    "    weighted_loss = base_loss * weights\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "class TimeSeriesAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_contribution(self, sales, product_ids, period_strength, auxiliary_features):\n",
    "        \"\"\"예측에 대한 각 컴포넌트의 기여도 분석\"\"\"\n",
    "        prediction, feature_weights, feature_maps = self.model([\n",
    "            sales, product_ids, period_strength, auxiliary_features\n",
    "        ])\n",
    "        \n",
    "        # 시간별 활성화 점수 계산\n",
    "        temporal_importance = tf.reduce_mean(feature_maps, axis=-1)  # (batch_size, sequence_length)\n",
    "        \n",
    "        # 컨볼루션 필터별 중요도\n",
    "        filter_importance = tf.reduce_mean(feature_maps, axis=[0, 1])  # (n_channels,)\n",
    "        \n",
    "        aux_contributions = {\n",
    "            'is_weekend': float(feature_weights[0, 0].numpy()),\n",
    "            'cultural_event': float(feature_weights[0, 1].numpy()),\n",
    "            'national_event': float(feature_weights[0, 2].numpy()),\n",
    "            'religious_event': float(feature_weights[0, 3].numpy()),\n",
    "            'sporting_event': float(feature_weights[0, 4].numpy()),\n",
    "            'price_change': float(feature_weights[0, 5].numpy())\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'prediction': float(prediction[0].numpy()),\n",
    "            'period_strength': float(period_strength[0].numpy()),\n",
    "            'auxiliary_contributions': aux_contributions,\n",
    "            'temporal_importance': temporal_importance.numpy(),\n",
    "            'filter_importance': filter_importance.numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 데이터 로드\n",
    "\n",
    "# 판매 시작 시점 \n",
    "with open('../data/preprocessed/first_sales_column_dict.pkl', 'rb') as f:\n",
    "    first_sales_column_dict = pickle.load(f)\n",
    "\n",
    "# 판매량 데이터 (아이템별 상이)\n",
    "detrended_sales = pd.read_csv(\"../data/loess/detrended_sales.csv\")\n",
    "# 판매량 데이터의 기본 주기, 주기 세기 (아이템별 상이)\n",
    "with open('../data/fourier/results.pkl', 'rb') as f:\n",
    "    fourier_results = pickle.load(f)\n",
    "\n",
    "# 판매 가격 데이터 (아이템별 상이)\n",
    "log_differenced_sell_prices = pd.read_csv(\"../data/log_differencing/log_differenced_sell_prices.csv\")\n",
    "# 주말 여부 데이터 (1941일)\n",
    "is_weekend = pd.read_csv(\"../data/preprocessed/is_weekend.csv\")\n",
    "# 이벤트 데이터 (1941일)\n",
    "event_type_cultural = pd.read_csv(\"../data/preprocessed/event_type_cultural.csv\")\n",
    "event_type_national = pd.read_csv(\"../data/preprocessed/event_type_national.csv\")\n",
    "event_type_religious = pd.read_csv(\"../data/preprocessed/event_type_religious.csv\")\n",
    "event_type_sporting = pd.read_csv(\"../data/preprocessed/event_type_sporting.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_each_item_based_on_first_sales(state_item_id, start_day, sequence_length=None):\n",
    "    \"\"\"\n",
    "    아이템 ID와 시작 시점을 기준으로 데이터를 슬라이싱.\n",
    "    \n",
    "    Args:\n",
    "        item_id: 아이템 ID.\n",
    "        start_day: 판매 시작 시점.\n",
    "        sequence_length: 슬라이싱할 시퀀스 길이 (None이면 끝까지).\n",
    "        \n",
    "    Returns:\n",
    "        아이템별 판매량, 가격, 주말, 이벤트 데이터를 슬라이싱한 결과.\n",
    "    \"\"\"\n",
    "    # 주 ID, 아이템 ID\n",
    "    state_id, item_id = state_item_id[0], state_item_id[1]\n",
    "\n",
    "    # 판매량 데이터\n",
    "    sales_data = detrended_sales[(detrended_sales['state_id']==state_id) & (detrended_sales['item_id']==item_id)][start_day:]\n",
    "\n",
    "    # 가격 데이터\n",
    "    price_data = log_differenced_sell_prices[item_id][start_day:]\n",
    "    \n",
    "    # 주말 데이터 \n",
    "    weekend_data = is_weekend[start_day:]\n",
    "\n",
    "    # 이벤트 데이터\n",
    "    cultural_data = event_type_cultural[start_day:]\n",
    "    national_data = event_type_national[start_day:]\n",
    "    religious_data = event_type_religious[start_day:]\n",
    "    sporting_data = event_type_sporting[start_day:]\n",
    "    \n",
    "    # 슬라이싱\n",
    "    if sequence_length:\n",
    "        sales_data = sales_data[:sequence_length]\n",
    "        price_data = price_data[:sequence_length]\n",
    "        weekend_data = weekend_data[:sequence_length]\n",
    "        cultural_data = cultural_data[:sequence_length]\n",
    "        national_data = national_data[:sequence_length]\n",
    "        religious_data = religious_data[:sequence_length]\n",
    "        sporting_data = sporting_data[:sequence_length]\n",
    "    \n",
    "    # 출력\n",
    "    return {\n",
    "        \"sales\": sales_data,\n",
    "        \"price\": price_data,\n",
    "        \"weekend\": weekend_data,\n",
    "        \"cultural\": cultural_data,\n",
    "        \"national\": national_data,\n",
    "        \"religious\": religious_data,\n",
    "        \"sporting\": sporting_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_generator(sequence_length=None):\n",
    "    \"\"\"\n",
    "    데이터를 아이템별로 제너레이터 형태로 반환.\n",
    "    \n",
    "    Args:\n",
    "        sequence_length: 슬라이싱할 시퀀스 길이.\n",
    "        \n",
    "    Yields:\n",
    "        아이템별 데이터를 딕셔너리 형태로 반환.\n",
    "    \"\"\"\n",
    "    for state_item_id, first_sales_day in first_sales_column_dict.items():\n",
    "        item_data = slice_each_item_based_on_first_sales(state_item_id, first_sales_day, sequence_length)\n",
    "        yield (\n",
    "            {\n",
    "                \"sales\": tf.convert_to_tensor(item_data[\"sales\"], dtype=tf.float32),\n",
    "                \"price\": tf.convert_to_tensor(item_data[\"price\"], dtype=tf.float32),\n",
    "                \"weekend\": tf.convert_to_tensor(item_data[\"weekend\"], dtype=tf.float32),\n",
    "                \"cultural\": tf.convert_to_tensor(item_data[\"cultural\"], dtype=tf.float32),\n",
    "                \"national\": tf.convert_to_tensor(item_data[\"national\"], dtype=tf.float32),\n",
    "                \"religious\": tf.convert_to_tensor(item_data[\"religious\"], dtype=tf.float32),\n",
    "                \"sporting\": tf.convert_to_tensor(item_data[\"sporting\"], dtype=tf.float32),\n",
    "            },\n",
    "            tf.convert_to_tensor(0.0, dtype=tf.float32),  # 예제 라벨 (추후 수정 가능)\n",
    "        )\n",
    "\n",
    "룩백 윈도우 크기와 예측 크기를 조합해 최적의 성능을 확인\n",
    "가장 큰 주기로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.sin(np.linspace(0, 100, 1000))  # 주기적인 데이터\n",
    "lookback = 65  # 룩백 윈도우 크기\n",
    "X, y = [], []\n",
    "for i in range(len(data) - lookback):\n",
    "    X.append(data[i:i+lookback])\n",
    "    y.append(data[i+lookback])\n",
    "X, y = np.array(X), np.array(y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

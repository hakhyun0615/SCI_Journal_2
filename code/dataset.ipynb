{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class InterpretableTimeSeriesModel(nn.Module):\n",
    "    def __init__(self, fundamental_period, n_channels, n_auxiliary_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            fundamental_period (int): 기본 주기 (예: 7일)\n",
    "            n_channels (int): 컨볼루션 출력 채널 수\n",
    "            n_auxiliary_features (int): 보조 특성 수 (주말여부, 이벤트, 가격변화 등)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.fundamental_period = fundamental_period\n",
    "        \n",
    "        # 단일 주기를 사용한 1D 컨볼루션\n",
    "        # Input: (batch_size, 1, sequence_length)\n",
    "        # Output: (batch_size, n_channels, sequence_length)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=1, \n",
    "            out_channels=n_channels, \n",
    "            kernel_size=fundamental_period,\n",
    "            stride=1,  # 모든 시점의 패턴을 분석하기 위해 stride=1 사용\n",
    "            padding=fundamental_period-1  # 시퀀스 앞부분의 패턴도 캡쳐하기 위한 패딩\n",
    "        )\n",
    "        \n",
    "        # 컨볼루션 가중치 해석을 위한 어텐션\n",
    "        # Input: (batch_size, n_channels * sequence_length)\n",
    "        # Output: (batch_size, n_channels)\n",
    "        self.conv_attention = nn.Sequential(\n",
    "            nn.Linear(n_channels * fundamental_period, n_channels),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "        \n",
    "        # 특성 중요도 계산 레이어\n",
    "        # Input: (batch_size, n_channels + n_auxiliary_features)\n",
    "        # Output: (batch_size, n_channels + n_auxiliary_features)\n",
    "        self.feature_importance = nn.Sequential(\n",
    "            nn.Linear(n_channels + n_auxiliary_features, \n",
    "                     n_channels + n_auxiliary_features),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # 최종 예측 레이어\n",
    "        # Input: (batch_size, n_channels + n_auxiliary_features)\n",
    "        # Output: (batch_size, 1)\n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(n_channels + n_auxiliary_features, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, sales, period_strength, auxiliary_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            sales (torch.Tensor): (batch_size, sequence_length)\n",
    "            period_strength (torch.Tensor): (batch_size, 1) 주기성 강도\n",
    "            auxiliary_features (torch.Tensor): (batch_size, n_auxiliary_features)\n",
    "            \n",
    "        Returns:\n",
    "            prediction (torch.Tensor): (batch_size, 1)\n",
    "            conv_weights (torch.Tensor): (batch_size, n_channels)\n",
    "            feature_weights (torch.Tensor): (batch_size, n_channels + n_auxiliary_features)\n",
    "        \"\"\"\n",
    "        batch_size = sales.shape[0]\n",
    "        \n",
    "        # 1D 컨볼루션 적용\n",
    "        conv_out = self.conv(sales.unsqueeze(1))  # (batch_size, n_channels, sequence_length)\n",
    "        \n",
    "        # 주기성 강도로 가중치 적용\n",
    "        conv_out = conv_out * period_strength.view(batch_size, 1, 1)\n",
    "        \n",
    "        # 컨볼루션 패턴의 중요도 계산\n",
    "        # 기본 주기만큼의 패턴만 분석 (최근 데이터)\n",
    "        recent_patterns = conv_out[:, :, -self.fundamental_period:]\n",
    "        conv_weights = self.conv_attention(recent_patterns.flatten(1))\n",
    "        weighted_conv = conv_out * conv_weights.unsqueeze(-1)\n",
    "        \n",
    "        # 시퀀스를 통합하여 각 채널의 최종 특성 추출\n",
    "        pooled_features = F.adaptive_avg_pool1d(weighted_conv, 1).squeeze(-1)\n",
    "        \n",
    "        # 보조 특성과 결합\n",
    "        combined_features = torch.cat([pooled_features, auxiliary_features], dim=1)\n",
    "        \n",
    "        # 특성 중요도 계산\n",
    "        feature_weights = self.feature_importance(combined_features)\n",
    "        weighted_features = combined_features * feature_weights\n",
    "        \n",
    "        # 최종 예측\n",
    "        prediction = self.predictor(weighted_features)\n",
    "        \n",
    "        return prediction, conv_weights, feature_weights\n",
    "\n",
    "class InterpretabilityAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        \n",
    "    def analyze_contribution(self, sales, period_strength, auxiliary_features):\n",
    "        \"\"\"\n",
    "        예측에 대한 각 컴포넌트의 기여도 분석\n",
    "        \n",
    "        Returns:\n",
    "            dict: {\n",
    "                'prediction': 예측값,\n",
    "                'conv_pattern_contributions': 컨볼루션 패턴별 기여도,\n",
    "                'auxiliary_contributions': 보조 특성별 기여도\n",
    "            }\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            pred, conv_weights, feature_weights = self.model(\n",
    "                sales, period_strength, auxiliary_features)\n",
    "            \n",
    "            # 컨볼루션 특성과 보조 특성의 중요도 분리\n",
    "            conv_importance = feature_weights[:, :self.model.n_channels]\n",
    "            aux_importance = feature_weights[:, self.model.n_channels:]\n",
    "            \n",
    "            # 컨볼루션 패턴의 기여도 분석\n",
    "            pattern_contributions = {}\n",
    "            for i, weight in enumerate(conv_weights[0]):\n",
    "                pattern_contributions[f'pattern_{i}'] = float(weight)\n",
    "            \n",
    "            # 보조 특성의 기여도 분석\n",
    "            aux_contributions = {\n",
    "                'is_weekend': float(aux_importance[0, 0]),\n",
    "                'cultural_event': float(aux_importance[0, 1]),\n",
    "                'national_event': float(aux_importance[0, 2]),\n",
    "                'religious_event': float(aux_importance[0, 3]),\n",
    "                'sporting_event': float(aux_importance[0, 4]),\n",
    "                'price_change': float(aux_importance[0, 5])\n",
    "            }\n",
    "            \n",
    "            return {\n",
    "                'prediction': float(pred[0]),\n",
    "                'conv_pattern_contributions': pattern_contributions,\n",
    "                'auxiliary_contributions': aux_contributions\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "class ProductSpecificConv1D(layers.Layer):\n",
    "    def __init__(self, n_products, fundamental_period, n_channels):\n",
    "        \"\"\"\n",
    "        각 제품별로 독립적인 1D 컨볼루션을 수행하는 레이어\n",
    "        \n",
    "        Args:\n",
    "            n_products: 전체 제품 수\n",
    "            fundamental_period: 기본 주기 (예: 7일)\n",
    "            n_channels: 컨볼루션 출력 채널 수\n",
    "        \"\"\"\n",
    "        super(ProductSpecificConv1D, self).__init__()\n",
    "        self.n_products = n_products\n",
    "        self.fundamental_period = fundamental_period\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        # 각 제품별 독립적인 컨볼루션 레이어\n",
    "        self.conv_layers = [\n",
    "            layers.Conv1D(\n",
    "                filters=n_channels,\n",
    "                kernel_size=fundamental_period,\n",
    "                padding='same'\n",
    "            ) for _ in range(n_products)\n",
    "        ]\n",
    "    \n",
    "    def call(self, inputs, product_ids):\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = []\n",
    "        \n",
    "        # 각 배치 항목에 대해 해당 제품의 컨볼루션 레이어 적용\n",
    "        for i in range(batch_size):\n",
    "            prod_id = product_ids[i]\n",
    "            conv_out = self.conv_layers[prod_id](tf.expand_dims(inputs[i], 0))\n",
    "            outputs.append(conv_out)\n",
    "            \n",
    "        return tf.concat(outputs, axis=0)\n",
    "\n",
    "class DomainKnowledgeWeights(layers.Layer):\n",
    "    def __init__(self, n_auxiliary_features):\n",
    "        \"\"\"도메인 지식 기반의 가중치 계산 레이어\"\"\"\n",
    "        super(DomainKnowledgeWeights, self).__init__()\n",
    "        self.n_auxiliary_features = n_auxiliary_features\n",
    "        \n",
    "        # 보조 특성의 베이스라인 중요도\n",
    "        self.base_importance = self.add_weight(\n",
    "            shape=(n_auxiliary_features,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='base_importance'\n",
    "        )\n",
    "    \n",
    "    def call(self, auxiliary_features, period_strength):\n",
    "        # 비-제로 비율 계산\n",
    "        non_zero_ratio = tf.reduce_mean(\n",
    "            tf.cast(auxiliary_features != 0, tf.float32),\n",
    "            axis=0\n",
    "        )\n",
    "        \n",
    "        # 기본 중요도와 비-제로 비율 결합\n",
    "        importance = self.base_importance * non_zero_ratio\n",
    "        \n",
    "        # Softmax로 정규화\n",
    "        importance = tf.nn.softmax(importance)\n",
    "        \n",
    "        return tf.tile(tf.expand_dims(importance, 0), [tf.shape(auxiliary_features)[0], 1])\n",
    "\n",
    "class InterpretableTimeSeriesModel(Model):\n",
    "    def __init__(self, n_products, fundamental_period, n_channels, n_auxiliary_features):\n",
    "        super(InterpretableTimeSeriesModel, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.fundamental_period = fundamental_period\n",
    "        \n",
    "        # 제품별 독립적인 1D 컨볼루션\n",
    "        self.conv = ProductSpecificConv1D(\n",
    "            n_products=n_products,\n",
    "            fundamental_period=fundamental_period,\n",
    "            n_channels=n_channels\n",
    "        )\n",
    "        \n",
    "        # 도메인 지식 기반 가중치\n",
    "        self.domain_weights = DomainKnowledgeWeights(n_auxiliary_features)\n",
    "        \n",
    "        # 예측 레이어\n",
    "        self.predictor = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            layers.Dense(1)\n",
    "        ])\n",
    "        \n",
    "        # 컨볼루션 활성화 저장용\n",
    "        self.conv_activations = None\n",
    "    \n",
    "    def call(self, inputs, training=None):\n",
    "        sales, product_ids, period_strength, auxiliary_features = inputs\n",
    "        \n",
    "        # 제품별 1D 컨볼루션 적용\n",
    "        conv_out = self.conv(sales, product_ids)\n",
    "        \n",
    "        # 주기성 강도 적용\n",
    "        conv_out = conv_out * tf.expand_dims(tf.expand_dims(period_strength, -1), -1)\n",
    "        \n",
    "        # 컨볼루션 활성화 저장 (해석용)\n",
    "        self.conv_activations = conv_out\n",
    "        \n",
    "        # 특성 추출 (시간 차원 보존)\n",
    "        feature_maps = conv_out  # (batch_size, sequence_length, n_channels)\n",
    "        \n",
    "        # 도메인 지식 기반 가중치 계산\n",
    "        feature_weights = self.domain_weights(auxiliary_features, period_strength)\n",
    "        \n",
    "        # 보조 특성 가중치 적용\n",
    "        weighted_aux_features = auxiliary_features * feature_weights\n",
    "        \n",
    "        # 시간 차원에 대한 평균\n",
    "        pooled_conv = tf.reduce_mean(feature_maps, axis=1)\n",
    "        \n",
    "        # 특성 결합\n",
    "        combined_features = tf.concat([pooled_conv, weighted_aux_features], axis=1)\n",
    "        \n",
    "        # 예측\n",
    "        prediction = self.predictor(combined_features)\n",
    "        \n",
    "        return prediction, feature_weights, feature_maps\n",
    "\n",
    "def weighted_loss(y_true, y_pred, sparse_mask, period_strength, alpha=0.5, beta=0.3):\n",
    "    \"\"\"\n",
    "    희소 데이터 발생 시점과 주기성 강도를 고려한 가중치 손실 함수\n",
    "    \n",
    "    Args:\n",
    "        y_true: 실제 값\n",
    "        y_pred: 예측 값\n",
    "        sparse_mask: 희소 데이터 발생 마스크\n",
    "        period_strength: 주기성 강도\n",
    "        alpha: 희소 데이터 가중치 계수\n",
    "        beta: 주기성 강도 가중치 계수\n",
    "    \"\"\"\n",
    "    # 기본 손실\n",
    "    base_loss = tf.keras.losses.mean_squared_error(y_true, y_pred)\n",
    "    \n",
    "    # 가중치 계산\n",
    "    weights = tf.ones_like(sparse_mask)\n",
    "    \n",
    "    # 희소 데이터 발생 시점 가중치\n",
    "    weights = weights + (sparse_mask * alpha)\n",
    "    \n",
    "    # 주기 강도 가중치\n",
    "    weights = weights * (1 + period_strength * beta)\n",
    "    \n",
    "    # 가중치 정규화\n",
    "    weights = weights / tf.reduce_mean(weights)\n",
    "    \n",
    "    # 가중치 적용된 손실\n",
    "    weighted_loss = base_loss * weights\n",
    "    return tf.reduce_mean(weighted_loss)\n",
    "\n",
    "class TimeSeriesAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_contribution(self, sales, product_ids, period_strength, auxiliary_features):\n",
    "        \"\"\"예측에 대한 각 컴포넌트의 기여도 분석\"\"\"\n",
    "        prediction, feature_weights, feature_maps = self.model([\n",
    "            sales, product_ids, period_strength, auxiliary_features\n",
    "        ])\n",
    "        \n",
    "        # 시간별 활성화 점수 계산\n",
    "        temporal_importance = tf.reduce_mean(feature_maps, axis=-1)  # (batch_size, sequence_length)\n",
    "        \n",
    "        # 컨볼루션 필터별 중요도\n",
    "        filter_importance = tf.reduce_mean(feature_maps, axis=[0, 1])  # (n_channels,)\n",
    "        \n",
    "        aux_contributions = {\n",
    "            'is_weekend': float(feature_weights[0, 0].numpy()),\n",
    "            'cultural_event': float(feature_weights[0, 1].numpy()),\n",
    "            'national_event': float(feature_weights[0, 2].numpy()),\n",
    "            'religious_event': float(feature_weights[0, 3].numpy()),\n",
    "            'sporting_event': float(feature_weights[0, 4].numpy()),\n",
    "            'price_change': float(feature_weights[0, 5].numpy())\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'prediction': float(prediction[0].numpy()),\n",
    "            'period_strength': float(period_strength[0].numpy()),\n",
    "            'auxiliary_contributions': aux_contributions,\n",
    "            'temporal_importance': temporal_importance.numpy(),\n",
    "            'filter_importance': filter_importance.numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판매 시작 시점\n",
    "with open('../data/preprocessed/first_sales_column_dict.pkl', 'rb') as f:\n",
    "    first_sales_column_dict = pickle.load(f)\n",
    "\n",
    "# 판매량 데이터\n",
    "detrended_sales = pd.read_csv(\"../data/loess/detrended_sales.csv\")\n",
    "# 기본 주기, 주기 세기\n",
    "with open('../data/fourier/results.pkl', 'rb') as f:\n",
    "    fourier_results = pickle.load(f)\n",
    "\n",
    "# 판매 가격 데이터\n",
    "log_differenced_sell_prices = pd.read_csv(\"../data/log_differencing/log_differenced_sell_prices.csv\")\n",
    "# 주말 여부 데이터\n",
    "is_weekend = pd.read_csv(\"../data/auxiliary_features/is_weekend.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

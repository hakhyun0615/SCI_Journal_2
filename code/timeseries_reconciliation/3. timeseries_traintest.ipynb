{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import time\n",
    "import pickle\n",
    "import shutil\n",
    "import psutil\n",
    "import GPUtil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)\n",
    "\n",
    "import mxnet as mx\n",
    "mx.random.seed(0)\n",
    "\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.model.predictor import Predictor\n",
    "from gluonts.mx import Trainer\n",
    "from gluonts.core.component import validated\n",
    "from gluonts.mx.trainer.callback import Callback\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from timeseries_models import *\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(Callback):\n",
    "    def __init__(self, patience: int):\n",
    "        self.patience = patience\n",
    "        self.best_loss = float('inf')\n",
    "        self.wait_count = 0\n",
    "        self.best_params_path = None\n",
    "        self.temp_dir = None\n",
    "        \n",
    "    def __del__(self):\n",
    "        if self.temp_dir is not None:\n",
    "            shutil.rmtree(self.temp_dir, ignore_errors=True)\n",
    "        \n",
    "    def on_validation_epoch_end(\n",
    "        self,\n",
    "        epoch_no: int,\n",
    "        epoch_loss: float,\n",
    "        training_network: mx.gluon.HybridBlock,\n",
    "        trainer: mx.gluon.Trainer,\n",
    "    ) -> bool:\n",
    "        if epoch_loss < self.best_loss:\n",
    "            self.best_loss = epoch_loss\n",
    "            self.wait_count = 0\n",
    "            if self.temp_dir is None:\n",
    "                self.temp_dir = tempfile.mkdtemp(prefix='early_stopping_')\n",
    "\n",
    "            self.best_params_path = os.path.join(self.temp_dir, 'best_model.params')\n",
    "            training_network.save_parameters(self.best_params_path)\n",
    "        else:\n",
    "            self.wait_count += 1\n",
    "        if self.wait_count >= self.patience:\n",
    "            print(f\"Early stopping triggered\")\n",
    "            if self.best_params_path is not None:\n",
    "                training_network.load_parameters(self.best_params_path)\n",
    "                print(\"Restored best estimator\")\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "class EarlyStoppingTrainer(Trainer):\n",
    "    @validated()\n",
    "    def __init__(self, patience: int, **kwargs):\n",
    "        callbacks = kwargs.get('callbacks', [])\n",
    "        callbacks.append(EarlyStoppingCallback(patience=patience))\n",
    "        kwargs['callbacks'] = callbacks\n",
    "        super().__init__(**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimal_num_batches(gpu_available, verbose=False):\n",
    "    # memory\n",
    "    memory_info = psutil.virtual_memory()\n",
    "    total_memory = memory_info.total\n",
    "    available_memory = memory_info.available\n",
    "\n",
    "    # cpu\n",
    "    cpu_usage = psutil.cpu_percent(interval=None)\n",
    "\n",
    "    # gpu\n",
    "    if gpu_available:\n",
    "        try:\n",
    "            gpus = GPUtil.getGPUs()\n",
    "            if gpus:\n",
    "                gpu_memory = sum(gpu.memoryTotal for gpu in gpus)\n",
    "                gpu_memory_available = sum(gpu.memoryFree for gpu in gpus)\n",
    "            else:\n",
    "                gpu_memory = 0\n",
    "                gpu_memory_available = 0\n",
    "        except ImportError:\n",
    "            gpu_memory = 0\n",
    "            gpu_memory_available = 0\n",
    "    else:\n",
    "        gpu_memory = 0\n",
    "        gpu_memory_available = 0\n",
    "\n",
    "    # optimal\n",
    "    memory_factor = available_memory / total_memory\n",
    "    cpu_factor = (100 - cpu_usage) / 100\n",
    "    gpu_factor = gpu_memory_available / max(gpu_memory, 1) if gpu_available else 1\n",
    "    optimal_num_batches = int(200 * (0.5 * memory_factor + 0.3 * cpu_factor + 0.2 * gpu_factor))\n",
    "\n",
    "    if verbose: print(f\"Optimal num batches: {optimal_num_batches}\")\n",
    "    \n",
    "    return max(1, optimal_num_batches)\n",
    "\n",
    "def train_estimator(epochs, learning_rate, estimator_name):    \n",
    "    save_dir = f'../../result/timeseries'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    for level_idx in range(1, 13):\n",
    "        level_dir = os.path.join(save_dir, f'level {level_idx}')\n",
    "        os.makedirs(level_dir, exist_ok=True)\n",
    "        estimator_dir = os.path.join(level_dir, estimator_name)\n",
    "        if any(existing_dir.startswith(estimator_name) for existing_dir in os.listdir(level_dir)):\n",
    "            continue\n",
    "        os.makedirs(estimator_dir, exist_ok=True)\n",
    "\n",
    "        highlight_print(f\"\\n========== Level {level_idx} ==========\")\n",
    "        print(f\"Loading dataset...\", end=' ')\n",
    "        dataset_start = time.time()\n",
    "        if estimator_name == 'TFT':\n",
    "            dataset_dir = '../../dataset/tft'\n",
    "        else:\n",
    "            dataset_dir = '../../dataset/else'\n",
    "        with open(os.path.join(dataset_dir, f'dataset_level_{level_idx}.pkl'), 'rb') as f:\n",
    "            dataset = pickle.load(f)\n",
    "        print(f\"{(time.time() - dataset_start)/60:.1f} minutes\")\n",
    "\n",
    "        estimator = create_estimator(\n",
    "            level_idx=level_idx, \n",
    "            train_dataset=dataset['train'],\n",
    "            estimator_name=estimator_name,\n",
    "        )  \n",
    "\n",
    "        estimator_start = time.time()\n",
    "        print(\"Start training...\")\n",
    "        train_start = time.time()\n",
    "        estimator.trainer = EarlyStoppingTrainer(\n",
    "            epochs=epochs,\n",
    "            learning_rate=learning_rate,\n",
    "            num_batches_per_epoch=get_optimal_num_batches(mx.context.num_gpus()),\n",
    "            patience=15,\n",
    "        )\n",
    "        predictor = estimator.train(\n",
    "            training_data=dataset['train'],\n",
    "            validation_data=dataset['valid']\n",
    "        )\n",
    "        predictor.serialize(Path(f\"{level_dir}/{estimator_name}\"))\n",
    "        highlight_print(f\"End training... {(time.time() - train_start)/60:.1f} minutes\", color='green')\n",
    "\n",
    "        print(\"Start predicting...\")\n",
    "        pred_start = time.time()\n",
    "        test_forecasts_it, test_labels_it = make_evaluation_predictions(\n",
    "            dataset=dataset['test'],\n",
    "            predictor=predictor,\n",
    "        )\n",
    "        test_forecasts = list(test_forecasts_it)\n",
    "        test_labels = list(test_labels_it)\n",
    "        highlight_print(f\"End predicting... {(time.time() - pred_start)/60:.1f} minutes\", color='green')\n",
    "\n",
    "        print(\"Start saving...\")\n",
    "        pred_save_start = time.time()\n",
    "        with open(f\"{level_dir}/{estimator_name}/test_labels.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_labels, f)\n",
    "        with open(f\"{level_dir}/{estimator_name}/test_forecasts.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_forecasts, f)\n",
    "        highlight_print(f\"End saving... {(time.time() - pred_save_start)/60:.1f} minutes\", color='green')\n",
    "\n",
    "        # print(\"Start plotting...\")\n",
    "        # plot_start = time.time()\n",
    "        # for i in range(len(test_forecasts)):\n",
    "        #     plt.figure(figsize=(12, 6))\n",
    "        #     plt.plot(test_labels[i][-100:].to_timestamp(), label=\"Actual\")\n",
    "        #     plt.plot(pd.Series(test_forecasts[i].quantile(0.5), index=test_forecasts[i].start_date.to_timestamp() + pd.to_timedelta(range(len(test_forecasts[0].quantile(0.5))), unit='D')), label=\"Forecast\")\n",
    "        #     plt.title(f'{test_forecasts[i].item_id}')\n",
    "        #     plt.xlabel('Date')\n",
    "        #     plt.ylabel('Sales')\n",
    "        #     plt.legend(loc=\"upper right\")\n",
    "        #     plt.savefig(os.path.join(estimator_dir, f'series_{i + 1}.png'))\n",
    "        #     plt.close()\n",
    "        # highlight_print(f\"End plotting... {(time.time() - plot_start)/60:.1f} minutes\", color='green')\n",
    "            \n",
    "        print(\"Start evaluating...\")\n",
    "        eval_start = time.time()\n",
    "        evaluator = Evaluator(quantiles=(0.5,), ignore_invalid_values=True)\n",
    "        test_metrics_all_id, test_metrics_per_id = evaluator(test_labels, test_forecasts)\n",
    "        highlight_print(f\"End evaluating... {(time.time() - eval_start)/60:.1f} minutes\", color='green')\n",
    "\n",
    "        print(\"Start saving...\")\n",
    "        eval_save_start = time.time()\n",
    "        with open(f\"{level_dir}/{estimator_name}/test_metrics_all_id.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_metrics_all_id, f)\n",
    "        with open(f\"{level_dir}/{estimator_name}/test_metrics_per_id.pkl\", \"wb\") as f:\n",
    "            pickle.dump(test_metrics_per_id, f)\n",
    "        highlight_print(f\"End saving... {(time.time() - eval_save_start)/60:.1f} minutes\", color='green')\n",
    "\n",
    "        highlight_print(f\"\\nTotal time: {(time.time() - estimator_start)/60:.1f} minutes\", color='red')\n",
    "\n",
    "        # rename estimator directory\n",
    "        os.rename(estimator_dir, f\"{estimator_dir}_{test_metrics_all_id['MSE']:.2f}\")\n",
    "\n",
    "        # reduce memory\n",
    "        del estimator, predictor, test_forecasts, test_labels\n",
    "        gc.collect()\n",
    "\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_estimator(epochs=500, learning_rate=1e-4, estimator_name='DeepAR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot result\n",
    "level_idx = 1\n",
    "estimator = 'DeepAR'\n",
    "\n",
    "ids = np.empty((0, ))\n",
    "test_labels = np.empty((0, 28))\n",
    "test_forecasts = np.empty((0, 28))\n",
    "\n",
    "for level in range(level_idx, level_idx+1):\n",
    "    level_dir = f\"../result/level {level}\"\n",
    "    for model_dir in os.listdir(level_dir):\n",
    "        if model_dir.startswith(estimator):\n",
    "            model_path = os.path.join(level_dir, model_dir)\n",
    "            if os.path.isdir(model_path):\n",
    "                for root, dirs, files in os.walk(model_path):\n",
    "                    for file in files:\n",
    "                        if file.startswith('test_labels'):\n",
    "                            with open(os.path.join(root, file), 'rb') as pickle_file:\n",
    "                                test_labels = pd.read_pickle(pickle_file)\n",
    "                        if file.startswith('test_forecasts'):\n",
    "                            with open(os.path.join(root, file), 'rb') as pickle_file:\n",
    "                                test_forecasts = pd.read_pickle(pickle_file)\n",
    "    \n",
    "    page = 2\n",
    "    graphs_per_page = 25\n",
    "\n",
    "    start_idx = page * graphs_per_page\n",
    "    end_idx = start_idx + graphs_per_page\n",
    "\n",
    "    num_plots = min(len(test_labels[start_idx:end_idx]), graphs_per_page)\n",
    "    cols = 5\n",
    "    rows = (num_plots + cols - 1) // cols \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(25, 5 * rows))\n",
    "\n",
    "    for i in range(num_plots):\n",
    "        row = i // cols\n",
    "        col = i % cols\n",
    "        ax = axes[row, col] if rows > 1 else axes[col]\n",
    "        \n",
    "        ax.plot(test_labels[start_idx + i][-150:].to_timestamp(), label=\"Actual\")\n",
    "        ax.plot(pd.Series(test_forecasts[start_idx + i].quantile(0.5), index=test_forecasts[start_idx + i].start_date.to_timestamp() + pd.to_timedelta(range(len(test_forecasts[0].quantile(0.5))), unit='D')), label=\"Forecast\")\n",
    "        ax.set_title(f'{test_forecasts[start_idx + i].item_id}')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Sales')\n",
    "        ax.legend(loc=\"upper right\")\n",
    "\n",
    "    for j in range(num_plots, rows * cols):\n",
    "        fig.delaxes(axes.flatten()[j])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

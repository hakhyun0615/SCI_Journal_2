{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42840,)\n",
      "(42840, 28)\n",
      "(42840, 28)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "estimator = \"DeepAR\"\n",
    "\n",
    "test_ids = np.empty((0, ))\n",
    "test_labels = np.empty((0, 28))\n",
    "test_forecasts = np.empty((0, 28))\n",
    "\n",
    "for level in range(1, 13):\n",
    "    level_dir = f\"../result/learning_rate_0.001/level {level}\"\n",
    "    for root, dirs, files in os.walk(level_dir):\n",
    "        for file in files:\n",
    "            if file.startswith('test_labels'):\n",
    "                with open(os.path.join(root, file), 'rb') as pickle_file:\n",
    "                    datas = pd.read_pickle(pickle_file)\n",
    "                    arrays = np.array(datas)\n",
    "                    arrays = np.squeeze(arrays, axis=-1)\n",
    "                    arrays = arrays[:, -28:]\n",
    "                    test_labels = np.concatenate((test_labels, arrays), axis=0)\n",
    "            if file.startswith('test_forecasts'):\n",
    "                with open(os.path.join(root, file), 'rb') as pickle_file:\n",
    "                    datas = pd.read_pickle(pickle_file)\n",
    "                    ids = np.array([data.item_id for data in datas])\n",
    "                    test_ids = np.concatenate((test_ids, ids), axis=0)\n",
    "                    arrays = np.array([data.quantile(0.5) for data in datas])\n",
    "                    test_forecasts = np.concatenate((test_forecasts, arrays), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wrmsse(y_true, y_pred):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv('../data/original/sell_prices.csv')\n",
    "\n",
    "    sales = sales.iloc[:, 6:].values\n",
    "\n",
    "    sell_prices['id'] = sell_prices['store_id'] + '_' + sell_prices['item_id']\n",
    "    sell_prices = sell_prices[sell_prices['wm_yr_wk'] <= 11613]\n",
    "    sell_prices = sell_prices.pivot(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "    sell_prices = sell_prices.values\n",
    "\n",
    "    N, h = y_true.shape \n",
    "    w = sell_prices.shape[1]  \n",
    "\n",
    "    daily_prices = np.repeat(sell_prices, repeats=7, axis=1)[:, -sales.shape[1]:]\n",
    "    daily_prices = np.where(np.isnan(daily_prices), np.nan, daily_prices)\n",
    "    \n",
    "    squared_errors = np.mean((y_true - y_pred) ** 2, axis=1)\n",
    "    scale = np.mean(np.diff(sales, axis=1) ** 2, axis=1)\n",
    "    rmsse = np.sqrt(squared_errors / (scale + 1e-10))\n",
    "\n",
    "    total_revenue = np.nansum(sales[:, -28:] * daily_prices[:, -28:], axis=1) \n",
    "    weight = total_revenue / np.nansum(total_revenue) \n",
    "\n",
    "    wrmsse = np.nansum(weight * rmsse)\n",
    "    \n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_wrmsse(test_labels, test_forecasts) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.linalg import pinv\n",
    "\n",
    "def create_S(y_id):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sales['id'] = sales['id'].str.replace('_validation', '') # 30490\n",
    "\n",
    "    states = sales['state_id'].unique()  \n",
    "    stores = sales['store_id'].unique()\n",
    "    cats = sales['cat_id'].unique()\n",
    "    depts = sales['dept_id'].unique()\n",
    "    states_cats = [f\"{state}_{cat}\" for state in states for cat in cats]\n",
    "    states_depts = [f\"{state}_{dept}\" for state in states for dept in depts]\n",
    "    stores_cats = [f\"{store}_{cat}\" for store in stores for cat in cats]\n",
    "    stores_depts = [f\"{store}_{dept}\" for store in stores for dept in depts]\n",
    "    items = sales['item_id'].unique()\n",
    "    items_states = [f\"{item}_{state}\" for item in items for state in states]\n",
    "    items_stores = [f\"{item}_{store}\" for item in items for store in stores]\n",
    "\n",
    "    S = np.zeros((42840, 30490))\n",
    "\n",
    "    for i, id in tqdm(enumerate(y_id), total=len(y_id)):\n",
    "        if id == 'total':\n",
    "            S[0, :] = 1            \n",
    "        elif id in states:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['state_id'] == id]['id']).astype(int).values\n",
    "        elif id in stores:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['store_id'] == id]['id']).astype(int).values\n",
    "        elif id in cats:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['cat_id'] == id]['id']).astype(int).values\n",
    "        elif id in depts:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['dept_id'] == id]['id']).astype(int).values\n",
    "        elif id in states_cats:\n",
    "            state, cat = id.split('_')\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['state_id'] == state) & (sales['cat_id'] == cat)]['id']).astype(int).values\n",
    "        elif id in states_depts:\n",
    "            splitted_id = id.split('_')\n",
    "            state, dept = splitted_id[0], '_'.join(splitted_id[1:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['state_id'] == state) & (sales['dept_id'] == dept)]['id']).astype(int).values\n",
    "        elif id in stores_cats:\n",
    "            splitted_id = id.split('_')\n",
    "            store, cat = '_'.join(splitted_id[:2]), splitted_id[2]\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['store_id'] == store) & (sales['cat_id'] == cat)]['id']).astype(int).values\n",
    "        elif id in stores_depts:\n",
    "            splitted_id = id.split('_')\n",
    "            store, dept = '_'.join(splitted_id[:2]), '_'.join(splitted_id[2:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['store_id'] == store) & (sales['dept_id'] == dept)]['id']).astype(int).values\n",
    "        elif id in items:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['item_id'] == id]['id']).astype(int).values\n",
    "        elif id in items_states:\n",
    "            splitted_id = id.split('_')\n",
    "            item, state = '_'.join(splitted_id[:3]), '_'.join(splitted_id[3:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['item_id'] == item) & (sales['state_id'] == state)]['id']).astype(int).values\n",
    "        elif id in items_stores:\n",
    "            splitted_id = id.split('_')\n",
    "            item, store = '_'.join(splitted_id[:3]), '_'.join(splitted_id[3:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['item_id'] == item) & (sales['store_id'] == store)]['id']).astype(int).values\n",
    "        else:\n",
    "            print(f\"Error: {id} not found\")\n",
    "\n",
    "    return S\n",
    "\n",
    "def compute_W(y_actual, y_pred):\n",
    "    E = y_actual - y_pred\n",
    "    W = (1 / (E.shape[1] - 1)) * (E @ E.T)\n",
    "    return W\n",
    "\n",
    "S = create_S(test_ids)\n",
    "W = compute_W(test_labels, test_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "\n",
    "def convert_to_appropriate_dtype(matrix):\n",
    "    c_min = matrix.min()\n",
    "    c_max = matrix.max()\n",
    "    \n",
    "    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "        return matrix.astype(np.float16)\n",
    "    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "        return matrix.astype(np.float32)\n",
    "    else:\n",
    "        return matrix.astype(np.float64)\n",
    "\n",
    "S = convert_to_appropriate_dtype(S)\n",
    "W = convert_to_appropriate_dtype(W)\n",
    "test_forecasts = convert_to_appropriate_dtype(test_forecasts)\n",
    "\n",
    "W_inv = inv(W)\n",
    "ST_Winv = S.T @ W_inv\n",
    "ST_Winv_S = ST_Winv @ S\n",
    "ST_Winv_S_inv = inv(ST_Winv_S)\n",
    "S_ST_Winv_S_inv = S @ ST_Winv_S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunked_dot_memmap(A, B, chunk_size, n_jobs=-1, temp_dir='C:/temp'):\n",
    "    def process_chunk_memmap(start, end, A, B, result_memmap):\n",
    "        result_memmap[start:end] = A[start:end] @ B\n",
    "\n",
    "    n_chunks = (A.shape[0] + chunk_size - 1) // chunk_size\n",
    "    result_shape = (A.shape[0], B.shape[1])\n",
    "    \n",
    "    # Ensure the temporary directory exists\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Use an absolute path for the memory-mapped file\n",
    "    result_memmap_path = os.path.join(temp_dir, 'result_memmap.dat')\n",
    "    result_memmap = np.memmap(result_memmap_path, dtype=A.dtype, mode='w+', shape=result_shape)\n",
    "\n",
    "    Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_chunk_memmap)(i * chunk_size, min((i + 1) * chunk_size, A.shape[0]), A, B, result_memmap)\n",
    "        for i in range(n_chunks)\n",
    "    )\n",
    "\n",
    "    return result_memmap\n",
    "\n",
    "# Example usage\n",
    "# Assuming S_ST_Winv_S_inv and ST_Winv are already defined\n",
    "chunk_size = 1000  # Adjust chunk size based on available memory\n",
    "P = chunked_dot_memmap(S_ST_Winv_S_inv, ST_Winv, chunk_size=chunk_size)\n",
    "\n",
    "# Use the result\n",
    "print(\"Shape of P:\", P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecasts_reconciled = P @ test_forecasts\n",
    "\n",
    "print(\"Before reconciliation:\", test_forecasts[:10])\n",
    "print(\"After reconciliation:\", test_forecasts_reconciled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint_reconciliation(S, W, y_pred):\n",
    "    W_inv = inv(W)\n",
    "    P = S @ inv(S.T @ W_inv @ S) @ S @ W_inv\n",
    "    return P @ y_pred\n",
    "\n",
    "test_forecasts_reconciled = mint_reconciliation(S, W, test_forecasts)\n",
    "\n",
    "print(\"Before reconcilation:\", test_forecasts[:10])\n",
    "print(\"After reconcilation:\", test_forecasts_reconciled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wrmsse(y_true, y_pred):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv('../data/original/sell_prices.csv')\n",
    "\n",
    "    sales = sales.iloc[:, 6:].values\n",
    "\n",
    "    sell_prices['id'] = sell_prices['store_id'] + '_' + sell_prices['item_id']\n",
    "    sell_prices = sell_prices[sell_prices['wm_yr_wk'] <= 11613]\n",
    "    sell_prices = sell_prices.pivot(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "    sell_prices = sell_prices.values\n",
    "\n",
    "    N, h = y_true.shape \n",
    "    w = sell_prices.shape[1]  \n",
    "\n",
    "    daily_prices = np.repeat(sell_prices, repeats=7, axis=1)[:, -sales.shape[1]:]\n",
    "    daily_prices = np.where(np.isnan(daily_prices), np.nan, daily_prices)\n",
    "    \n",
    "    squared_errors = np.mean((y_true - y_pred) ** 2, axis=1)\n",
    "    scale = np.mean(np.diff(sales, axis=1) ** 2, axis=1)\n",
    "    rmsse = np.sqrt(squared_errors / (scale + 1e-10))\n",
    "\n",
    "    total_revenue = np.nansum(sales[:, -28:] * daily_prices[:, -28:], axis=1) \n",
    "    weight = total_revenue / np.nansum(total_revenue) \n",
    "\n",
    "    wrmsse = np.nansum(weight * rmsse)\n",
    "    \n",
    "    return wrmsse\n",
    "\n",
    "calculate_wrmsse(test_labels, test_forecasts) # 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

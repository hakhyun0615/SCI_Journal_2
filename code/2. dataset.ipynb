{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.bool = np.bool_\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from utils import highlight_print, reduce_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPreparing dataset for level 1\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 2\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 3\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 4\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 5\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 6\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 7\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 8\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 9\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 10\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 11\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def prepare_datasets(save_dir='../dataset/else'):\n",
    "    levels = [\n",
    "        [],                        # Level 1: Total\n",
    "        ['state_id'],              # Level 2: State\n",
    "        ['store_id'],              # Level 3: Store\n",
    "        ['cat_id'],                # Level 4: Category\n",
    "        ['dept_id'],               # Level 5: Department\n",
    "        ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "        ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "        ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "        ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "        ['item_id'],               # Level 10: Item\n",
    "        ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "        ['item_id', 'store_id']    # Level 12: Individual\n",
    "    ]\n",
    "\n",
    "    for level_idx, level in enumerate(levels, start=1):\n",
    "        highlight_print(f\"Preparing dataset for level {level_idx}\")\n",
    "        datasets = {'train': {}, 'test': {}}\n",
    "\n",
    "        # load data\n",
    "        agg_df = pd.read_csv(f'../data/preprocessed/agg_df_level_{level_idx}.csv')\n",
    "        calendar_df = pd.read_csv('../data/preprocessed/calendar_df.csv')\n",
    "\n",
    "        # reduce memory\n",
    "        agg_df = reduce_memory(agg_df)\n",
    "        calendar_df = reduce_memory(calendar_df)\n",
    "\n",
    "        # convert to datetime\n",
    "        start_date = pd.to_datetime('2011-01-29')\n",
    "        valid_start_date = pd.to_datetime('2016-03-28')\n",
    "        agg_df['d'] = agg_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        agg_df['d'] = start_date + pd.to_timedelta(agg_df['d'], unit='D')\n",
    "        calendar_df['d'] = calendar_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        calendar_df['d'] = start_date + pd.to_timedelta(calendar_df['d'], unit='D')\n",
    "        \n",
    "        # create id (group)\n",
    "        if len(level) == 0:\n",
    "            agg_df.insert(1, 'id', 'total')\n",
    "        elif len(level) == 1: \n",
    "            agg_df.insert(1, 'id', agg_df[level[0]])\n",
    "            del agg_df[level[0]]\n",
    "        elif len(level) > 1:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]] + '_' + agg_df[level[1]])\n",
    "            del agg_df[level[0]]\n",
    "            del agg_df[level[1]]\n",
    "\n",
    "        # id (group) encoding\n",
    "        groups = agg_df['id'].unique()\n",
    "        group_encoder = {group: group_idx for group_idx, group in enumerate(groups)}\n",
    " \n",
    "        # merge\n",
    "        group_df = agg_df.merge(calendar_df, on=\"d\", how=\"left\")\n",
    "\n",
    "        # datasets\n",
    "        train_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:-56],\n",
    "                    FieldName.START: pd.Period(start_date, freq=\"1D\"),\n",
    "                    FieldName.FEAT_STATIC_CAT: [group_encoder[id]],\n",
    "                    FieldName.FEAT_DYNAMIC_REAL: group[[\n",
    "                        'sales_mean', 'sales_std', 'sales_max', 'sales_min', 'sales_diff_mean', \n",
    "                        'sales_lag1_mean', 'sales_lag7_mean', 'sales_lag28_mean', \n",
    "                        'sales_rolling7_mean', 'sales_rolling28_mean', 'sales_rolling7_diff_mean', 'sales_rolling28_diff_mean', \n",
    "                        'release_mean', 'out_of_stock_mean', \n",
    "                        'sell_price_mean', 'sell_price_std', 'sell_price_max', 'sell_price_min', 'sell_price_diff_mean',\n",
    "                        'sell_price_lag_mean', 'sell_price_rolling_mean', 'sell_price_rolling_diff_mean',\n",
    "                        'sell_price_in_store_mean',\n",
    "                        \"year_delta\", \"quarter_sin\", \"quarter_cos\", \"month_sin\", \"month_cos\",  \n",
    "                        \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n",
    "                        'event_count'\n",
    "                    ]].values[:-56].T,\n",
    "                    FieldName.FEAT_DYNAMIC_CAT: group[[\n",
    "                        'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                        'event_name_1_enc', 'event_name_2_enc', \n",
    "                        'event_type_1_enc', 'event_type_2_enc'\n",
    "                    ]].values[:-56].T,\n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )\n",
    "        valid_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:-28],\n",
    "                    FieldName.START: pd.Period(valid_start_date, freq=\"1D\"),\n",
    "                    FieldName.FEAT_STATIC_CAT: [group_encoder[id]],\n",
    "                    FieldName.FEAT_DYNAMIC_REAL: group[[\n",
    "                        'sales_mean', 'sales_std', 'sales_max', 'sales_min', 'sales_diff_mean', \n",
    "                        'sales_lag1_mean', 'sales_lag7_mean', 'sales_lag28_mean', \n",
    "                        'sales_rolling7_mean', 'sales_rolling28_mean', 'sales_rolling7_diff_mean', 'sales_rolling28_diff_mean', \n",
    "                        'release_mean', 'out_of_stock_mean', \n",
    "                        'sell_price_mean', 'sell_price_std', 'sell_price_max', 'sell_price_min', 'sell_price_diff_mean',\n",
    "                        'sell_price_lag_mean', 'sell_price_rolling_mean', 'sell_price_rolling_diff_mean',\n",
    "                        'sell_price_in_store_mean',\n",
    "                        \"year_delta\", \"quarter_sin\", \"quarter_cos\", \"month_sin\", \"month_cos\",  \n",
    "                        \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n",
    "                        'event_count'\n",
    "                    ]].values[:-28].T,\n",
    "                    FieldName.FEAT_DYNAMIC_CAT: group[[\n",
    "                        'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                        'event_name_1_enc', 'event_name_2_enc', \n",
    "                        'event_type_1_enc', 'event_type_2_enc'\n",
    "                    ]].values[:-28].T,\n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )\n",
    "        test_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:], \n",
    "                    FieldName.START: pd.Period(start_date, freq=\"1D\"),\n",
    "                    FieldName.FEAT_STATIC_CAT: [group_encoder[id]],\n",
    "                    FieldName.FEAT_DYNAMIC_REAL: group[[\n",
    "                        'sales_mean', 'sales_std', 'sales_max', 'sales_min', 'sales_diff_mean', \n",
    "                        'sales_lag1_mean', 'sales_lag7_mean', 'sales_lag28_mean', \n",
    "                        'sales_rolling7_mean', 'sales_rolling28_mean', 'sales_rolling7_diff_mean', 'sales_rolling28_diff_mean', \n",
    "                        'release_mean', 'out_of_stock_mean', \n",
    "                        'sell_price_mean', 'sell_price_std', 'sell_price_max', 'sell_price_min', 'sell_price_diff_mean',\n",
    "                        'sell_price_lag_mean', 'sell_price_rolling_mean', 'sell_price_rolling_diff_mean',\n",
    "                        'sell_price_in_store_mean',\n",
    "                        \"year_delta\", \"quarter_sin\", \"quarter_cos\", \"month_sin\", \"month_cos\",  \n",
    "                        \"day_sin\", \"day_cos\", \"weekday_sin\", \"weekday_cos\",\n",
    "                        'event_count'\n",
    "                    ]].values[:].T,  \n",
    "                    FieldName.FEAT_DYNAMIC_CAT: group[[\n",
    "                        'snap_CA', 'snap_TX', 'snap_WI', \n",
    "                        'event_name_1_enc', 'event_name_2_enc', \n",
    "                        'event_type_1_enc', 'event_type_2_enc'\n",
    "                    ]].values[:].T,  \n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )   \n",
    "        \n",
    "        datasets['train'] = train_dataset\n",
    "        datasets['valid'] = valid_dataset\n",
    "        datasets['test'] = test_dataset\n",
    "\n",
    "        # save\n",
    "        with open(os.path.join(save_dir, f'dataset_level_{level_idx}.pkl'), 'wb') as f:\n",
    "            pickle.dump(datasets, f)\n",
    "\n",
    "        # reduce memory\n",
    "        del agg_df\n",
    "        del calendar_df\n",
    "        del group_df\n",
    "        del train_dataset\n",
    "        del valid_dataset\n",
    "        del test_dataset\n",
    "        del datasets\n",
    "\n",
    "prepare_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mPreparing dataset for level 1\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 2\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 3\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 4\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 5\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 6\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 7\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 8\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 9\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 10\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 11\u001b[0m\n",
      "\u001b[93mPreparing dataset for level 12\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def prepare_tft_datasets(save_dir='../dataset/tft'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    levels = [\n",
    "        [],                        # Level 1: Total\n",
    "        ['state_id'],              # Level 2: State\n",
    "        ['store_id'],              # Level 3: Store\n",
    "        ['cat_id'],                # Level 4: Category\n",
    "        ['dept_id'],               # Level 5: Department\n",
    "        ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "        ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "        ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "        ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "        ['item_id'],               # Level 10: Item\n",
    "        ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "        ['item_id', 'store_id']    # Level 12: Individual\n",
    "    ]\n",
    "\n",
    "    for level_idx, level in enumerate(levels, start=1):\n",
    "        highlight_print(f\"Preparing dataset for level {level_idx}\")\n",
    "        datasets = {'train': {}, 'test': {}}\n",
    "\n",
    "        # load data\n",
    "        agg_df = pd.read_csv(f'../data/preprocessed/agg_df_level_{level_idx}.csv')\n",
    "        calendar_df = pd.read_csv('../data/preprocessed/calendar_df.csv')\n",
    "\n",
    "        # reduce memory\n",
    "        agg_df = reduce_memory(agg_df)\n",
    "        calendar_df = reduce_memory(calendar_df)\n",
    "\n",
    "        # convert to datetime\n",
    "        start_date = pd.to_datetime('2011-01-29')\n",
    "        valid_start_date = pd.to_datetime('2016-03-28')\n",
    "        agg_df['d'] = agg_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        agg_df['d'] = start_date + pd.to_timedelta(agg_df['d'], unit='D')\n",
    "        calendar_df['d'] = calendar_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        calendar_df['d'] = start_date + pd.to_timedelta(calendar_df['d'], unit='D')\n",
    "        \n",
    "        # create id (group)\n",
    "        if len(level) == 0:\n",
    "            agg_df.insert(1, 'id', 'total')\n",
    "        elif len(level) == 1: \n",
    "            agg_df.insert(1, 'id', agg_df[level[0]])\n",
    "            del agg_df[level[0]]\n",
    "        elif len(level) > 1:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]] + '_' + agg_df[level[1]])\n",
    "            del agg_df[level[0]]\n",
    "            del agg_df[level[1]]\n",
    "\n",
    "        # id (group) encoding\n",
    "        groups = agg_df['id'].unique()\n",
    "        group_encoder = {group: group_idx for group_idx, group in enumerate(groups)}\n",
    " \n",
    "        # merge\n",
    "        group_df = agg_df.merge(calendar_df, on=\"d\", how=\"left\")\n",
    "\n",
    "        # datasets\n",
    "        train_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:-56],\n",
    "                    FieldName.START: pd.Period(start_date, freq=\"1D\"),\n",
    "                    'id': [group_encoder[id]],\n",
    "                    \"sales_mean\": group[\"sales_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_std\": group[\"sales_std\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_max\": group[\"sales_max\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_min\": group[\"sales_min\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_diff_mean\": group[\"sales_diff_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_lag1_mean\": group[\"sales_lag1_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_lag7_mean\": group[\"sales_lag7_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_lag28_mean\": group[\"sales_lag28_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_rolling7_mean\": group[\"sales_rolling7_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_rolling28_mean\": group[\"sales_rolling28_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_rolling7_diff_mean\": group[\"sales_rolling7_diff_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sales_rolling28_diff_mean\": group[\"sales_rolling28_diff_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"release_mean\": group[\"release_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"out_of_stock_mean\": group[\"out_of_stock_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_mean\": group[\"sell_price_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_std\": group[\"sell_price_std\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_max\": group[\"sell_price_max\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_min\": group[\"sell_price_min\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_diff_mean\": group[\"sell_price_diff_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_lag_mean\": group[\"sell_price_lag_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_rolling_mean\": group[\"sell_price_rolling_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_rolling_diff_mean\": group[\"sell_price_rolling_diff_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"sell_price_in_store_mean\": group[\"sell_price_in_store_mean\"].values[:-56].reshape(1, -1),\n",
    "                    \"year_delta\": group[\"year_delta\"].values[:-56].reshape(1, -1),\n",
    "                    \"quarter_sin\": group[\"quarter_sin\"].values[:-56].reshape(1, -1),\n",
    "                    \"quarter_cos\": group[\"quarter_cos\"].values[:-56].reshape(1, -1),\n",
    "                    \"month_sin\": group[\"month_sin\"].values[:-56].reshape(1, -1),\n",
    "                    \"month_cos\": group[\"month_cos\"].values[:-56].reshape(1, -1),\n",
    "                    \"day_sin\": group[\"day_sin\"].values[:-56].reshape(1, -1),\n",
    "                    \"day_cos\": group[\"day_cos\"].values[:-56].reshape(1, -1),\n",
    "                    \"weekday_sin\": group[\"weekday_sin\"].values[:-56].reshape(1, -1),\n",
    "                    \"weekday_cos\": group[\"weekday_cos\"].values[:-56].reshape(1, -1),\n",
    "                    \"event_count\": group[\"event_count\"].values[:-56].reshape(1, -1),\n",
    "                    'snap_CA': group['snap_CA'].values[:-56],\n",
    "                    'snap_TX': group['snap_TX'].values[:-56],\n",
    "                    'snap_WI': group['snap_WI'].values[:-56],\n",
    "                    'event_name_1_enc': group['event_name_1_enc'].values[:-56],\n",
    "                    'event_name_2_enc': group['event_name_2_enc'].values[:-56],\n",
    "                    'event_type_1_enc': group['event_type_1_enc'].values[:-56],\n",
    "                    'event_type_2_enc': group['event_type_2_enc'].values[:-56]\n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )\n",
    "\n",
    "        valid_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:-28],\n",
    "                    FieldName.START: pd.Period(valid_start_date, freq=\"1D\"),\n",
    "                    'id': [group_encoder[id]],\n",
    "                    \"sales_mean\": group[\"sales_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_std\": group[\"sales_std\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_max\": group[\"sales_max\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_min\": group[\"sales_min\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_diff_mean\": group[\"sales_diff_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_lag1_mean\": group[\"sales_lag1_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_lag7_mean\": group[\"sales_lag7_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_lag28_mean\": group[\"sales_lag28_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_rolling7_mean\": group[\"sales_rolling7_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_rolling28_mean\": group[\"sales_rolling28_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_rolling7_diff_mean\": group[\"sales_rolling7_diff_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sales_rolling28_diff_mean\": group[\"sales_rolling28_diff_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"release_mean\": group[\"release_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"out_of_stock_mean\": group[\"out_of_stock_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_mean\": group[\"sell_price_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_std\": group[\"sell_price_std\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_max\": group[\"sell_price_max\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_min\": group[\"sell_price_min\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_diff_mean\": group[\"sell_price_diff_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_lag_mean\": group[\"sell_price_lag_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_rolling_mean\": group[\"sell_price_rolling_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_rolling_diff_mean\": group[\"sell_price_rolling_diff_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"sell_price_in_store_mean\": group[\"sell_price_in_store_mean\"].values[:-28].reshape(1, -1),\n",
    "                    \"year_delta\": group[\"year_delta\"].values[:-28].reshape(1, -1),\n",
    "                    \"quarter_sin\": group[\"quarter_sin\"].values[:-28].reshape(1, -1),\n",
    "                    \"quarter_cos\": group[\"quarter_cos\"].values[:-28].reshape(1, -1),\n",
    "                    \"month_sin\": group[\"month_sin\"].values[:-28].reshape(1, -1),\n",
    "                    \"month_cos\": group[\"month_cos\"].values[:-28].reshape(1, -1),\n",
    "                    \"day_sin\": group[\"day_sin\"].values[:-28].reshape(1, -1),\n",
    "                    \"day_cos\": group[\"day_cos\"].values[:-28].reshape(1, -1),\n",
    "                    \"weekday_sin\": group[\"weekday_sin\"].values[:28].reshape(1, -1),\n",
    "                    \"weekday_cos\": group[\"weekday_cos\"].values[:-28].reshape(1, -1),\n",
    "                    \"event_count\": group[\"event_count\"].values[:-28].reshape(1, -1),\n",
    "                    'snap_CA': group['snap_CA'].values[:-28],\n",
    "                    'snap_TX': group['snap_TX'].values[:-28],\n",
    "                    'snap_WI': group['snap_WI'].values[:-28],\n",
    "                    'event_name_1_enc': group['event_name_1_enc'].values[:-28],\n",
    "                    'event_name_2_enc': group['event_name_2_enc'].values[:-28],\n",
    "                    'event_type_1_enc': group['event_type_1_enc'].values[:-28],\n",
    "                    'event_type_2_enc': group['event_type_2_enc'].values[:-28]\n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )\n",
    "\n",
    "        test_dataset = ListDataset(\n",
    "            [\n",
    "                {\n",
    "                    FieldName.ITEM_ID: id,\n",
    "                    FieldName.TARGET: group[\"sales_sum\"].values[:], \n",
    "                    FieldName.START: pd.Period(start_date, freq=\"1D\"),\n",
    "                    'id': [group_encoder[id]],\n",
    "                    \"sales_mean\": group[\"sales_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_std\": group[\"sales_std\"].values[:].reshape(1, -1),\n",
    "                    \"sales_max\": group[\"sales_max\"].values[:].reshape(1, -1),\n",
    "                    \"sales_min\": group[\"sales_min\"].values[:].reshape(1, -1),\n",
    "                    \"sales_diff_mean\": group[\"sales_diff_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_lag1_mean\": group[\"sales_lag1_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_lag7_mean\": group[\"sales_lag7_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_lag28_mean\": group[\"sales_lag28_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_rolling7_mean\": group[\"sales_rolling7_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_rolling28_mean\": group[\"sales_rolling28_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_rolling7_diff_mean\": group[\"sales_rolling7_diff_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sales_rolling28_diff_mean\": group[\"sales_rolling28_diff_mean\"].values[:].reshape(1, -1),\n",
    "                    \"release_mean\": group[\"release_mean\"].values[:].reshape(1, -1),\n",
    "                    \"out_of_stock_mean\": group[\"out_of_stock_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_mean\": group[\"sell_price_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_std\": group[\"sell_price_std\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_max\": group[\"sell_price_max\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_min\": group[\"sell_price_min\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_diff_mean\": group[\"sell_price_diff_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_lag_mean\": group[\"sell_price_lag_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_rolling_mean\": group[\"sell_price_rolling_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_rolling_diff_mean\": group[\"sell_price_rolling_diff_mean\"].values[:].reshape(1, -1),\n",
    "                    \"sell_price_in_store_mean\": group[\"sell_price_in_store_mean\"].values[:].reshape(1, -1),\n",
    "                    \"year_delta\": group[\"year_delta\"].values[:].reshape(1, -1),\n",
    "                    \"quarter_sin\": group[\"quarter_sin\"].values[:].reshape(1, -1),\n",
    "                    \"quarter_cos\": group[\"quarter_cos\"].values[:].reshape(1, -1),\n",
    "                    \"month_sin\": group[\"month_sin\"].values[:].reshape(1, -1),\n",
    "                    \"month_cos\": group[\"month_cos\"].values[:].reshape(1, -1),\n",
    "                    \"day_sin\": group[\"day_sin\"].values[:].reshape(1, -1),\n",
    "                    \"day_cos\": group[\"day_cos\"].values[:].reshape(1, -1),\n",
    "                    \"weekday_sin\": group[\"weekday_sin\"].values[:].reshape(1, -1),\n",
    "                    \"weekday_cos\": group[\"weekday_cos\"].values[:].reshape(1, -1),\n",
    "                    \"event_count\": group[\"event_count\"].values[:].reshape(1, -1),\n",
    "                    'snap_CA': group['snap_CA'].values[:],\n",
    "                    'snap_TX': group['snap_TX'].values[:],\n",
    "                    'snap_WI': group['snap_WI'].values[:],\n",
    "                    'event_name_1_enc': group['event_name_1_enc'].values[:],\n",
    "                    'event_name_2_enc': group['event_name_2_enc'].values[:],\n",
    "                    'event_type_1_enc': group['event_type_1_enc'].values[:],\n",
    "                    'event_type_2_enc': group['event_type_2_enc'].values[:]\n",
    "                }\n",
    "                for id, group in group_df.groupby(\"id\")\n",
    "            ],\n",
    "            freq=\"D\",\n",
    "        )   \n",
    "        \n",
    "        datasets['train'] = train_dataset\n",
    "        datasets['valid'] = valid_dataset\n",
    "        datasets['test'] = test_dataset\n",
    "    \n",
    "        # save\n",
    "        with open(os.path.join(save_dir, f'dataset_level_{level_idx}.pkl'), 'wb') as f:\n",
    "            pickle.dump(datasets, f)\n",
    "\n",
    "        # reduce memory\n",
    "        del agg_df\n",
    "        del calendar_df\n",
    "        del group_df\n",
    "        del train_dataset\n",
    "        del valid_dataset\n",
    "        del test_dataset\n",
    "        del datasets\n",
    "\n",
    "prepare_tft_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lgb_datasets(save_dir='../dataset/lgb'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    levels = [\n",
    "        [],                        # Level 1: Total\n",
    "        ['state_id'],              # Level 2: State\n",
    "        ['store_id'],              # Level 3: Store\n",
    "        ['cat_id'],                # Level 4: Category\n",
    "        ['dept_id'],               # Level 5: Department\n",
    "        ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "        ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "        ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "        ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "        ['item_id'],               # Level 10: Item\n",
    "        ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "        ['item_id', 'store_id']    # Level 12: Individual\n",
    "    ]\n",
    "    \n",
    "    for level_idx, level in enumerate(levels, start=1):\n",
    "        highlight_print(f\"Preparing dataset for level {level_idx}\")\n",
    "        \n",
    "        # load data\n",
    "        agg_df = pd.read_csv(f'../data/preprocessed/agg_df_level_{level_idx}.csv')\n",
    "        calendar_df = pd.read_csv('../data/preprocessed/calendar_df.csv')\n",
    "        \n",
    "        # reduce memory\n",
    "        agg_df = reduce_memory(agg_df)\n",
    "        calendar_df = reduce_memory(calendar_df)\n",
    "        \n",
    "        # convert dates\n",
    "        start_date = pd.to_datetime('2011-01-29')\n",
    "        valid_start_date = pd.to_datetime('2016-03-28')\n",
    "        agg_df['d'] = agg_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        agg_df['d'] = start_date + pd.to_timedelta(agg_df['d'], unit='D')\n",
    "        calendar_df['d'] = calendar_df['d'].apply(lambda x: int(x.split('_')[1]) - 1)\n",
    "        calendar_df['d'] = start_date + pd.to_timedelta(calendar_df['d'], unit='D')\n",
    "        \n",
    "        # create id (group)\n",
    "        if len(level) == 0:\n",
    "            agg_df.insert(1, 'id', 'total')\n",
    "        elif len(level) == 1:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]])\n",
    "            del agg_df[level[0]]\n",
    "        else:\n",
    "            agg_df.insert(1, 'id', agg_df[level[0]] + '_' + agg_df[level[1]])\n",
    "            del agg_df[level[0]]\n",
    "            del agg_df[level[1]]\n",
    "            \n",
    "        # merge\n",
    "        df = agg_df.merge(calendar_df, on=\"d\", how=\"left\")\n",
    "        \n",
    "        # create time-based features\n",
    "        df['year'] = df['d'].dt.year\n",
    "        df['month'] = df['d'].dt.month\n",
    "        df['week'] = df['d'].dt.isocalendar().week\n",
    "        df['day'] = df['d'].dt.day\n",
    "        df['dayofweek'] = df['d'].dt.dayofweek\n",
    "        df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "        \n",
    "        # Create lag features specific\n",
    "        groups = df.groupby('id')\n",
    "        for lag in [1, 7, 14, 28]:\n",
    "            df[f'sales_lag_{lag}'] = groups['sales_sum'].transform(lambda x: x.shift(lag))\n",
    "            \n",
    "        # create rolling features\n",
    "        for window in [7, 14, 28]:\n",
    "            df[f'sales_rolling_mean_{window}'] = groups['sales_sum'].transform(\n",
    "                lambda x: x.shift(1).rolling(window=window).mean())\n",
    "            df[f'sales_rolling_std_{window}'] = groups['sales_sum'].transform(\n",
    "                lambda x: x.shift(1).rolling(window=window).std())\n",
    "        \n",
    "        # split datasets\n",
    "        train_df = df[df['d'] < valid_start_date].copy()\n",
    "        valid_df = df[(df['d'] >= valid_start_date) & (df['d'] < valid_start_date + pd.Timedelta(days=28))].copy()\n",
    "        test_df = df.copy()\n",
    "        \n",
    "        # drop unnecessary columns and handle missing values\n",
    "        drop_cols = ['d'] + [col for col in df.columns if 'enc' in col]\n",
    "        train_df = train_df.drop(columns=drop_cols).fillna(0)\n",
    "        valid_df = valid_df.drop(columns=drop_cols).fillna(0)\n",
    "        test_df = test_df.drop(columns=drop_cols).fillna(0)\n",
    "        \n",
    "        # datasets\n",
    "        datasets = {\n",
    "            'train': {\n",
    "                'data': train_df,\n",
    "                'target': train_df['sales_sum'],\n",
    "                'groups': train_df['id']\n",
    "            },\n",
    "            'valid': {\n",
    "                'data': valid_df,\n",
    "                'target': valid_df['sales_sum'],\n",
    "                'groups': valid_df['id']\n",
    "            },\n",
    "            'test': {\n",
    "                'data': test_df,\n",
    "                'target': test_df['sales_sum'],\n",
    "                'groups': test_df['id']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # save\n",
    "        with open(os.path.join(save_dir, f'dataset_level_{level_idx}.pkl'), 'wb') as f:\n",
    "            pickle.dump(datasets, f)\n",
    "            \n",
    "        # reduce memory\n",
    "        del agg_df, calendar_df, df, train_df, valid_df, test_df, datasets\n",
    "\n",
    "prepare_lgb_datasets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

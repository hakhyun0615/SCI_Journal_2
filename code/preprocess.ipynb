{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def reduce_mem(df, verbose=True):\n",
    "    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "    start_mem = df.memory_usage().sum() / 1024**2    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtypes\n",
    "        if col_type in numerics:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)    \n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_dir = '../data/original'\n",
    "save_data_dir = '../data/preprocessed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem. usage decreased to 96.13 Mb (78.8% reduction)\n",
      "Mem. usage decreased to 130.48 Mb (37.5% reduction)\n",
      "Mem. usage decreased to  0.12 Mb (41.9% reduction)\n"
     ]
    }
   ],
   "source": [
    "sales = pd.read_csv(f'{input_data_dir}/sales_train_evaluation.csv')\n",
    "sell_prices = pd.read_csv(f'{input_data_dir}/sell_prices.csv')\n",
    "calendar = pd.read_csv(f'{input_data_dir}/calendar.csv')\n",
    "\n",
    "# reduce memory usage\n",
    "sales = reduce_mem(sales)\n",
    "sell_prices = reduce_mem(sell_prices)\n",
    "calendar = reduce_mem(calendar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar.date = pd.to_datetime(calendar.date)\n",
    "\n",
    "# \n",
    "calendar['year_delta'] = 2016 - calendar.year\n",
    "\n",
    "# cyclic encodings\n",
    "calendar['quarter_sin'] = np.sin(2 * np.pi * calendar.date.dt.quarter/4.0)\n",
    "calendar['quarter_cos'] = np.cos(2 * np.pi * calendar.date.dt.quarter/4.0)\n",
    "calendar['month_sin'] = np.sin(2 * np.pi * calendar.month/12.0)\n",
    "calendar['month_cos'] = np.cos(2 * np.pi * calendar.month/12.0)\n",
    "calendar['day_sin'] = np.sin(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
    "calendar['day_cos'] = np.cos(2 * np.pi * calendar.date.dt.day/calendar.date.dt.days_in_month)\n",
    "calendar['weekday_sin'] = np.sin(2 * np.pi * calendar.wday/7.0)\n",
    "calendar['weekday_cos'] = np.cos(2 * np.pi * calendar.wday/7.0)\n",
    "\n",
    "# event count\n",
    "calendar['event_count'] = calendar[['event_name_1', 'event_name_2']].notna().sum(axis=1)\n",
    "\n",
    "# event encodings\n",
    "event_names = ['event_name_1', 'event_name_2']\n",
    "event_names_enc = ['event_name_1_enc', 'event_name_2_enc']\n",
    "calendar[event_names_enc] = calendar[event_names]\n",
    "event_names_encoder = ce.OrdinalEncoder(cols=event_names_enc)\n",
    "event_names_encoder.fit(calendar)\n",
    "event_names_encoder.mapping[1]['mapping'] = event_names_encoder.mapping[0]['mapping']\n",
    "calendar = event_names_encoder.transform(calendar)\n",
    "for col in event_names_enc:\n",
    "    calendar[col] = calendar[col] - 1\n",
    "\n",
    "event_types = ['event_type_1', 'event_type_2']\n",
    "event_types_enc = ['event_type_1_enc', 'event_type_2_enc']\n",
    "calendar[event_types_enc] = calendar[event_types]\n",
    "event_type_encoder = ce.OrdinalEncoder(cols=event_types_enc)\n",
    "event_type_encoder.fit(calendar)\n",
    "event_type_encoder.mapping[1]['mapping'] = event_type_encoder.mapping[0]['mapping']\n",
    "calendar = event_type_encoder.transform(calendar)\n",
    "for col in event_types_enc:\n",
    "    calendar[col] = calendar[col] - 1\n",
    "\n",
    "#\n",
    "calendar_df = calendar[['wm_yr_wk', 'd', 'snap_CA', 'snap_TX', 'snap_WI', 'year_delta',\n",
    "                        'quarter_sin', 'quarter_cos', 'month_sin', 'month_cos', \n",
    "                        'day_sin', 'day_cos', 'weekday_sin', 'weekday_cos', 'event_count']\n",
    "                        + event_names_enc \n",
    "                        + event_types_enc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "del sales['id']\n",
    "\n",
    "# release\n",
    "release = sell_prices.groupby(['item_id', 'store_id'])['wm_yr_wk'].min().reset_index()\n",
    "release['release'] = 1\n",
    "sell_prices = sell_prices.merge(right=release[['item_id', 'store_id', 'release']], on=['item_id', 'store_id'], how='left')\n",
    "sell_prices['release'] = sell_prices['release'].fillna(0)\n",
    "\n",
    "# relative sell price\n",
    "sell_prices['sell_price_in_store'] = sell_prices['sell_price'] / sell_prices.groupby(['store_id', 'wm_yr_wk'])['sell_price'].transform('mean')\n",
    "\n",
    "# diff\n",
    "sell_prices['sell_price_diff'] = sell_prices['sell_price'].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "data_df = pd.melt(sales, id_vars=['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_vars=['d_'+str(i) for i in range(1, 1942)], value_name='sales')\n",
    "\n",
    "# merge\n",
    "data_df = data_df.merge(right=calendar_df[['d', 'wm_yr_wk']], on=['d'], how='left')\n",
    "data_df = data_df.merge(right=sell_prices[['item_id', 'store_id', 'wm_yr_wk', 'sell_price', 'release', 'sell_price_in_store', 'sell_price_diff']], on=['item_id', 'store_id', 'wm_yr_wk'], how='left')\n",
    "\n",
    "# fill nan 0 (before release)\n",
    "data_df.release = data_df.release.fillna(0.0)\n",
    "data_df.sell_price = data_df.sell_price.fillna(0.0)\n",
    "\n",
    "# accumulate after release\n",
    "data_df['release'] = data_df.groupby(['item_id', 'store_id'])['release'].transform(lambda x: (x > 0).cumsum())\n",
    "\n",
    "# out of stock\n",
    "def count_consecutive_zeros(group):\n",
    "    zeros = group['sales'] == 0\n",
    "    reset = ~zeros\n",
    "    groups = reset.cumsum()\n",
    "    result = zeros.groupby(groups).cumsum()\n",
    "    result = result.where(zeros, 0)\n",
    "    return result\n",
    "data_df['out_of_stock'] = data_df.groupby(['item_id', 'store_id']).apply(lambda x: count_consecutive_zeros(x)).reset_index(drop=True)\n",
    "\n",
    "#\n",
    "del data_df['wm_yr_wk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "del calendar_df['wm_yr_wk']\n",
    "\n",
    "#\n",
    "calendar_df.to_csv(f'{save_data_dir}/calendar_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [4:18:18<00:00, 1291.56s/it]  \n"
     ]
    }
   ],
   "source": [
    "levels = [\n",
    "    [],                        # Level 1: Total\n",
    "    ['state_id'],              # Level 2: State\n",
    "    ['store_id'],              # Level 3: Store\n",
    "    ['cat_id'],                # Level 4: Category\n",
    "    ['dept_id'],               # Level 5: Department\n",
    "    ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "    ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "    ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "    ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "    ['item_id'],               # Level 10: Item\n",
    "    ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "    ['item_id', 'store_id']    # Level 12: Individual\n",
    "]\n",
    "\n",
    "agg_funcs = {\n",
    "    'sales': [ \n",
    "        ('sales_sum', 'sum'), # 판매량 합계\n",
    "        ('sales_mean', 'mean'), # 판매량 평균값\n",
    "        ('sales_std', 'std'), # 판매량 표준편차\n",
    "        ('sales_max', 'max'), # 판매량 최대값\n",
    "        ('sales_min', 'min'), # 판매량 최소값\n",
    "        ('sales_lag1', lambda x: x.shift(1).iloc[-1]), # 1일 전 판매량\n",
    "        ('sales_lag7', lambda x: x.shift(7).iloc[-1]), # 7일 전 판매량\n",
    "        ('sales_lag28', lambda x: x.shift(28).iloc[-1]), # 28일 전 판매량\n",
    "        ('sales_rolling7_mean', lambda x: x.rolling(window=7, min_periods=1).mean().iloc[-1]), # 최근 7일 이동 평균\n",
    "        ('sales_rolling28_mean', lambda x: x.rolling(window=28, min_periods=1).mean().iloc[-1]), # 최근 28일 이동 평균\n",
    "        ('sales_trend', lambda x: x.reset_index(drop=True).corr(pd.Series(range(len(x))))), # 판매량 추세\n",
    "     ],\n",
    "    'release': [('release_mean', 'mean')], # 최초 판매량의 평균값\n",
    "    'out_of_stock': [('out_of_stock_mean', 'mean')], # 재고 없음 평균값\n",
    "    'sell_price': [\n",
    "        ('sell_price_mean', 'mean'), # 평균 판매 가격\n",
    "        ('sell_price_std', 'std'), # 판매 가격 표준편차\n",
    "        ('sell_price_max', 'max'), # 판매 가격 최대값\n",
    "        ('sell_price_min', 'min'), # 판매 가격 최소값\n",
    "        ('sell_price_diff', 'mean'), # 판매 가격 변화량\n",
    "        ('sell_price_trend', lambda x: x.reset_index(drop=True).corr(pd.Series(range(len(x))))), # 판매 가격 추세\n",
    "    ],\n",
    "    'sell_price_in_store': [('sell_price_in_store_mean', 'mean')], # 메징 안에서의 상대적 판매 가격\n",
    "}\n",
    "\n",
    "for i, level in tqdm(enumerate(levels), total=len(levels)):\n",
    "    # aggregation\n",
    "    agg_cols = level + ['d'] if level else ['d']\n",
    "    agg_df = data_df.groupby(agg_cols).agg(**{\n",
    "        new_col: (col, func) \n",
    "        for col, aggs in agg_funcs.items() \n",
    "        for new_col, func in aggs\n",
    "    }).reset_index()\n",
    "\n",
    "    # sorting\n",
    "    agg_df['sort_key'] = agg_df['d'].str[2:].astype(int)\n",
    "    agg_df = agg_df.sort_values(['sort_key'] + level)\n",
    "    agg_df = agg_df.drop(columns=['sort_key']).reset_index(drop=True)\n",
    "    \n",
    "    agg_df.insert(0, 'level', i+1)\n",
    "    agg_df.to_csv(f'{save_data_dir}/agg_df_level_{i+1}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

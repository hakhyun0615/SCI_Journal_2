{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU Detected. Configuring GPU...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"\\033[92mGPU Detected. Configuring GPU...\\033[0m\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)  # 메모리 동적 할당\n",
    "else:\n",
    "    print(\"\\033[91mNo GPU detected. Running on CPU.\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 판매 시작 시점 \n",
    "with open('../data/preprocessed/first_sales_column_dict.pkl', 'rb') as f:\n",
    "    first_sales_column_dict = pickle.load(f)\n",
    "\n",
    "# 판매량 데이터 (아이템별 상이)\n",
    "detrended_sales = pd.read_csv(\"../data/loess/detrended_sales.csv\")\n",
    "# 판매량 데이터의 주기, snr (아이템별 상이)\n",
    "with open('../data/fourier/results.pkl', 'rb') as f:\n",
    "    fourier_results = pickle.load(f)\n",
    "\n",
    "# 판매 가격 데이터 (아이템별 상이)\n",
    "log_differenced_sell_prices = pd.read_csv(\"../data/log_differencing/log_differenced_sell_prices.csv\")\n",
    "# 주말 여부 데이터 (1941일)\n",
    "is_weekend = pd.read_csv(\"../data/preprocessed/is_weekend.csv\")\n",
    "# 이벤트 데이터 (1941일)\n",
    "event_type_cultural = pd.read_csv(\"../data/preprocessed/event_type_cultural.csv\")\n",
    "event_type_national = pd.read_csv(\"../data/preprocessed/event_type_national.csv\")\n",
    "event_type_religious = pd.read_csv(\"../data/preprocessed/event_type_religious.csv\")\n",
    "event_type_sporting = pd.read_csv(\"../data/preprocessed/event_type_sporting.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_first_sale_day(state_item_id, first_sales_day):\n",
    "    # 주 ID, 아이템 ID\n",
    "    state_id, item_id = state_item_id\n",
    "\n",
    "    # 필요한 데이터 필터링 (한 번씩만 필터링)\n",
    "    sales_df = detrended_sales.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    price_df = log_differenced_sell_prices.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    \n",
    "    # 각 데이터의 슬라이싱 시작 인덱스 계산\n",
    "    start_col_sales = sales_df.index.get_loc(first_sales_day)\n",
    "    start_col_price = price_df.index.get_loc(first_sales_day)\n",
    "\n",
    "    # 각 데이터 슬라이싱\n",
    "    sales_data = sales_df.iloc[start_col_sales:].values\n",
    "    price_data = price_df.iloc[start_col_price:].values\n",
    "    \n",
    "    # 고정된 데이터 (weekend와 이벤트 데이터)\n",
    "    start_col_weekend_event = is_weekend.columns.get_loc(first_sales_day)\n",
    "\n",
    "    weekend_data = is_weekend.iloc[0, start_col_weekend_event:].values\n",
    "    cultural_data = event_type_cultural.iloc[0, start_col_weekend_event:].values\n",
    "    national_data = event_type_national.iloc[0, start_col_weekend_event:].values\n",
    "    religious_data = event_type_religious.iloc[0, start_col_weekend_event:].values\n",
    "    sporting_data = event_type_sporting.iloc[0, start_col_weekend_event:].values\n",
    "    \n",
    "    # 출력\n",
    "    return {\n",
    "        \"sales\": sales_data,\n",
    "        \"price\": price_data,\n",
    "        \"weekend\": weekend_data,\n",
    "        \"cultural\": cultural_data,\n",
    "        \"national\": national_data,\n",
    "        \"religious\": religious_data,\n",
    "        \"sporting\": sporting_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator_for_each_item(num_kernels, look_back_window_size, look_forward_window_size):\n",
    "    for state_item_id, first_sales_day in first_sales_column_dict.items(): # 아이템별로\n",
    "        data = sort_by_first_sale_day(state_item_id, first_sales_day)\n",
    "\n",
    "        kernel_sizes = [k for k in fourier_results[state_item_id]['selected_periods'][:num_kernels]]\n",
    "        kernel_strengths = [k for k in fourier_results[state_item_id]['selected_strengths'][:num_kernels]]\n",
    "        max_kernel_size = max(kernel_sizes)\n",
    "        look_back_window_size = max(look_back_window_size, max_kernel_size)\n",
    "\n",
    "        # 슬라이딩 윈도우 데이터 생성\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "        for i in range(len(data[\"sales\"]) - look_back_window_size - look_forward_window_size): # 슬라이딩 윈도우\n",
    "            ##### 슬라이딩 윈도우 간격\n",
    "            input = {\n",
    "                \"sales\": data[\"sales\"][i:i+look_back_window_size],\n",
    "                \"price\": data[\"price\"][i:i+look_back_window_size],\n",
    "                \"weekend\": data[\"weekend\"][i:i+look_back_window_size],\n",
    "                \"cultural\": data[\"cultural\"][i:i+look_back_window_size],\n",
    "                \"national\": data[\"national\"][i:i+look_back_window_size],\n",
    "                \"religious\": data[\"religious\"][i:i+look_back_window_size],\n",
    "                \"sporting\": data[\"sporting\"][i:i+look_back_window_size],\n",
    "            }\n",
    "            output = data[\"sales\"][i+look_back_window_size:i+look_back_window_size+look_forward_window_size]\n",
    "            input_data.append(input)\n",
    "            output_data.append(output)\n",
    "\n",
    "        yield input_data, output_data, kernel_sizes, kernel_strengths  # 아이템별로 훈련/테스트 데이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelWeights(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel_strengths):\n",
    "        super().__init__()\n",
    "        self.kernel_strengths = self.add_weight(\n",
    "            shape=(len(kernel_strengths),),\n",
    "            initializer=tf.constant_initializer(kernel_strengths),\n",
    "            trainable=True,\n",
    "            name=\"kernel_weights\"\n",
    "        )\n",
    "\n",
    "    def call(self, conv):\n",
    "        # 모든 conv 출력을 하나의 텐서로 stack\n",
    "        stacked_conv = tf.stack(conv, axis=-1)  # (batch, time, filters, num_kernels)\n",
    "        \n",
    "        # kernel_strengths를 Softmax로 정규화\n",
    "        weights = tf.nn.softmax(self.kernel_strengths)  # (num_kernels,)\n",
    "        \n",
    "        # 가중치 적용을 위해 차원 추가\n",
    "        expanded_weights = weights[tf.newaxis, tf.newaxis, tf.newaxis, :]  # (1, 1, 1, num_kernels)\n",
    "        \n",
    "        # 가중치 적용\n",
    "        weighted_conv = tf.multiply(stacked_conv, expanded_weights)  # (batch, time, filters, num_kernels)\n",
    "        \n",
    "        # num_kernels 축을 가중합으로 축소\n",
    "        reduced_output = tf.reduce_sum(weighted_conv, axis=-1)  # (batch, time, filters)\n",
    "        \n",
    "        return reduced_output\n",
    "\n",
    "\n",
    "class AuxWeights(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_aux):\n",
    "        super().__init__()\n",
    "        self.aux_weights = self.add_weight(\n",
    "            shape=(num_aux,),\n",
    "            initializer='ones',\n",
    "            trainable=True,\n",
    "            name='aux_weights'\n",
    "        )\n",
    "\n",
    "    def call(self, aux, non_zero_ratios):\n",
    "        # 입력 aux shape: (batch, time, 32, 6)\n",
    "        \n",
    "        # 비-제로 비율과 기본 가중치 결합\n",
    "        weights = self.aux_weights * non_zero_ratios  # (batch, 6)\n",
    "        \n",
    "        # Softmax로 정규화\n",
    "        weights = tf.nn.softmax(weights, axis=-1)  # (batch, 6)\n",
    "        \n",
    "        # weights를 (batch, 1, 1, 6) 형태로 확장\n",
    "        expanded_weights = tf.expand_dims(tf.expand_dims(weights, axis=1), axis=2)\n",
    "        \n",
    "        # 가중치 적용\n",
    "        weighted_aux = aux * expanded_weights  # (batch, time, 32, 6)\n",
    "        \n",
    "        # 마지막 차원(6)에 대해 sum\n",
    "        weighted_sum = tf.reduce_sum(weighted_aux, axis=-1)  # (batch, time, 32)\n",
    "        \n",
    "        return weighted_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BuildModel(tf.keras.Model):\n",
    "    def __init__(self, kernel_sizes, kernel_strengths, look_forward_window_size):\n",
    "        super().__init__()\n",
    "        self.kernel_weights_layer = KernelWeights(kernel_strengths)\n",
    "        self.aux_weights_layer = AuxWeights(6)  # 6개의 aux features\n",
    "        self.look_forward_window_size = look_forward_window_size\n",
    "        \n",
    "        # Conv1D layers for all features\n",
    "        self.convs = [\n",
    "            tf.keras.layers.Conv1D(\n",
    "                filters=32,\n",
    "                kernel_size=int(k),\n",
    "                padding='same'\n",
    "            ) for k in kernel_sizes\n",
    "        ]\n",
    "        \n",
    "        # Multi-head attention\n",
    "        self.feature_attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=4,\n",
    "            key_dim=32\n",
    "        )\n",
    "        \n",
    "        # Smoothing layers\n",
    "        self.smooth_conv = tf.keras.layers.Conv1D(\n",
    "            filters=32,\n",
    "            kernel_size=3,\n",
    "            padding='same',\n",
    "            activation='relu'\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = tf.keras.layers.LayerNormalization()\n",
    "        self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "        \n",
    "        # Final dense layers\n",
    "        self.dense1 = tf.keras.layers.Dense(64, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(look_forward_window_size, activation='relu')  # ReLU 추가\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        # Process sales with Conv1D and KernelWeights\n",
    "        sales_input = tf.expand_dims(inputs[\"sales\"], axis=-1)  # (batch_size, look_back_window_size, 1)\n",
    "        sales_conv_outputs = [conv(sales_input) for conv in self.convs] #  (batch_size, look_back_window_size, filters=32) * num_kernels\n",
    "        print('sales_conv_outputs', len(sales_conv_outputs))\n",
    "        print(sales_conv_outputs[0].shape)\n",
    "        sales_features = self.kernel_weights_layer(sales_conv_outputs)  # (batch, time, 32)\n",
    "        print('sales_features',len(sales_features))\n",
    "        print(sales_features[0].shape)\n",
    "        \n",
    "        # Process auxiliary features\n",
    "        feature_names = [\"price\", \"weekend\", \"cultural\", \"national\", \"religious\", \"sporting\"]\n",
    "        aux_inputs = tf.stack([inputs[name] for name in feature_names], axis=-1)  # (batch, time, 6)\n",
    "        \n",
    "        # Calculate non-zero ratios\n",
    "        non_zero_ratios = tf.reduce_mean(\n",
    "            tf.cast(aux_inputs != 0, tf.float32),\n",
    "            axis=1\n",
    "        )  # (batch, 6)\n",
    "        \n",
    "        # Process each aux feature\n",
    "        aux_conv_features = []\n",
    "        for i in range(aux_inputs.shape[-1]):\n",
    "            feature_input = tf.expand_dims(aux_inputs[..., i], axis=-1)\n",
    "            aux_conv_outputs = [conv(feature_input) for conv in self.convs]\n",
    "            conv_feature = self.kernel_weights_layer(aux_conv_outputs)\n",
    "            aux_conv_features.append(conv_feature)\n",
    "        \n",
    "        # Stack and weight aux features\n",
    "        stacked_aux = tf.stack(aux_conv_features, axis=-1)  # (batch, time, 32, 6)\n",
    "        weighted_aux_features = self.aux_weights_layer(stacked_aux, non_zero_ratios)  # (batch, time, 32)\n",
    "        \n",
    "        # Combine features\n",
    "        all_features = tf.stack([sales_features, weighted_aux_features], axis=1)  # (batch, 2, time, 32)\n",
    "        \n",
    "        # Reshape for attention\n",
    "        B = tf.shape(all_features)[0]\n",
    "        T = tf.shape(all_features)[2]\n",
    "        D = tf.shape(all_features)[3]\n",
    "        reshaped_features = tf.reshape(all_features, (B, 2, T*D))\n",
    "        \n",
    "        # Apply attention\n",
    "        sales_query = reshaped_features[:, 0:1, :]\n",
    "        attention_output, attention_scores = self.feature_attention(\n",
    "            query=sales_query,\n",
    "            key=reshaped_features,\n",
    "            value=reshaped_features,\n",
    "            return_attention_scores=True\n",
    "        )\n",
    "        \n",
    "        # Reshape attention output\n",
    "        attention_output = tf.reshape(attention_output, (B, T, D))\n",
    "        \n",
    "        # Apply smoothing\n",
    "        x = self.smooth_conv(attention_output)\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.dropout(x, training=training)\n",
    "        \n",
    "        # Final layers\n",
    "        x = x[:, -1, :]  # 마지막 시점 사용\n",
    "        x = self.dense1(x)\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        self.last_attention_scores = tf.reduce_mean(attention_scores, axis=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_peak_loss(y_true, y_pred):\n",
    "    # 기본 MSE\n",
    "    mse = tf.square(y_true - y_pred)\n",
    "    \n",
    "    # 큰 값에 대한 가중치 (지수적으로 증가)\n",
    "    weights = tf.exp(tf.abs(y_true) / tf.reduce_max(tf.abs(y_true))) \n",
    "    weighted_mse = mse * weights\n",
    "    \n",
    "    return tf.reduce_mean(weighted_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[93mItem 0: ('CA', 'FOODS_1_001')\u001b[0m\n",
      "\u001b[92mPreparing Data\u001b[0m\n",
      "\u001b[92mPreparing Loss and Optimizer\u001b[0m\n",
      "\u001b[92mBuilding Model\u001b[0m\n",
      "\u001b[92mTraining\u001b[0m\n",
      "Epoch 1/300\n",
      "sales_conv_outputs 20\n",
      "(None, 92, 32)\n",
      "sales_features Tensor(\"build_model_35/strided_slice:0\", shape=(), dtype=int32)\n",
      "(92, 32)\n",
      "sales_conv_outputs 20\n",
      "(None, 92, 32)\n",
      "sales_features Tensor(\"build_model_35/strided_slice:0\", shape=(), dtype=int32)\n",
      "(92, 32)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 108\u001b[0m\n\u001b[0;32m    101\u001b[0m     early_stopping \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mEarlyStopping(\n\u001b[0;32m    102\u001b[0m         monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    103\u001b[0m         patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m,\n\u001b[0;32m    104\u001b[0m         restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    105\u001b[0m     )\n\u001b[0;32m    106\u001b[0m     lr_scheduler \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mLearningRateScheduler(lr_schedule)\n\u001b[1;32m--> 108\u001b[0m     history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[43m\t\t\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[92mTesting\u001b[39m\u001b[38;5;130;01m\\033\u001b[39;00m\u001b[38;5;124m[0m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    115\u001b[0m     test_dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mfrom_tensor_slices(({\n\u001b[0;32m    116\u001b[0m         \t\t\t\t\t\t\t\t\t\t\t\t\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_sales_test, \n\u001b[0;32m    117\u001b[0m                                                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_price_test,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    122\u001b[0m                                                         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msporting\u001b[39m\u001b[38;5;124m\"\u001b[39m: X_sporting_test, \n\u001b[0;32m    123\u001b[0m                                                         }, y_test))\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\keras\\engine\\training.py:1216\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1210\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1211\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1212\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1213\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1214\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1215\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1216\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1217\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1218\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:910\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    907\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    909\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 910\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    912\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    913\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:975\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    971\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall through to cond-based initialization.\u001b[39;00m\n\u001b[0;32m    972\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;66;03m# Lifting succeeded, so variables are initialized and we can run the\u001b[39;00m\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;66;03m# stateless function.\u001b[39;00m\n\u001b[1;32m--> 975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m   _, _, _, filtered_flat_args \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m    978\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_function_spec\u001b[38;5;241m.\u001b[39mcanonicalize_function_inputs(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    979\u001b[0m           \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3130\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3127\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   3128\u001b[0m   (graph_function,\n\u001b[0;32m   3129\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1959\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1955\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1956\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1957\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1958\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1959\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1960\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1961\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m     args,\n\u001b[0;32m   1963\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1964\u001b[0m     executing_eagerly)\n\u001b[0;32m   1965\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:598\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    596\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    597\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 598\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    602\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    604\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    605\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    606\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    607\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    610\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    611\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\capstone\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:58\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     57\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 58\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     61\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# 파라미터 설정\n",
    "test_size = 0.2\n",
    "\n",
    "n_epochs = 300\n",
    "initial_learning_rate = 5e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=1000,\n",
    "    t_mul=2.0,\n",
    "    m_mul=0.9,\n",
    "    alpha=1e-5\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_kernels = 20 # 상위 몇 개 커널 사용할지\n",
    "look_back_window_size = 1\n",
    "look_forward_window_size = 1\n",
    "\n",
    "num_filters=128\n",
    "\n",
    "for idx, (state_item_id, first_sales_day) in enumerate(first_sales_column_dict.items()): # 아이템별로\n",
    "    if idx > 100:\n",
    "        break\n",
    "    \n",
    "    # 제너레이터에서 해당 아이템의 데이터만 가져오기\n",
    "    item_windows_generator = create_generator_for_each_item(num_kernels=num_kernels, \n",
    "                                                            look_back_window_size=look_back_window_size, \n",
    "                                                            look_forward_window_size=look_forward_window_size)\n",
    "\n",
    "    for item_idx, (input_data, output_data, kernel_sizes, kernel_strengths) in enumerate(item_windows_generator): # 윈도우별로 (배치별로)\n",
    "        print(f\"\\033[93mItem {item_idx}: {state_item_id}\\033[0m\")\n",
    "\n",
    "        print(f\"\\033[92mPreparing Data\\033[0m\")\n",
    "        split_index = int(len(input_data) * (1 - test_size))\n",
    "        train_data = [(input_data[i], output_data[i]) for i in range(split_index)]\n",
    "        test_data = [(input_data[i], output_data[i]) for i in range(split_index - look_back_window_size, len(input_data))]\n",
    "        \n",
    "        x_scaler = QuantileTransformer(output_distribution='normal')\n",
    "        y_scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "        X_sales_train = np.array([x['sales'] for x, _ in train_data]).astype(np.float32)  # (train_size, look_back_window_size)\n",
    "        X_sales_train = x_scaler.fit_transform(X_sales_train)\t\n",
    "\n",
    "        X_price_train = np.array([x['price'] for x, _ in train_data]).astype(np.float32)\n",
    "        X_weekend_train = np.array([x['weekend'] for x, _ in train_data]).astype(np.float32)\n",
    "        X_cultural_train = np.array([x['cultural'] for x, _ in train_data]).astype(np.float32)\n",
    "        X_national_train = np.array([x['national'] for x, _ in train_data]).astype(np.float32)\n",
    "        X_religious_train = np.array([x['religious'] for x, _ in train_data]).astype(np.float32)\n",
    "        X_sporting_train = np.array([x['sporting'] for x, _ in train_data]).astype(np.float32)\n",
    "        \n",
    "        y_train = np.array([y for _, y in train_data]).astype(np.float32)  # (train_size, look_forward_window_size)\n",
    "        y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1, look_forward_window_size)\n",
    "\n",
    "        X_sales_test = np.array([x['sales'] for x, _ in test_data]).astype(np.float32)  # (test_size, look_back_window_size)\n",
    "        X_sales_test = x_scaler.transform(X_sales_test)\n",
    "\n",
    "        X_price_test = np.array([x['price'] for x, _ in test_data]).astype(np.float32)\n",
    "        X_weekend_test = np.array([x['weekend'] for x, _ in test_data]).astype(np.float32)\n",
    "        X_cultural_test = np.array([x['cultural'] for x, _ in test_data]).astype(np.float32)\n",
    "        X_national_test = np.array([x['national'] for x, _ in test_data]).astype(np.float32)\n",
    "        X_religious_test = np.array([x['religious'] for x, _ in test_data]).astype(np.float32)\n",
    "        X_sporting_test = np.array([x['sporting'] for x, _ in test_data]).astype(np.float32)\n",
    "\n",
    "        y_test = np.array([y for _, y in test_data]).astype(np.float32)  # (test_size, look_forward_window_size)\n",
    "        y_test = y_scaler.transform(y_test.reshape(-1, 1)).reshape(-1, look_forward_window_size)\n",
    "\n",
    "        print(f\"\\033[92mPreparing Loss and Optimizer\\033[0m\")\n",
    "        loss = tf.keras.losses.Huber(delta=1.0)\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "\t\t\tlearning_rate=lr_schedule,\n",
    "\t\t\tclipnorm=0.5\n",
    "\t\t)\n",
    "\n",
    "        print(f\"\\033[92mBuilding Model\\033[0m\")\n",
    "        model = BuildModel(\n",
    "            kernel_sizes=kernel_sizes,\n",
    "            kernel_strengths=kernel_strengths,\n",
    "            look_forward_window_size=look_forward_window_size\n",
    "        )\n",
    "\n",
    "        print(f\"\\033[92mTraining\\033[0m\")\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            \t\t\t\t\t\t\t\t\t\t\t\t\"sales\": X_sales_train, \n",
    "                                                            \"price\": X_price_train,\n",
    "                                                            \"weekend\": X_weekend_train, \n",
    "                                                            \"cultural\": X_cultural_train,\n",
    "                                                            \"national\": X_national_train, \n",
    "                                                            \"religious\": X_religious_train,\n",
    "                                                            \"sporting\": X_sporting_train, \n",
    "                                                            }, y_train))\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        \n",
    "        model.compile(\n",
    "\t\t\toptimizer=optimizer,\n",
    "\t\t\tloss=custom_peak_loss\n",
    "\t\t)\n",
    "        \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "                  \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=n_epochs,\n",
    "    \t\tcallbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        print(f\"\\033[92mTesting\\033[0m\")\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "            \t\t\t\t\t\t\t\t\t\t\t\t\"sales\": X_sales_test, \n",
    "                                                            \"price\": X_price_test,\n",
    "                                                            \"weekend\": X_weekend_test, \n",
    "                                                            \"cultural\": X_cultural_test,\n",
    "                                                            \"national\": X_national_test, \n",
    "                                                            \"religious\": X_religious_test,\n",
    "                                                            \"sporting\": X_sporting_test, \n",
    "                                                            }, y_test))\n",
    "        test_dataset = test_dataset.batch(batch_size)\n",
    "        \n",
    "        test_loss = model.evaluate(test_dataset, verbose=1)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # 예측과 실제값 시각화 준비\n",
    "        predictions = model.predict(test_dataset)\n",
    "\n",
    "        # 스케일링 역변환\n",
    "        labels = y_scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "        predictions = y_scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # 그래프 그리기\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(labels, label='Labels', linewidth=2)\n",
    "        plt.plot(predictions, label='Predictions', linewidth=2, linestyle='--')\n",
    "        plt.xlabel(\"Time Steps\")\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.legend()\n",
    "\n",
    "        # 그림 저장\n",
    "        plt.savefig(f\"Item {item_idx}: {state_item_id}.png\")\n",
    "        plt.show()\n",
    "\n",
    "        break\n",
    "    break\n",
    "    #     # print(f\"\\033[92mAnalyzing\\033[0m\")\n",
    "    #     # # 모델 해석 단계\n",
    "    #     # analyzer = AnalyzeModel(model, X_sales_test, X_aux_test)\n",
    "    #     # analyzer.pdp(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 PDP\n",
    "    #     # analyzer.ice(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 ICE\n",
    "    #     # analyzer.feature_weights()  # 모델의 학습된 가중치 출력\n",
    "\n",
    "    # # # 모델 저장\n",
    "    # # model.save(f\"model_{state_item_id}.h5\")\n",
    "\n",
    "    # # # 모델 가중치 초기화\n",
    "    # # model.reset_states()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import shap\n",
    "\n",
    "\n",
    "class AnalyzeModel:\n",
    "    def __init__(self, model, sales_input, aux_input):\n",
    "        \"\"\"\n",
    "        AnalyzeModel 클래스 초기화\n",
    "\n",
    "        :param model: 분석할 학습된 모델\n",
    "        :param sales_input: 시계열 데이터 입력 (e.g., sales input)\n",
    "        :param aux_input: 보조 특성 입력\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.sales_input = sales_input\n",
    "        self.aux_input = aux_input\n",
    "\n",
    "    def pdp(self, feature_index):\n",
    "        \"\"\"\n",
    "        Partial Dependence Plot (PDP):\n",
    "        특정 입력 특성이 모델 출력에 미치는 평균적인 영향을 분석\n",
    "        :param feature_index: 보조 특성(aux_input)에서 분석할 특성의 인덱스\n",
    "        \"\"\"\n",
    "        feature_values = np.linspace(\n",
    "            np.min(self.aux_input[:, :, feature_index]), \n",
    "            np.max(self.aux_input[:, :, feature_index]), \n",
    "            50\n",
    "        )\n",
    "        pdp_values = []\n",
    "        data_copy = self.aux_input.copy()\n",
    "\n",
    "        for val in feature_values:\n",
    "            data_copy[:, :, feature_index] = val\n",
    "            preds = self.model([self.sales_input, data_copy], training=False)\n",
    "            pdp_values.append(np.mean(preds))\n",
    "\n",
    "        plt.plot(feature_values, pdp_values)\n",
    "        plt.xlabel(f\"Feature {feature_index}\")\n",
    "        plt.ylabel(\"Predicted Output\")\n",
    "        plt.title(f\"PDP for Feature {feature_index}\")\n",
    "        plt.show()\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # PDP를 통해 중요도가 낮은 특성을 제거하거나, 중요한 특성을 강조하는 추가적인 전처리 단계를 설계할 수 있음.\n",
    "\n",
    "    def ice(self, feature_index):\n",
    "        \"\"\"\n",
    "        Individual Conditional Expectation (ICE):\n",
    "        특정 입력 특성이 각 샘플별로 모델 출력에 미치는 영향을 시각화\n",
    "        :param feature_index: 보조 특성(aux_input)에서 분석할 특성의 인덱스\n",
    "        \"\"\"\n",
    "        feature_values = np.linspace(\n",
    "            np.min(self.aux_input[:, :, feature_index]), \n",
    "            np.max(self.aux_input[:, :, feature_index]), \n",
    "            50\n",
    "        )\n",
    "        data_copy = self.aux_input.copy()\n",
    "\n",
    "        for sample_idx in range(len(data_copy)):\n",
    "            ice_values = []\n",
    "            for val in feature_values:\n",
    "                data_copy[sample_idx, :, feature_index] = val\n",
    "                preds = self.model([self.sales_input, data_copy], training=False)\n",
    "                ice_values.append(preds[sample_idx])\n",
    "\n",
    "            plt.plot(feature_values, ice_values, alpha=0.5)\n",
    "\n",
    "        plt.xlabel(f\"Feature {feature_index}\")\n",
    "        plt.ylabel(\"Predicted Output\")\n",
    "        plt.title(f\"ICE for Feature {feature_index}\")\n",
    "        plt.show()\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # ICE를 통해 각 샘플별로 중요한 입력 값을 찾아내고, 입력 데이터를 샘플별로 정규화하거나 특성 공학을 적용할 수 있음.\n",
    "\n",
    "    def lime(self, instance_index):\n",
    "        \"\"\"\n",
    "        LIME (Local Interpretable Model-agnostic Explanations):\n",
    "        개별 예측에 대한 해석을 제공하기 위해 모델을 선형 근사\n",
    "        :param instance_index: 설명할 샘플의 인덱스\n",
    "        \"\"\"\n",
    "\n",
    "        data = self.aux_input.reshape(self.aux_input.shape[0], -1)  # Flatten aux_input\n",
    "        explainer = LimeTabularExplainer(data, mode=\"regression\")\n",
    "        instance = data[instance_index]\n",
    "        prediction_fn = lambda x: self.model([self.sales_input[:len(x)], x.reshape(-1, *self.aux_input.shape[1:])])\n",
    "\n",
    "        exp = explainer.explain_instance(instance, prediction_fn)\n",
    "        exp.show_in_notebook()\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # LIME 결과를 바탕으로 특정 입력 값의 비선형적 영향을 보정하는 특성 변환을 설계할 수 있음.\n",
    "\n",
    "    def temporal_lime(self, instance_index):\n",
    "        \"\"\"\n",
    "        Temporal LIME:\n",
    "        시간 축 데이터를 고려한 LIME 해석\n",
    "        :param instance_index: 설명할 샘플의 인덱스\n",
    "        \"\"\"\n",
    "        # 시간 축이 있는 데이터에 대해, LIME을 변형하여 적용 가능\n",
    "        # 구체적인 구현은 데이터 구조와 시간적 특성을 반영해야 함.\n",
    "        pass\n",
    "\n",
    "    def shap_analysis(self):\n",
    "        \"\"\"\n",
    "        SHAP (SHapley Additive exPlanations):\n",
    "        게임 이론에 기반한 Shapley 값을 사용하여 모델 예측 해석\n",
    "        \"\"\"\n",
    "        explainer = shap.KernelExplainer(self.model, [self.sales_input, self.aux_input])\n",
    "        shap_values = explainer.shap_values([self.sales_input, self.aux_input])\n",
    "\n",
    "        shap.summary_plot(shap_values, [self.sales_input, self.aux_input])\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # SHAP 분석 결과를 통해 출력에 큰 영향을 미치는 특성을 확인하고, 중요하지 않은 특성을 제거하거나 데이터 전처리를 개선할 수 있음.\n",
    "\n",
    "    def integrated_gradients(self):\n",
    "        \"\"\"\n",
    "        Integrated Gradients:\n",
    "        출력에 대한 입력의 기여도를 계산\n",
    "        \"\"\"\n",
    "        # TensorFlow의 gradient 기능을 사용하여 구현 가능\n",
    "        pass\n",
    "\n",
    "    def saliency_maps(self):\n",
    "        \"\"\"\n",
    "        Saliency Maps:\n",
    "        입력 데이터에서 중요한 영역을 시각적으로 강조\n",
    "        \"\"\"\n",
    "        # Gradient 기반으로 입력의 중요도를 시각화\n",
    "        pass\n",
    "\n",
    "    def feature_weights(self):\n",
    "        \"\"\"\n",
    "        Feature Weights:\n",
    "        모델에서 학습된 가중치를 사용하여 특성 중요도를 분석\n",
    "        \"\"\"\n",
    "        print(\"Auxiliary Feature Weights:\", self.model.aux_weights_layer.aux_weights.numpy())\n",
    "        print(\"Kernel Feature Weights:\", self.model.kernel_weights_layer.kernel_strengths.numpy())\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # 학습된 가중치가 너무 낮거나 높은 특성을 확인하고, 학습 과정에서 해당 특성을 조정하거나 제거할 수 있음.\n",
    "\n",
    "    def attention_weights(self):\n",
    "        \"\"\"\n",
    "        Attention Weights:\n",
    "        모델의 Attention Layer에서 학습된 가중치를 가져와 해석\n",
    "        \"\"\"\n",
    "        print(\"Attention Weights는 모델에 특정 Attention Layer가 포함된 경우 활용 가능\")\n",
    "\n",
    "        # 모델 개선 방안:\n",
    "        # Attention 가중치를 분석하여, 모델이 어떤 입력에 더 집중하고 있는지 확인하고 입력 데이터 품질을 개선할 수 있음.\n",
    "\n",
    "    def lrp(self):\n",
    "        \"\"\"\n",
    "        Layer-wise Relevance Propagation (LRP):\n",
    "        출력의 관련성을 입력 데이터로 역전파\n",
    "        \"\"\"\n",
    "        # 출력에서 입력으로의 관련성 점수를 계산\n",
    "        pass\n",
    "\n",
    "    def channel_activation_map(self):\n",
    "        \"\"\"\n",
    "        Channel Activation Map:\n",
    "        모델 내부의 채널 활성화를 시각화하여 특정 입력이 어떤 활성화를 유도하는지 분석\n",
    "        \"\"\"\n",
    "        # Conv1D 등에서 채널별 활성화 맵을 시각화 가능\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TimeSeriesAnalyzer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def analyze_contribution(self, sales, product_ids, period_strength, auxiliary_features):\n",
    "        \"\"\"예측에 대한 각 컴포넌트의 기여도 분석\"\"\"\n",
    "        prediction, feature_weights, feature_maps = self.model([\n",
    "            sales, product_ids, period_strength, auxiliary_features\n",
    "        ])\n",
    "        \n",
    "        # 시간별 활성화 점수 계산\n",
    "        temporal_importance = tf.reduce_mean(feature_maps, axis=-1)  # (batch_size, sequence_length)\n",
    "        \n",
    "        # 컨볼루션 필터별 중요도\n",
    "        filter_importance = tf.reduce_mean(feature_maps, axis=[0, 1])  # (n_channels,)\n",
    "        \n",
    "        aux_contributions = {\n",
    "            'is_weekend': float(feature_weights[0, 0].numpy()),\n",
    "            'cultural_event': float(feature_weights[0, 1].numpy()),\n",
    "            'national_event': float(feature_weights[0, 2].numpy()),\n",
    "            'religious_event': float(feature_weights[0, 3].numpy()),\n",
    "            'sporting_event': float(feature_weights[0, 4].numpy()),\n",
    "            'price_change': float(feature_weights[0, 5].numpy())\n",
    "        }\n",
    "        \n",
    "        return {\n",
    "            'prediction': float(prediction[0].numpy()),\n",
    "            'period_strength': float(period_strength[0].numpy()),\n",
    "            'auxiliary_contributions': aux_contributions,\n",
    "            'temporal_importance': temporal_importance.numpy(),\n",
    "            'filter_importance': filter_importance.numpy()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiOutputTimeSeriesModel(Model):\n",
    "    def __init__(self, n_products, fundamental_period, n_channels):\n",
    "        # ... 기존 레이어들 ...\n",
    "        \n",
    "        # 최종 예측 레이어 수정\n",
    "        self.predictor = tf.keras.Sequential([\n",
    "            layers.Dense(64, activation='relu'),\n",
    "            # fundamental_period 만큼 동시 예측\n",
    "            layers.Dense(fundamental_period)  \n",
    "        ])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # ... 기존 처리 ...\n",
    "        \n",
    "        # 전체 주기에 대한 예측\n",
    "        predictions = self.predictor(combined_features)  \n",
    "        # shape: (batch_size, fundamental_period)\n",
    "        \n",
    "        return predictions, feature_weights, feature_maps\n",
    "\n",
    "    def compute_attention(self, predictions, feature_maps):\n",
    "        \"\"\"각 예측 시점별 feature importance 계산\"\"\"\n",
    "        importance_scores = []\n",
    "        for t in range(self.fundamental_period):\n",
    "            # t 시점 예측에 대한 각 특성의 기여도\n",
    "            scores = self._compute_gradients(\n",
    "                predictions[:, t], \n",
    "                feature_maps\n",
    "            )\n",
    "            importance_scores.append(scores)\n",
    "        return importance_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존\n",
    "pooled_features = tf.reduce_mean(conv_out, axis=2)  # 시간 정보 손실\n",
    "\n",
    "# 수정 제안\n",
    "# 1. Feature map 자체를 활용\n",
    "attention_scores = tf.reduce_mean(conv_out, axis=1)  # (batch_size, sequence_length)\n",
    "# 각 시점별로 얼마나 활성화되었는지 확인 가능\n",
    "\n",
    "# 2. Grad-CAM과 유사한 방식\n",
    "# 각 시점의 기여도를 그래디언트를 통해 계산"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

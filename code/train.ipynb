{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU Detected. Configuring GPU...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt, freqz\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from tensorflow.keras.layers import Conv1D, LSTM, Dense, Attention, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.cluster import KMeans, SpectralClustering, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import MeanShift\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial import ConvexHull\n",
    "\n",
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"\\033[92mGPU Detected. Configuring GPU...\\033[0m\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)  # 메모리 동적 할당\n",
    "else:\n",
    "    print(\"\\033[91mNo GPU detected. Running on CPU.\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_sales_column_dict(df, save_result=False):\n",
    "    # sell_prices의 각 행에서 NaN이 끝나고 처음으로 숫자인 column을 찾기\n",
    "    first_sales_column_dict = {}\n",
    "    \n",
    "    for idx in df.index:\n",
    "        row = df.iloc[idx]\n",
    "        # state_id와 item_id로 키 튜플 생성\n",
    "        key = (row['state_id'], row['item_id'])\n",
    "        \n",
    "        # NaN 값들의 위치를 찾습니다\n",
    "        nan_mask = row.isna()\n",
    "        \n",
    "        if not nan_mask.any():\n",
    "            # NaN이 없는 경우 첫 번째 데이터 컬럼('d_1')을 저장\n",
    "            first_sales_column_dict[key] = 'd_1'\n",
    "        else:\n",
    "            # 마지막 NaN의 위치를 찾습니다\n",
    "            last_nan_idx = nan_mask[::-1].idxmax()\n",
    "            # 마지막 NaN의 위치 이후의 첫 번째 숫자가 있는 컬럼을 찾습니다\n",
    "            last_nan_position = row.index.get_loc(last_nan_idx)\n",
    "            first_number_col = row.index[last_nan_position + 1]\n",
    "            first_sales_column_dict[key] = first_number_col\n",
    "    \n",
    "    if save_result:\n",
    "        with open('../data/preprocessed/first_sales_column_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(first_sales_column_dict, f)\n",
    "            \n",
    "    return first_sales_column_dict\n",
    "\n",
    "# 판매량 데이터 (아이템별 상이)\n",
    "sales = pd.read_csv(\"../data/preprocessed/sales.csv\")\n",
    "# 판매 가격 데이터 (아이템별 상이)\n",
    "sell_prices = pd.read_csv(\"../data/preprocessed/sell_prices.csv\")\n",
    "# 주말 여부 데이터 (1941일)\n",
    "is_weekend = pd.read_csv(\"../data/preprocessed/is_weekend.csv\")\n",
    "# 이벤트 데이터 (1941일)\n",
    "event_type_cultural = pd.read_csv(\"../data/preprocessed/event_type_cultural.csv\")\n",
    "event_type_national = pd.read_csv(\"../data/preprocessed/event_type_national.csv\")\n",
    "event_type_religious = pd.read_csv(\"../data/preprocessed/event_type_religious.csv\")\n",
    "event_type_sporting = pd.read_csv(\"../data/preprocessed/event_type_sporting.csv\")\n",
    "\n",
    "# 판매 시작 시점 \n",
    "# first_sales_column_dict = create_first_sales_column_dict(sell_prices, save_result==True)\n",
    "with open('../data/preprocessed/first_sales_column_dict.pkl', 'rb') as f:\n",
    "    first_sales_column_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_by_first_sale_day(state_item_id, first_sales_day):\n",
    "    # 주 ID, 아이템 ID\n",
    "    state_id, item_id = state_item_id\n",
    "\n",
    "    # 필요한 데이터 필터링 (한 번씩만 필터링)\n",
    "    sales_df = sales.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    price_df = sell_prices.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    \n",
    "    # 각 데이터의 슬라이싱 시작 인덱스 계산\n",
    "    start_col_sales = sales_df.index.get_loc(first_sales_day)\n",
    "    start_col_price = price_df.index.get_loc(first_sales_day)\n",
    "\n",
    "    # 각 데이터 슬라이싱\n",
    "    sales_data = sales_df.iloc[start_col_sales:].values\n",
    "    price_data = price_df.iloc[start_col_price:].values\n",
    "    \n",
    "    # 고정된 데이터 (weekend와 이벤트 데이터)\n",
    "    start_col_weekend_event = is_weekend.columns.get_loc(first_sales_day)\n",
    "    weekend_data = is_weekend.iloc[0, start_col_weekend_event:].values\n",
    "    cultural_data = event_type_cultural.iloc[0, start_col_weekend_event:].values\n",
    "    national_data = event_type_national.iloc[0, start_col_weekend_event:].values\n",
    "    religious_data = event_type_religious.iloc[0, start_col_weekend_event:].values\n",
    "    sporting_data = event_type_sporting.iloc[0, start_col_weekend_event:].values\n",
    "    \n",
    "    # 출력\n",
    "    return {\n",
    "        \"sales\": sales_data,\n",
    "        \"price\": price_data,\n",
    "        \"weekend\": weekend_data,\n",
    "        \"cultural\": cultural_data,\n",
    "        \"national\": national_data,\n",
    "        \"religious\": religious_data,\n",
    "        \"sporting\": sporting_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator_for_each_item(num_kernels, look_back_window_size, look_forward_window_size):\n",
    "    for state_item_id, first_sales_day in first_sales_column_dict.items(): # 아이템별로\n",
    "        data = sort_by_first_sale_day(state_item_id, first_sales_day)\n",
    "\n",
    "        kernel_sizes = [k for k in fourier_results[state_item_id]['selected_periods'][:num_kernels]]\n",
    "        max_kernel_size = max(kernel_sizes)\n",
    "        look_back_window_size = max(look_back_window_size, max_kernel_size)\n",
    "\n",
    "        # 슬라이딩 윈도우 데이터 생성\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "        for i in range(len(data[\"sales\"]) - look_back_window_size - look_forward_window_size): # 슬라이딩 윈도우\n",
    "            ##### 슬라이딩 윈도우 간격\n",
    "            input = {\n",
    "                \"sales\": data[\"sales\"][i:i+look_back_window_size],\n",
    "                \"price\": data[\"price\"][i:i+look_back_window_size],\n",
    "                \"weekend\": data[\"weekend\"][i:i+look_back_window_size],\n",
    "                \"cultural\": data[\"cultural\"][i:i+look_back_window_size],\n",
    "                \"national\": data[\"national\"][i:i+look_back_window_size],\n",
    "                \"religious\": data[\"religious\"][i:i+look_back_window_size],\n",
    "                \"sporting\": data[\"sporting\"][i:i+look_back_window_size],\n",
    "            }\n",
    "            output = data[\"sales\"][i+look_back_window_size:i+look_back_window_size+look_forward_window_size]\n",
    "            input_data.append(input)\n",
    "            output_data.append(output)\n",
    "\n",
    "        yield input_data, output_data, kernel_sizes  # 아이템별로 훈련/테스트 데이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_periods_in_sales_data(sales_data, c_algorithm, n_clusters, save_plot, fourier_save_path):\n",
    "    # 아이템별 판매 데이터를 분석하여 주기 분석과 SNR 계산을 수행하는 함수\n",
    "\n",
    "    # 푸리에 변환\n",
    "    fft = np.fft.fft(sales_data)\n",
    "    frequencies = np.fft.fftfreq(len(sales_data))\n",
    "    magnitudes = np.abs(fft) / len(sales_data)\n",
    "    phases = np.angle(fft)\n",
    "\n",
    "    # 군집화 알고리즘 정의\n",
    "    clustering_algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=n_clusters, random_state=42),\n",
    "        'Spectral': SpectralClustering(n_clusters=n_clusters, random_state=42),\n",
    "        'DBSCAN': DBSCAN(eps=0.3),\n",
    "        'Agglomerative': AgglomerativeClustering(n_clusters=n_clusters),\n",
    "        'GMM': GaussianMixture(n_components=n_clusters, random_state=42),\n",
    "        'MeanShift': MeanShift(bandwidth=0.6)\n",
    "    }\n",
    "    clustering_algorithm = clustering_algorithms[c_algorithm]\n",
    "\n",
    "    # 군집화\n",
    "    positive_mask = frequencies >= 0\n",
    "    frequencies_magnitudes_phases = np.column_stack((\n",
    "        frequencies[positive_mask], \n",
    "        magnitudes[positive_mask],\n",
    "        phases[positive_mask]\n",
    "    ))\n",
    "    predicts = clustering_algorithm.fit_predict(frequencies_magnitudes_phases[:, :2])\n",
    "    labels = np.unique(predicts)\n",
    "\n",
    "    # 군집화 시각화\n",
    "    if save_plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        scatter = plt.scatter(frequencies_magnitudes_phases[:, 0], frequencies_magnitudes_phases[:, 1], c=predicts, alpha=0.6)\n",
    "        plt.xlabel('Frequency')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fourier_save_path, f\"cluster_{c_algorithm}_{n_clusters}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # 진폭이 가장 큰 군집 대표값 추출\n",
    "    cluster_represents = []\n",
    "    for label in labels:\n",
    "        cluster_mask = predicts == label\n",
    "\n",
    "        cluster_represent_frequency = frequencies_magnitudes_phases[cluster_mask, 0]\n",
    "        cluster_represent_magnitude = frequencies_magnitudes_phases[cluster_mask, 1]\n",
    "        cluster_represent_phase = frequencies_magnitudes_phases[cluster_mask, 2]\n",
    "\n",
    "        cluster_represent_idx = np.argmax(cluster_represent_magnitude)\n",
    " \n",
    "        cluster_represents.append({\n",
    "            'frequency': cluster_represent_frequency[cluster_represent_idx],\n",
    "            'magnitude': cluster_represent_magnitude[cluster_represent_idx],\n",
    "            'phase': cluster_represent_phase[cluster_represent_idx]\n",
    "        })\n",
    "\n",
    "    # 푸리에 역변환\n",
    "    t = np.arange(len(sales_data))\n",
    "    reconstructed_sales_data = np.zeros_like(sales_data, dtype=float)\n",
    "\n",
    "    for cluster_represent in cluster_represents:\n",
    "        if cluster_represent['frequency'] == 0:  # DC\n",
    "            reconstructed_sales_data += cluster_represent['magnitude'] * np.cos(2 * np.pi * cluster_represent['frequency'] * t + cluster_represent['phase'])\n",
    "        else: # 음수, 양수\n",
    "            reconstructed_sales_data += 2 * cluster_represent['magnitude'] * np.cos(2 * np.pi * cluster_represent['frequency'] * t + cluster_represent['phase'])\n",
    "\n",
    "    # 푸리에 역변환 시각화\n",
    "    if save_plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(t, sales_data, label=\"Original Signal\", alpha=0.7)\n",
    "        plt.plot(t, reconstructed_sales_data, label=\"Reconstructed Signal\", alpha=0.7)\n",
    "        plt.xlabel(\"Time\")\n",
    "        plt.ylabel(\"Magnitude\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(fourier_save_path, f\"reconstruct_{c_algorithm}_{n_clusters-1}.png\"))\n",
    "        plt.close()\n",
    "\n",
    "    # SNR\n",
    "    original_energy = np.sum(np.abs(sales_data) ** 2)\n",
    "    error_energy = np.sum(np.abs(sales_data - reconstructed_sales_data) ** 2)\n",
    "    snr = 10 * np.log10(original_energy / error_energy)\n",
    "\n",
    "    # 주기\n",
    "    periods = [1 / cluster_represent['frequency'] for cluster_represent in cluster_represents if cluster_represent['frequency'] != 0] # DC 제거\n",
    "\n",
    "    # 주기, SNR 저장\n",
    "    with open(os.path.join(fourier_save_path, f\"periods_{c_algorithm}_{n_clusters-1}.pkl\"), 'wb') as f:\n",
    "        pickle.dump({\"periods\": periods, \"snr\": snr}, f)\n",
    "\n",
    "    return periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 파라미터 설정\n",
    "\n",
    "## 푸리에 변환 & 군집화\n",
    "c_algorithm = 'KMeans' # 'Spectral','DBSCAN','Agglomerative','GMM','MeanShift'\n",
    "n_clusters = 30 + 1 # DC 포함\n",
    "save_plot = True\n",
    "\n",
    "\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "n_epochs = 300\n",
    "initial_learning_rate = 5e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=1000,\n",
    "    t_mul=2.0,\n",
    "    m_mul=0.9,\n",
    "    alpha=1e-5\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_kernels = 20 # 상위 몇 개 커널 사용할지\n",
    "look_back_window_size = 1\n",
    "look_forward_window_size = 1\n",
    "\n",
    "num_filters=128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (state_item_id, first_sales_day) in enumerate(first_sales_column_dict.items()): # 아이템별로\n",
    "    aligned_data = align_by_first_sale_day(state_item_id, first_sales_day)\n",
    "\n",
    "    ### 판매 데이터 \n",
    "    sales_data = aligned_data[\"sales\"]\n",
    "\n",
    "    ## 주기 추출\n",
    "    fourier_save_path = f\"../data/fourier/{state_item_id[0]}_{state_item_id[1]}/\"\n",
    "    periods_file_path = os.path.join(fourier_save_path, f\"periods_{c_algorithm}_{n_clusters-1}.pkl\")\n",
    "    if os.path.exists(periods_file_path):\n",
    "        with open(periods_file_path, 'rb') as f:\n",
    "            periods_data = pickle.load(f)\n",
    "            periods = periods_data[\"periods\"]\n",
    "    else:\n",
    "        os.makedirs(fourier_save_path, exist_ok=True) # 폴더 생성\n",
    "        periods = extract_periods_in_sales_data(sales_data, c_algorithm, n_clusters, save_plot, fourier_save_path)\n",
    "\n",
    "\n",
    "    continue\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ## 푸리에 변환을 통해 주기 추출\n",
    "    periods_path = f\"../data/fourier/{state_item_id[0]}_{state_item_id[1]}/periods.pkl\"\n",
    "    if os.path.exists(periods_path): \n",
    "        with open(periods_path, 'rb') as f:\n",
    "            periods = pickle.load(f)\n",
    "    else:\n",
    "        periods = extract_periods_in_sales_data_with_fourier(\n",
    "                        sales_data, \n",
    "                        low_cutoff_period=low_cutoff_period,\n",
    "                        high_cutoff_period=high_cutoff_period,\n",
    "                        snr_percentile=snr_percentile, \n",
    "                        fourier_save_plot=fourier_save_plot, \n",
    "                        fourier_save_path=f\"../data/fourier/{state_item_id[0]}_{state_item_id[1]}/\"\n",
    "                    )\n",
    "    \n",
    "    ## 추출한 주기로 STL 분해 반복적으로 수행\n",
    "    seasonals = np.zeros_like(sales_data)\n",
    "    residuals = np.zeros_like(sales_data)\n",
    "    for period in periods:\n",
    "        stl = STL(sales_data, period=period)\n",
    "        result = stl.fit()\n",
    "        print(result.seasonal.shape, result.resid.shape)\n",
    "        break\n",
    "        seasonals += result.seasonal\n",
    "        residuals += result.resid\n",
    "\n",
    "    ### 외부 데이터\n",
    "    # price_data = aligned_data[\"price\"]\n",
    "    # weekend_data = aligned_data[\"weekend\"]\n",
    "    # cultural_data = aligned_data[\"cultural\"]\n",
    "    # national_data = aligned_data[\"national\"]\n",
    "    # religious_data = aligned_data[\"religious\"]\n",
    "    # sporting_data = aligned_data[\"sporting\"]\n",
    "\n",
    "    def create_windows(data, look_back_window_size, look_forward_window_size):\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "        for i in range(len(data) - look_back_window_size - look_forward_window_size):\n",
    "            input_data.append(data[i:i+look_back_window_size])\n",
    "            output_data.append(data[i+look_back_window_size:i+look_back_window_size+look_forward_window_size])\n",
    "        return np.array(input_data), np.array(output_data)\n",
    "\n",
    "    def build_seq2seq_model(input_shape, num_filters, look_forward_window_size):\n",
    "        inputs = Input(shape=input_shape)\n",
    "        x = Conv1D(filters=num_filters, kernel_size=3, padding='same', activation='relu')(inputs)\n",
    "        x = Attention()([x, x])\n",
    "        encoder_output = LSTM(128, return_state=True)\n",
    "        encoder_outputs, state_h, state_c = encoder_output(x)\n",
    "        encoder_states = [state_h, state_c]\n",
    "\n",
    "        decoder_inputs = Input(shape=(None, input_shape[1]))\n",
    "        decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n",
    "        decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "        decoder_dense = Dense(look_forward_window_size, activation='linear')\n",
    "        decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "        model = Model([inputs, decoder_inputs], decoder_outputs)\n",
    "        return model\n",
    "\n",
    "    # Create sliding windows\n",
    "    look_back_window_size = 30\n",
    "    look_forward_window_size = 7\n",
    "    input_data, output_data = create_windows(seasonals, look_back_window_size, look_forward_window_size)\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    split_index = int(len(input_data) * (1 - test_size))\n",
    "    X_train, X_test = input_data[:split_index], input_data[split_index:]\n",
    "    y_train, y_test = output_data[:split_index], output_data[split_index:]\n",
    "\n",
    "    # Build and compile the model\n",
    "    input_shape = (look_back_window_size, 1)\n",
    "    model = build_seq2seq_model(input_shape, num_filters, look_forward_window_size)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit([X_train, X_train], y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2)\n",
    "\n",
    "    # Evaluate the model\n",
    "    test_loss = model.evaluate([X_test, X_test], y_test)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    # Predict and plot results\n",
    "    predictions = model.predict([X_test, X_test])\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.flatten(), label='True')\n",
    "    plt.plot(predictions.flatten(), label='Predicted')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    break\n",
    "    #     # print(f\"\\033[92mAnalyzing\\033[0m\")\n",
    "    #     # # 모델 해석 단계\n",
    "    #     # analyzer = AnalyzeModel(model, X_sales_test, X_aux_test)\n",
    "    #     # analyzer.pdp(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 PDP\n",
    "    #     # analyzer.ice(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 ICE\n",
    "    #     # analyzer.feature_weights()  # 모델의 학습된 가중치 출력\n",
    "\n",
    "    # # # 모델 저장\n",
    "    # # model.save(f\"model_{state_item_id}.h5\")\n",
    "\n",
    "    # # # 모델 가중치 초기화\n",
    "    # # model.reset_states()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

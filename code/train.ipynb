{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92mGPU Detected. Configuring GPU...\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from models.kernel_predictor import MultiKernelPredictor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, filtfilt\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# GPU 설정\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "if physical_devices:\n",
    "    print(\"\\033[92mGPU Detected. Configuring GPU...\\033[0m\")\n",
    "    for device in physical_devices:\n",
    "        tf.config.experimental.set_memory_growth(device, True)  # 메모리 동적 할당\n",
    "else:\n",
    "    print(\"\\033[91mNo GPU detected. Running on CPU.\\033[0m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_first_sales_column_dict(df, save_result=False):\n",
    "    # sell_prices의 각 행에서 NaN이 끝나고 처음으로 숫자인 column을 찾기\n",
    "    first_sales_column_dict = {}\n",
    "    \n",
    "    for idx in df.index:\n",
    "        row = df.iloc[idx]\n",
    "        # state_id와 item_id로 키 튜플 생성\n",
    "        key = (row['state_id'], row['item_id'])\n",
    "        \n",
    "        # NaN 값들의 위치를 찾습니다\n",
    "        nan_mask = row.isna()\n",
    "        \n",
    "        if not nan_mask.any():\n",
    "            # NaN이 없는 경우 첫 번째 데이터 컬럼('d_1')을 저장\n",
    "            first_sales_column_dict[key] = 'd_1'\n",
    "        else:\n",
    "            # 마지막 NaN의 위치를 찾습니다\n",
    "            last_nan_idx = nan_mask[::-1].idxmax()\n",
    "            # 마지막 NaN의 위치 이후의 첫 번째 숫자가 있는 컬럼을 찾습니다\n",
    "            last_nan_position = row.index.get_loc(last_nan_idx)\n",
    "            first_number_col = row.index[last_nan_position + 1]\n",
    "            first_sales_column_dict[key] = first_number_col\n",
    "    \n",
    "    if save_result:\n",
    "        with open('../data/preprocessed/first_sales_column_dict.pkl', 'wb') as f:\n",
    "            pickle.dump(first_sales_column_dict, f)\n",
    "            \n",
    "    return first_sales_column_dict\n",
    "\n",
    "# 판매량 데이터 (아이템별 상이)\n",
    "sales = pd.read_csv(\"../data/preprocessed/sales.csv\")\n",
    "# 판매 가격 데이터 (아이템별 상이)\n",
    "sell_prices = pd.read_csv(\"../data/preprocessed/sell_prices.csv\")\n",
    "# 주말 여부 데이터 (1941일)\n",
    "is_weekend = pd.read_csv(\"../data/preprocessed/is_weekend.csv\")\n",
    "# 이벤트 데이터 (1941일)\n",
    "event_type_cultural = pd.read_csv(\"../data/preprocessed/event_type_cultural.csv\")\n",
    "event_type_national = pd.read_csv(\"../data/preprocessed/event_type_national.csv\")\n",
    "event_type_religious = pd.read_csv(\"../data/preprocessed/event_type_religious.csv\")\n",
    "event_type_sporting = pd.read_csv(\"../data/preprocessed/event_type_sporting.csv\")\n",
    "\n",
    "# 판매 시작 시점 \n",
    "# first_sales_column_dict = create_first_sales_column_dict(sell_prices, save_result==True)\n",
    "with open('../data/preprocessed/first_sales_column_dict.pkl', 'rb') as f:\n",
    "    first_sales_column_dict = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_by_first_sale_day(state_item_id, first_sales_day):\n",
    "    # 주 ID, 아이템 ID\n",
    "    state_id, item_id = state_item_id\n",
    "\n",
    "    # 필요한 데이터 필터링 (한 번씩만 필터링)\n",
    "    sales_df = sales.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    price_df = sell_prices.query(\"state_id == @state_id and item_id == @item_id\").iloc[0]\n",
    "    \n",
    "    # 각 데이터의 슬라이싱 시작 인덱스 계산\n",
    "    start_col_sales = sales_df.index.get_loc(first_sales_day)\n",
    "    start_col_price = price_df.index.get_loc(first_sales_day)\n",
    "\n",
    "    # 각 데이터 슬라이싱\n",
    "    sales_data = sales_df.iloc[start_col_sales:].values\n",
    "    price_data = price_df.iloc[start_col_price:].values\n",
    "    \n",
    "    # 고정된 데이터 (weekend와 이벤트 데이터)\n",
    "    start_col_weekend_event = is_weekend.columns.get_loc(first_sales_day)\n",
    "    weekend_data = is_weekend.iloc[0, start_col_weekend_event:].values\n",
    "    cultural_data = event_type_cultural.iloc[0, start_col_weekend_event:].values\n",
    "    national_data = event_type_national.iloc[0, start_col_weekend_event:].values\n",
    "    religious_data = event_type_religious.iloc[0, start_col_weekend_event:].values\n",
    "    sporting_data = event_type_sporting.iloc[0, start_col_weekend_event:].values\n",
    "    \n",
    "    # 출력\n",
    "    return {\n",
    "        \"sales\": sales_data,\n",
    "        \"price\": price_data,\n",
    "        \"weekend\": weekend_data,\n",
    "        \"cultural\": cultural_data,\n",
    "        \"national\": national_data,\n",
    "        \"religious\": religious_data,\n",
    "        \"sporting\": sporting_data,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_generator_for_each_item(num_kernels, look_back_window_size, look_forward_window_size):\n",
    "    for state_item_id, first_sales_day in first_sales_column_dict.items(): # 아이템별로\n",
    "        data = sort_by_first_sale_day(state_item_id, first_sales_day)\n",
    "\n",
    "        kernel_sizes = [k for k in fourier_results[state_item_id]['selected_periods'][:num_kernels]]\n",
    "        max_kernel_size = max(kernel_sizes)\n",
    "        look_back_window_size = max(look_back_window_size, max_kernel_size)\n",
    "\n",
    "        # 슬라이딩 윈도우 데이터 생성\n",
    "        input_data = []\n",
    "        output_data = []\n",
    "        for i in range(len(data[\"sales\"]) - look_back_window_size - look_forward_window_size): # 슬라이딩 윈도우\n",
    "            ##### 슬라이딩 윈도우 간격\n",
    "            input = {\n",
    "                \"sales\": data[\"sales\"][i:i+look_back_window_size],\n",
    "                \"price\": data[\"price\"][i:i+look_back_window_size],\n",
    "                \"weekend\": data[\"weekend\"][i:i+look_back_window_size],\n",
    "                \"cultural\": data[\"cultural\"][i:i+look_back_window_size],\n",
    "                \"national\": data[\"national\"][i:i+look_back_window_size],\n",
    "                \"religious\": data[\"religious\"][i:i+look_back_window_size],\n",
    "                \"sporting\": data[\"sporting\"][i:i+look_back_window_size],\n",
    "            }\n",
    "            output = data[\"sales\"][i+look_back_window_size:i+look_back_window_size+look_forward_window_size]\n",
    "            input_data.append(input)\n",
    "            output_data.append(output)\n",
    "\n",
    "        yield input_data, output_data, kernel_sizes  # 아이템별로 훈련/테스트 데이터 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_periods_in_sales_data_with_fourier(sales_data, low_cutoff_period, high_cutoff_period, \n",
    "                                               save_dominant_period_and_normalized_magnitude, \n",
    "                                               save_plot, snr_percentile, save_final_periods, save_path):\n",
    "    \"\"\"\n",
    "    아이템별 판매 데이터를 분석하여 주기 분석과 SNR 계산을 수행하는 함수.\n",
    "    \n",
    "    Args:\n",
    "    - sales_data: 판매 데이터 (시간 순서대로)\n",
    "    - low_cutoff_period: 저주파/긴 주기 제거 (기본값 365일)\n",
    "    - high_cutoff_period: 고주파/짧은 주기 제거 (기본값 3일)\n",
    "    - save_dominant_period_and_normalized_magnitude: dominant_period_and_normalized_magnitude 엑셀 파일 저장 여부\n",
    "    - save_plot: 그래프 저장 여부\n",
    "    - snr_percentile: SNR의 퍼센트로 선택할 주기의 비율 (기본값 0.75)\n",
    "    - save_final_periods: 최종 주기 결과를 피클로 저장 여부\n",
    "    - save_path: 저장 경로\n",
    "    \n",
    "    Returns:\n",
    "    - final_periods: 최종적으로 선택된 주기들\n",
    "    \"\"\"\n",
    "\n",
    "    # 폴더가 없으면 생성\n",
    "    os.makedirs(save_path, exist_ok=True) \n",
    "\n",
    "    # 샘플링 주파수와 나이퀴스트 주파수 설정\n",
    "    sampling_frequency = 1  # 1일 단위 샘플링\n",
    "    nyquist_frequency = sampling_frequency / 2  # 나이퀴스트 주파수\n",
    "    \n",
    "    # 필터링 주파수 설정\n",
    "    low_cutoff_frequency = 1 / low_cutoff_period  # 저주파/긴 주기 제거 # 주기 ?일 이상 제거\n",
    "    high_cutoff_frequency = 1 / high_cutoff_period  # 고주파/짧은 주기 제거 # 주기 ?일 이하 제거\n",
    "    normal_low_cutoff_frequency = low_cutoff_frequency / nyquist_frequency  # 정규화된 컷오프 주파수\n",
    "    normal_high_cutoff_frequency = high_cutoff_frequency / nyquist_frequency  # 정규화된 컷오프 주파수\n",
    "    \n",
    "    # 밴드패스 필터 설계\n",
    "    b, a = butter(N=3, Wn=[normal_low_cutoff_frequency, normal_high_cutoff_frequency], btype='band', analog=False)\n",
    "    sales_data_filtered = filtfilt(b, a, sales_data)  # 필터 적용\n",
    "\n",
    "    # 푸리에 변환\n",
    "    fft = np.fft.fft(sales_data_filtered)\n",
    "    frequencies = np.fft.fftfreq(len(sales_data_filtered), d=1)[:len(sales_data_filtered)//2]  # 대칭 구조이기에 절반만 사용\n",
    "    magnitudes = np.abs(fft)[:len(sales_data_filtered)//2]  # 대칭 구조이기에 절반만 사용\n",
    "    \n",
    "    # 나이퀴스트 조건 고려 (에일리어싱 방지) & 주파수 0 제거\n",
    "    valid_indices = (frequencies <= nyquist_frequency) & (frequencies > 0)\n",
    "    frequencies = frequencies[valid_indices]\n",
    "    magnitudes = magnitudes[valid_indices]\n",
    "\n",
    "    # 진폭순으로 정렬\n",
    "    sorted_indices = np.argsort(magnitudes)[::-1]\n",
    "    frequencies = frequencies[sorted_indices]\n",
    "    magnitudes = magnitudes[sorted_indices]\n",
    "\n",
    "    # 진폭 스펙트럼 시각화\n",
    "    if save_plot:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "\n",
    "        # 원본 진폭 스펙트럼\n",
    "        plt.plot(frequencies, magnitudes_original, label='Original Magnitude Spectrum', color='orange')\n",
    "\n",
    "        # 필터링된 진폭 스펙트럼\n",
    "        plt.plot(frequencies, magnitudes, label='Filtered Magnitude Spectrum', color='blue')\n",
    "\n",
    "        # 그래프 설정\n",
    "        plt.title(\"Original and Filtered Magnitude Spectrum\")\n",
    "        plt.xlabel('Frequency (Hz)')\n",
    "        plt.ylabel('Magnitude')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # 플롯 저장\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(save_path, \"amplitude_spectrum_overlap.png\"))\n",
    "        plt.close()\n",
    "    \n",
    "    # 주기 계산\n",
    "    periods = [round(1 / frequency) for frequency in frequencies]  # 주기 반올림    \n",
    "\n",
    "    # 가장 큰 진폭을 가진 주기와 정규화된 진폭의 퍼센트 데이터프레임에 저장\n",
    "    dominant_period_and_normalized_magnitude = pd.DataFrame(columns=[\"Dominant Period\", \"Normalized Magnitude\"])\n",
    "    dominant_period_and_normalized_magnitude.loc[\"dominant_period\"] = [periods[0], (magnitudes[0] / magnitudes.sum()) * 100] # 백분율\n",
    "\n",
    "    # 선택된 주기 개수별 SNR 계산\n",
    "    snr_results = []  # 주기 개수별 SNR 결과\n",
    "    for i in range(1, len(periods) + 1):\n",
    "        # 선택된 주기와 진폭\n",
    "        selected_periods = periods[:i]\n",
    "        selected_magnitudes = magnitudes[:i]\n",
    "\n",
    "        # 중복된 주기 제거\n",
    "        deduplicated_periods = []  # 중복 제거된 주기\n",
    "        deduplicated_magnitudes = []  # 중복 제거된 진폭\n",
    "        deduplicated_indices = []  # 선택된 주파수 인덱스\n",
    "        for j, selected_period in enumerate(selected_periods):  # 선택된 주기 반복\n",
    "            if selected_period not in deduplicated_periods:  # (이미 진폭순으로 내림차순이기 때문에) 선택된 주기가 중복되지 않을 때만 추가\n",
    "                deduplicated_periods.append(selected_period)\n",
    "                deduplicated_magnitudes.append(selected_magnitudes[j])\n",
    "                deduplicated_indices.append(sorted_indices[j])\n",
    "\n",
    "        # 인버스 푸리에 변환\n",
    "        inverse_fft = np.zeros_like(fft, dtype=complex)\n",
    "        for selected_index in deduplicated_indices:\n",
    "            inverse_fft[selected_index] = fft[selected_index]\n",
    "            inverse_fft[-selected_index] = fft[-selected_index]\n",
    "        reconstructed_sales_data = np.fft.ifft(inverse_fft).real\n",
    "\n",
    "        # SNR 계산\n",
    "        original_energy = np.sum(np.abs(sales_data_filtered) ** 2)\n",
    "        error_energy = np.sum(np.abs(sales_data_filtered - reconstructed_sales_data) ** 2)\n",
    "        current_snr = 10 * np.log10(original_energy / error_energy)\n",
    "        snr_results.append(current_snr)\n",
    "    \n",
    "    # 가장 큰 진폭을 가진 주기와 정규화된 진폭의 퍼센트 엑셀에 저장\n",
    "    if save_dominant_period_and_normalized_magnitude:\n",
    "        dominant_period_and_normalized_magnitude.to_excel(f\"../data/fourier/dominant_period_and_normalized_magnitude_low{low_cutoff_period}_high{high_cutoff_period}_snr{snr_percentile}.xlsx\")\n",
    "\n",
    "    # 시각화\n",
    "    if save_plot:\n",
    "        plt.figure(figsize=(15, 15))\n",
    "\n",
    "        # 원본 판매 데이터와 복원 판매 데이터 시각화\n",
    "        plt.subplot(211)\n",
    "        plt.plot(sales_data_filtered, label='Original Sales Data')\n",
    "        plt.plot(reconstructed_sales_data, label='Reconstructed Sales Data')\n",
    "        plt.xlabel(\"Time Steps\")\n",
    "        plt.ylabel(\"Sales\")\n",
    "\n",
    "        # 누적 SNR 시각화\n",
    "        plt.subplot(212)\n",
    "        plt.plot(range(1, len(snr_results) + 1), snr_results)\n",
    "        plt.xlabel('Number of Periods')\n",
    "        plt.ylabel('SNR (dB)')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"../data/fourier/sales_data_plot.png\")\n",
    "        plt.close()\n",
    "\n",
    "    # SNR 결과 정규화\n",
    "    max_snr, min_snr = max(snr_results), min(snr_results)\n",
    "    normalized_snr = [(x - min_snr) / (max_snr - min_snr) for x in snr_results]  # 최소-최대 정규화\n",
    "\n",
    "    # SNR의 퍼센트에 해당하는 인덱스 찾기\n",
    "    percentile_snr = np.percentile(normalized_snr, snr_percentile * 100)  # ?%에 해당하는 SNR 값\n",
    "    percentile_snr_index = (np.abs(np.array(normalized_snr) - percentile_snr)).argmin()  # 가장 가까운 값의 인덱스\n",
    "\n",
    "    # 선택된 SNR의 퍼센트에 해당되는 주기 개수만큼 가져오기\n",
    "    final_periods = periods[:percentile_snr_index + 1]\n",
    "\n",
    "    # final_periods 피클로 저장\n",
    "    if save_final_periods:\n",
    "        \n",
    "        final_periods_file_name = f\"final_periods_low{low_cutoff_period}_high{high_cutoff_period}_snr{snr_percentile}.pkl\"\n",
    "        result = {\n",
    "            \"final_periods\": final_periods\n",
    "        }\n",
    "        with open(os.path.join(save_path, final_periods_file_name), 'wb') as f:\n",
    "            pickle.dump(result, f)\n",
    "\n",
    "    return final_periods\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('CA', 'FOODS_1_001')\n"
     ]
    }
   ],
   "source": [
    "from scipy.signal import butter, filtfilt, freqz\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# 파라미터 설정\n",
    "test_size = 0.2\n",
    "\n",
    "n_epochs = 300\n",
    "initial_learning_rate = 5e-4\n",
    "lr_schedule = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "    initial_learning_rate,\n",
    "    first_decay_steps=1000,\n",
    "    t_mul=2.0,\n",
    "    m_mul=0.9,\n",
    "    alpha=1e-5\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_kernels = 20 # 상위 몇 개 커널 사용할지\n",
    "look_back_window_size = 1\n",
    "look_forward_window_size = 1\n",
    "\n",
    "num_filters=128\n",
    "\n",
    "dominant_period_and_normalized_magnitude = pd.DataFrame(columns=['dominant_period', 'normalized_magnitude'], index=first_sales_column_dict.keys())  # 가장 큰 진폭을 가진 주파수 정규화된 진폭의 퍼센트 저장\n",
    "\n",
    "for idx, (state_item_id, first_sales_day) in enumerate(first_sales_column_dict.items()): # 아이템별로\n",
    "    aligned_data = align_by_first_sale_day(state_item_id, first_sales_day)\n",
    "\n",
    "    ### 판매 데이터 \n",
    "    sales_data = aligned_data[\"sales\"]\n",
    "    \n",
    "    ## 푸리에 변환을 통해 주기 추출\n",
    "    sampling_frequency = 1 # 1일 단위 샘플링\n",
    "    nyquist_frequency = sampling_frequency/2 # 나이퀴스트 주파수\n",
    "    \n",
    "    # 밴드패스 필터 적용 (저주파/고주파 제거)\n",
    "    low_cutoff_frequency = 1/365 # 저주파/긴 주기(트렌드) 제거 # 주기 ?일 이상 제거\n",
    "    high_cutoff_frequency = 1/3 # 고주파/짧은 주기(노이즈) 제거 # 주기 ?일 이하 제거\n",
    "    normal_low_cutoff_frequency = low_cutoff_frequency / nyquist_frequency  # 정규화된 컷오프 주파수\n",
    "    normal_high_cutoff_frequency = high_cutoff_frequency / nyquist_frequency  # 정규화된 컷오프 주파수\n",
    "    b, a = butter(N=3, Wn=[normal_low_cutoff_frequency, normal_high_cutoff_frequency], btype='band', analog=False)\n",
    "    sales_data = filtfilt(b, a, sales_data)  # 필터 적용\n",
    "\n",
    "    # 푸리에 변환\n",
    "    fft = np.fft.fft(sales_data)\n",
    "    frequencies = np.fft.fftfreq(len(sales_data), d=1)[:len(sales_data)//2]  # 대칭 구조이기에 절반만 사용\n",
    "    magnitudes = np.abs(fft)[:len(sales_data)//2] # 대칭 구조이기에 절반만 사용\n",
    "    \n",
    "    # 나이퀴스트 조건 고려 (에일리어싱 방지) & 주파수 0 제거\n",
    "    valid_indices = (frequencies <= nyquist_frequency) & (frequencies > 0)\n",
    "    frequencies = frequencies[valid_indices]\n",
    "    magnitudes = magnitudes[valid_indices]\n",
    "\n",
    "    # 진폭순으로 정렬\n",
    "    sorted_indices = np.argsort(magnitudes)[::-1]\n",
    "    frequencies = frequencies[sorted_indices]\n",
    "    magnitudes = magnitudes[sorted_indices]\n",
    "\n",
    "    # 주기 계산\n",
    "    periods = [round(1/frequency) for frequency in frequencies]  # 주기 반올림\t\n",
    "\n",
    "    # 가장 큰 진폭을 가진 주기와 정규화된 진폭의 퍼센트를 데이터프레임에 저장\n",
    "    dominant_period_and_normalized_magnitude.loc[state_item_id] = [periods[0], (magnitudes[0]/magnitudes.sum())*100]\n",
    "\n",
    "    # 선택된 주기 개수별 SNR 계산\n",
    "    snr_results = [] # 주기 개수별 SNR 결과\n",
    "    for i in range(1, len(periods) + 1):\n",
    "        # 선택된 주기와 진폭\n",
    "        selected_periods = periods[:i]\n",
    "        selected_magnitudes = magnitudes[:i]\n",
    "\n",
    "        # 중복된 주기 제거\n",
    "        deduplicated_periods = []  # 중복 제거된 주기\n",
    "        deduplicated_magnitudes = []  # 중복 제거된 진폭\n",
    "        deduplicated_indices = []  # 선택된 주파수 인덱스\n",
    "        for j, selected_period in enumerate(selected_periods): # 선택된 주기 반복\n",
    "            if selected_period not in deduplicated_periods: # (이미 진폭순으로 내림차순이기 때문에) 선택된 주기가 중복되지 않을 때만 추가\n",
    "                deduplicated_periods.append(selected_period)\n",
    "                deduplicated_magnitudes.append(selected_magnitudes[j])\n",
    "                deduplicated_indices.append(sorted_indices[j])\n",
    "\n",
    "        # 인버스 푸리에 변환\n",
    "        inverse_fft = np.zeros_like(fft, dtype=complex)\n",
    "        for selected_index in deduplicated_indices:\n",
    "            inverse_fft[selected_index] = fft[selected_index] \n",
    "            inverse_fft[-selected_index] = fft[-selected_index]\n",
    "        reconstructed_sales_data = np.fft.ifft(inverse_fft).real\n",
    "\n",
    "        # SNR 계산\n",
    "        original_energy = np.sum(np.abs(sales_data) ** 2)\n",
    "        error_energy = np.sum(np.abs(sales_data - reconstructed_sales_data) ** 2)\n",
    "        current_snr = 10 * np.log10(original_energy / error_energy)\n",
    "        snr_results.append(current_snr)\n",
    "    \n",
    "    # 가장 큰 진폭을 가진 주기와 정규화된 진폭의 퍼센트 엑셀에 저장\n",
    "    dominant_period_and_normalized_magnitude.to_excel('../data/fourier/dominant_period_and_normalized_magnitude.xlsx')\n",
    "\n",
    "    # 원본 판매 데이터와 복원 판매 데이터 시각화\n",
    "    plt.figure(figsize=(15, 15))\n",
    "\n",
    "    plt.subplot(211)\n",
    "    plt.plot(sales_data, label='Original Sales Data')\n",
    "    plt.plot(reconstructed_sales_data, label='Reconstructed Sales Data')\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Sales\")\n",
    "\n",
    "    # 누적 SNR 시각화\n",
    "    plt.subplot(212)\n",
    "    plt.plot(range(1, len(snr_results) + 1), snr_results)\n",
    "    plt.xlabel('Number of Periods')\n",
    "    plt.ylabel('SNR (dB)')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show() # plt.savefig(f\"../data/fourier/plot/{state_item_id}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # SNR 결과 정규화\n",
    "    max_snr, min_snr = max(snr_results), min(snr_results)\n",
    "    normalized_snr = [(x-min_snr)/(max_snr-min_snr) for x in snr_results] # 최소-최대 정규화\n",
    "\n",
    "    # SNR의 퍼센트에 해당하는 인덱스 찾기\n",
    "    snr_percentile = 0.75  # ?% # 입력\n",
    "    percentile_snr = np.percentile(normalized_snr, snr_percentile*100) # ?%에 해당하는 SNR 값\n",
    "    percentile_snr_index = (np.abs(np.array(normalized_snr) - percentile_snr)).argmin()  # 가장 가까운 값의 인덱스\n",
    "\n",
    "    # 선택된 SNR의 퍼센트에 해당되는 주기 개수만큼 가져오기\n",
    "    final_periods = periods[:percentile_snr_index+1]\n",
    "\n",
    "    break\n",
    "\n",
    "\n",
    "    for item_idx, (input_data, output_data, kernel_sizes, kernel_strengths) in enumerate(item_windows_generator): # 윈도우별로 (배치별로)\n",
    "        print(f\"\\033[93mItem {item_idx}: {state_item_id}\\033[0m\")\n",
    "\n",
    "        print(f\"\\033[92mPreparing Data\\033[0m\")\n",
    "        split_index = int(len(input_data) * (1 - test_size))\n",
    "        train_data = [(input_data[i], output_data[i]) for i in range(split_index)]\n",
    "        test_data = [(input_data[i], output_data[i]) for i in range(split_index - look_back_window_size, len(input_data))]\n",
    "        \n",
    "        x_scaler = QuantileTransformer(output_distribution='normal')\n",
    "        y_scaler = QuantileTransformer(output_distribution='normal')\n",
    "\n",
    "        X_sales_train = np.array([x['sales'] for x, _ in train_data]).astype(np.float32)  # (train_size, look_back_window_size)\n",
    "        X_sales_train = x_scaler.fit_transform(X_sales_train)\t\n",
    "\n",
    "        \n",
    "        y_train = np.array([y for _, y in train_data]).astype(np.float32)  # (train_size, look_forward_window_size)\n",
    "        y_train = y_scaler.fit_transform(y_train.reshape(-1, 1)).reshape(-1, look_forward_window_size)\n",
    "\n",
    "        X_sales_test = np.array([x['sales'] for x, _ in test_data]).astype(np.float32)  # (test_size, look_back_window_size)\n",
    "        X_sales_test = x_scaler.transform(X_sales_test)\n",
    "\n",
    "\n",
    "        y_test = np.array([y for _, y in test_data]).astype(np.float32)  # (test_size, look_forward_window_size)\n",
    "        y_test = y_scaler.transform(y_test.reshape(-1, 1)).reshape(-1, look_forward_window_size)\n",
    "\n",
    "        print(f\"\\033[92mPreparing Loss and Optimizer\\033[0m\")\n",
    "        loss = tf.keras.losses.Huber(delta=1.0)\n",
    "        optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=lr_schedule,\n",
    "            clipnorm=0.5\n",
    "        )\n",
    "\n",
    "        print(f\"\\033[92mBuilding Model\\033[0m\")\n",
    "        model = MultiKernelPredictor(\n",
    "            kernel_sizes=kernel_sizes,\n",
    "        )\n",
    "\n",
    "        print(f\"\\033[92mTraining\\033[0m\")\n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "                                                            \"sales\": X_sales_train, \n",
    "                                                            }, y_train))\n",
    "        train_dataset = train_dataset.batch(batch_size)\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer=optimizer,\n",
    "            loss=custom_peak_loss\n",
    "        )\n",
    "        \n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='loss',\n",
    "            patience=20,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)\n",
    "                  \n",
    "        history = model.fit(\n",
    "            train_dataset,\n",
    "            epochs=n_epochs,\n",
    "    \t\tcallbacks=[early_stopping]\n",
    "        )\n",
    "\n",
    "        print(f\"\\033[92mTesting\\033[0m\")\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "                                                            \"sales\": X_sales_test, \n",
    "                                                            \"price\": X_price_test,\n",
    "                                                            \"weekend\": X_weekend_test, \n",
    "                                                            \"cultural\": X_cultural_test,\n",
    "                                                            \"national\": X_national_test, \n",
    "                                                            \"religious\": X_religious_test,\n",
    "                                                            \"sporting\": X_sporting_test, \n",
    "                                                            }, y_test))\n",
    "        test_dataset = test_dataset.batch(batch_size)\n",
    "        \n",
    "        test_loss = model.evaluate(test_dataset, verbose=1)\n",
    "        print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "        # 예측과 실제값 시각화 준비\n",
    "        predictions = model.predict(test_dataset)\n",
    "\n",
    "        # 스케일링 역변환\n",
    "        labels = y_scaler.inverse_transform(y_test.reshape(-1, 1)).reshape(-1)\n",
    "        predictions = y_scaler.inverse_transform(predictions.reshape(-1, 1)).reshape(-1)\n",
    "\n",
    "        # 그래프 그리기\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(labels, label='Labels', linewidth=2)\n",
    "        plt.plot(predictions, label='Predictions', linewidth=2, linestyle='--')\n",
    "        plt.xlabel(\"Time Steps\")\n",
    "        plt.ylabel(\"Values\")\n",
    "        plt.legend()\n",
    "\n",
    "        # 그림 저장\n",
    "        plt.savefig(f\"Item {item_idx}: {state_item_id}.png\")\n",
    "        plt.show()\n",
    "\n",
    "        break\n",
    "    break\n",
    "    #     # print(f\"\\033[92mAnalyzing\\033[0m\")\n",
    "    #     # # 모델 해석 단계\n",
    "    #     # analyzer = AnalyzeModel(model, X_sales_test, X_aux_test)\n",
    "    #     # analyzer.pdp(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 PDP\n",
    "    #     # analyzer.ice(feature_index=0)  # 예시: 첫 번째 보조 특성에 대한 ICE\n",
    "    #     # analyzer.feature_weights()  # 모델의 학습된 가중치 출력\n",
    "\n",
    "    # # # 모델 저장\n",
    "    # # model.save(f\"model_{state_item_id}.h5\")\n",
    "\n",
    "    # # # 모델 가중치 초기화\n",
    "    # # model.reset_states()\n",
    "\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

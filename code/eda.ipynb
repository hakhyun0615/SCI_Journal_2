{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Level 1\n",
      "\n",
      "Processing Level 2\n",
      "\n",
      "Processing Level 3\n",
      "\n",
      "Processing Level 4\n",
      "\n",
      "Processing Level 5\n",
      "\n",
      "Processing Level 6\n",
      "\n",
      "Processing Level 7\n",
      "\n",
      "Processing Level 8\n",
      "\n",
      "Processing Level 9\n",
      "\n",
      "Processing Level 10\n",
      "\n",
      "Processing Level 11\n"
     ]
    }
   ],
   "source": [
    "levels = [\n",
    "    [],                        # Level 1: Total\n",
    "    ['state_id'],              # Level 2: State\n",
    "    ['store_id'],              # Level 3: Store\n",
    "    ['cat_id'],                # Level 4: Category\n",
    "    ['dept_id'],               # Level 5: Department\n",
    "    ['state_id', 'cat_id'],    # Level 6: State-Category\n",
    "    ['state_id', 'dept_id'],   # Level 7: State-Department\n",
    "    ['store_id', 'cat_id'],    # Level 8: Store-Category\n",
    "    ['store_id', 'dept_id'],   # Level 9: Store-Department\n",
    "    ['item_id'],               # Level 10: Item\n",
    "    ['item_id', 'state_id'],   # Level 11: Item-State\n",
    "    ['item_id', 'store_id']    # Level 12: Individual\n",
    "]\n",
    "\n",
    "def fit_distributions(data, key, distributions = [\n",
    "    'weibull_min',\n",
    "    'loggamma',\n",
    "    'pareto',\n",
    "    'exponnorm',\n",
    "    'tukeylambda',\n",
    "    'genlogistic',\n",
    "    'pearson3',\n",
    "    'truncexpon',\n",
    "    'levy',\n",
    "    't',\n",
    "    'rice',\n",
    "    'halfcauchy',\n",
    "    'halfnorm',\n",
    "    'norm',\n",
    "    'gumbel_l',\n",
    "    'halflogistic',\n",
    "    'burr',\n",
    "    'lognorm',\n",
    "    'beta',\n",
    "    'nct',\n",
    "    'f',\n",
    "    'gumbel_r',\n",
    "    'laplace',\n",
    "    'johnsonsu',\n",
    "    'fatiguelife',\n",
    "    'truncnorm',\n",
    "    'exponweib',\n",
    "    'wald',\n",
    "    'chi2',\n",
    "    'levy_l',\n",
    "    'vonmises_line',\n",
    "    'invgauss',\n",
    "    'skewnorm',\n",
    "    'powernorm',\n",
    "    'powerlognorm',\n",
    "    'gamma',\n",
    "    'cauchy',\n",
    "    'genexpon',\n",
    "    'genhalflogistic',\n",
    "    'loglaplace',\n",
    "    'vonmises',\n",
    "    'genextreme',\n",
    "    'gengamma',\n",
    "    'gennorm',\n",
    "    'genpareto'\n",
    "]):\n",
    "    results = pd.DataFrame(columns=['level', 'group', 'distribution', 'parameters', 'p_value', 'significant', 'aic', 'bic'])\n",
    "\n",
    "    for dist_name in distributions:\n",
    "        dist = getattr(scipy.stats, dist_name)\n",
    "        params = dist.fit(data)\n",
    "        \n",
    "        fitted_dist = dist(*params)\n",
    "        _, ks_p_value = scipy.stats.kstest(data, fitted_dist.cdf)\n",
    "\n",
    "        loglik = np.sum(fitted_dist.logpdf(data))\n",
    "        n_params = len(params)\n",
    "        n_samples = len(data)\n",
    "        aic = 2 * n_params - 2 * loglik\n",
    "        bic = n_params * np.log(n_samples) - 2 * loglik\n",
    "        \n",
    "        result = {\n",
    "            'level': '_'.join(key.split('_')[:2]),\n",
    "            'group': '_'.join(key.split('_')[2:]) if len(key.split('_')) > 2 else 'Total',\n",
    "            'distribution': dist_name,\n",
    "            'parameters': str(params),\n",
    "            'p_value': ks_p_value,\n",
    "            'significant': ks_p_value > 0.01,\n",
    "            'aic': aic,\n",
    "            'bic': bic\n",
    "        }\n",
    "        results = results.append(result, ignore_index=True)\n",
    "\n",
    "    results = results.sort_values(['significant','aic','bic'], ascending=[False,True,True])\n",
    "    \n",
    "    return results\n",
    "\n",
    "# def plot_distribution_fit(data, dist_name, params, save_path, level=None, group=None):\n",
    "#     plt.figure(figsize=(15, 8))\n",
    "    \n",
    "#     plt.hist(data, bins='auto', density=True, alpha=0.7, color='skyblue', label='data')\n",
    "\n",
    "#     dist = getattr(scipy.stats, dist_name)\n",
    "#     x = np.linspace(min(data), max(data), 100)\n",
    "#     y = dist.pdf(x, *params)\n",
    "#     plt.plot(x, y, 'r-', label=f'{dist_name}')\n",
    "\n",
    "#     if level is not None and group is not None:\n",
    "#         title = f'Level {level} {group}'\n",
    "#     elif level is not None:\n",
    "#         title = f'Level {level}'\n",
    "        \n",
    "#     plt.title(title)\n",
    "#     plt.xlabel('Value')\n",
    "#     plt.ylabel('Density')\n",
    "#     plt.legend(loc='upper right')\n",
    "#     plt.savefig(save_path)\n",
    "#     plt.close()\n",
    "\n",
    "for level_idx, group_cols in enumerate(levels):\n",
    "    print(f\"\\nProcessing Level {level_idx+1}\")\n",
    "    df = pd.read_csv(f'../data/preprocessed/agg_df_level_{level_idx+1}.csv')\n",
    "    \n",
    "    if not group_cols: # Level 1\n",
    "        data = df['sales_sum']\n",
    "        dist_results = fit_distributions(data, f'level_{level_idx+1}')\n",
    "        \n",
    "        # if best_result:\n",
    "        #     all_level_results.append(best_result)\n",
    "        #     plot_distribution_fit(\n",
    "        #         data,\n",
    "        #         best_result['distribution'],\n",
    "        #         eval(best_result['parameters']),\n",
    "        #         f'../result/distribution_fitting/distribution_fit_level_{level_idx+1}.png',\n",
    "        #         level=level_idx+1\n",
    "        #     )\n",
    "\n",
    "        dist_results.to_excel(f'../result/distribution_fitting/distribution_fit_level_{level_idx+1}.xlsx', index=False)\n",
    "    \n",
    "    else: # Level 2~12\n",
    "        for group_name, group_data in df.groupby(group_cols):\n",
    "            group_key = '_'.join(map(str, group_name)) if isinstance(group_name, tuple) else str(group_name)\n",
    "            \n",
    "            dist_results = fit_distributions(group_data['sales_sum'], f'level_{level_idx+1}_{group_key}')\n",
    "            \n",
    "            # if best_result:\n",
    "            #     all_level_results.append(best_result)\n",
    "            #     plot_distribution_fit(\n",
    "            #         group_data['sales_sum'],\n",
    "            #         best_result['distribution'],\n",
    "            #         eval(best_result['parameters']),\n",
    "            #         f'../result/distribution_fitting/distribution_fit_level_{level_idx+1}_{group_key}.png',\n",
    "            #         level=level_idx+1,\n",
    "            #         group=group_key\n",
    "            #     )\n",
    "\n",
    "            dist_results.to_excel(f'../result/distribution_fitting/distribution_fit_level_{level_idx+1}_{group_key}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

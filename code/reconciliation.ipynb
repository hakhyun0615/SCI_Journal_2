{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from reconciliation_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = \"DeepAR\"\n",
    "\n",
    "ids = np.empty((0, ))\n",
    "\n",
    "labels = np.empty((0, 1941))\n",
    "forecasts = np.empty((0, 1941))\n",
    "\n",
    "for level in range(1, 13):\n",
    "    level_dir = f\"../result/level {level}\"\n",
    "    for estimator_root, estimator_dirs, estimator_files in os.walk(level_dir):\n",
    "        for estimator_dir in estimator_dirs:\n",
    "            if estimator_dir.startswith(estimator):\n",
    "                estimator_dir = os.path.join(estimator_root, estimator_dir)\n",
    "                for estimator_file in os.listdir(estimator_dir):\n",
    "                    if estimator_file.startswith('train_labels'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            arrays = np.array(datas)\n",
    "                            arrays = np.squeeze(arrays, axis=-1)\n",
    "                            arrays = arrays[:, -28:]\n",
    "                            train_labels = np.concatenate((train_labels, arrays), axis=0)\n",
    "                    if estimator_file.startswith('valid_labels'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            arrays = np.array(datas)\n",
    "                            arrays = np.squeeze(arrays, axis=-1)\n",
    "                            arrays = arrays[:, -28:]\n",
    "                            valid_labels = np.concatenate((valid_labels, arrays), axis=0)\n",
    "                    if estimator_file.startswith('test_labels'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            arrays = np.array(datas)\n",
    "                            arrays = np.squeeze(arrays, axis=-1)\n",
    "                            arrays = arrays[:, -28:]\n",
    "                            test_labels = np.concatenate((test_labels, arrays), axis=0)\n",
    "                    if estimator_file.startswith('train_forecasts'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            print(datas)\n",
    "                            ids = np.concatenate((ids, np.array([data.item_id for data in datas])), axis=0)\n",
    "                            arrays = np.array([data.quantile(0.5) for data in datas])\n",
    "                            print(arrays.shape)\n",
    "                            train_forecasts = np.concatenate((train_forecasts, arrays), axis=0)\n",
    "                    if estimator_file.startswith('valid_forecasts'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            arrays = np.array([data.quantile(0.5) for data in datas])\n",
    "                            valid_forecasts = np.concatenate((valid_forecasts, arrays), axis=0)\n",
    "                    if estimator_file.startswith('test_forecasts'):\n",
    "                        with open(os.path.join(estimator_dir, estimator_file), 'rb') as pickle_file:\n",
    "                            datas = pd.read_pickle(pickle_file)\n",
    "                            arrays = np.array([data.quantile(0.5) for data in datas])\n",
    "                            test_forecasts = np.concatenate((test_forecasts, arrays), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_mapping = {id: idx for idx, id in enumerate(ids)}\n",
    "node_info = {id: parse_id_components(id) for id in ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges, edge_types = create_edges(node_mapping)\n",
    "   \n",
    "# 4. 결과 저장\n",
    "# np.save('edges.npy', edges)\n",
    "# np.save('edge_types.npy', edge_types)\n",
    "   \n",
    "# with open('node_mapping.pkl', 'wb') as f:\n",
    "#     pickle.dump(node_mapping, f)\n",
    "   \n",
    "# 5. 통계 출력\n",
    "print(f\"Total nodes: {len(node_mapping)}\")\n",
    "print(f\"Total edges: {len(edges)}\")\n",
    "print(f\"Up edges: {np.sum(edge_types == 0)}\")\n",
    "print(f\"Down edges: {np.sum(edge_types == 1)}\")\n",
    "print(f\"Cross edges: {np.sum(edge_types == 2)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def numpy_to_tensor(forecasts, edges, edge_types, device=device):\n",
    "    if isinstance(forecasts, np.ndarray):\n",
    "        forecasts_tensor = torch.FloatTensor(forecasts).to(device)\n",
    "    elif isinstance(forecasts, torch.Tensor):\n",
    "        forecasts_tensor = forecasts.float().to(device)\n",
    "        \n",
    "    if isinstance(edges, np.ndarray):\n",
    "        edges_tensor = torch.LongTensor(edges).to(device)\n",
    "    elif isinstance(edges, torch.Tensor):\n",
    "        edges_tensor = edges.long().to(device)\n",
    "        \n",
    "    if isinstance(edge_types, np.ndarray):\n",
    "        edge_types_tensor = torch.LongTensor(edge_types).to(device)\n",
    "    elif isinstance(edge_types, torch.Tensor):\n",
    "        edge_types_tensor = edge_types.long().to(device)\n",
    "        \n",
    "    return forecasts_tensor, edges_tensor, edge_types_tensor\n",
    "\n",
    "def create_train_val_split(forecasts, targets, edges, edge_types, val_ratio=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    학습 및 검증 데이터셋 분할\n",
    "    \n",
    "    Args:\n",
    "        forecasts: 예측값 텐서\n",
    "        targets: 실제값 텐서 \n",
    "        edges: 엣지 텐서\n",
    "        edge_types: 엣지 타입 텐서\n",
    "        val_ratio: 검증 세트 비율\n",
    "        random_state: 랜덤 시드\n",
    "        \n",
    "    Returns:\n",
    "        train_forecasts, val_forecasts: 학습/검증용 예측값\n",
    "        train_targets, val_targets: 학습/검증용 실제값\n",
    "    \"\"\"\n",
    "    # 재현성을 위한 시드 설정\n",
    "    np.random.seed(random_state)\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "    # 노드 수\n",
    "    num_nodes = forecasts.shape[0]\n",
    "    \n",
    "    # 노드 인덱스 섞기\n",
    "    indices = np.random.permutation(num_nodes)\n",
    "    split_idx = int(num_nodes * (1 - val_ratio))\n",
    "    \n",
    "    train_indices = indices[:split_idx]\n",
    "    val_indices = indices[split_idx:]\n",
    "    \n",
    "    # 학습/검증 데이터 분할\n",
    "    train_forecasts = forecasts[train_indices]\n",
    "    val_forecasts = forecasts[val_indices]\n",
    "    \n",
    "    train_targets = targets[train_indices]\n",
    "    val_targets = targets[val_indices]\n",
    "    \n",
    "    return train_forecasts, val_forecasts, train_targets, val_targets, edges, edge_types\n",
    "\n",
    "# 사용 예시:\n",
    "\"\"\"\n",
    "# 데이터 준비\n",
    "forecasts_tensor, edges_tensor, edge_types_tensor = prepare_data_for_gnn(\n",
    "    forecasts=test_forecasts,\n",
    "    edges=edges,\n",
    "    edge_types=edge_types\n",
    ")\n",
    "\n",
    "# 학습/검증 분할\n",
    "train_forecasts, val_forecasts, train_targets, val_targets, edges, edge_types = create_train_val_split(\n",
    "    forecasts=forecasts_tensor,\n",
    "    targets=targets_tensor,\n",
    "    edges=edges_tensor,\n",
    "    edge_types=edge_types_tensor\n",
    ")\n",
    "\n",
    "# 모델 학습\n",
    "model = GATModel(input_dim=train_forecasts.shape[1])\n",
    "trainer = Trainer(model)\n",
    "history = trainer.train(\n",
    "    train_forecasts, train_targets,\n",
    "    val_forecasts, val_targets,\n",
    "    edges, edge_types\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import os\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "def prepare_time_based_data(forecasts, targets, time_splits=[1885, 1913, 1941]):\n",
    "    \"\"\"\n",
    "    시간 단위로 데이터 분할\n",
    "    \n",
    "    Args:\n",
    "        forecasts: 전체 예측값 배열 [num_nodes, 1941]\n",
    "        targets: 전체 실제값 배열 [num_nodes, 1941]\n",
    "        time_splits: 시간 분할 지점 (예: [1885, 1913, 1941])\n",
    "    \n",
    "    Returns:\n",
    "        train_forecasts: 학습용 예측값\n",
    "        train_targets: 학습용 실제값\n",
    "        val_forecasts: 검증용 예측값\n",
    "        val_targets: 검증용 실제값\n",
    "        test_forecasts: 테스트용 예측값 (최종 예측 타겟)\n",
    "    \"\"\"\n",
    "    # 시간 분할 지점\n",
    "    train_end = time_splits[0]\n",
    "    val_end = time_splits[1]\n",
    "    \n",
    "    # 학습 데이터: 1 ~ train_end 일\n",
    "    train_forecasts = forecasts[:, :train_end]\n",
    "    train_targets = targets[:, :train_end]\n",
    "    \n",
    "    # 검증 데이터: train_end+1 ~ val_end 일\n",
    "    val_forecasts = forecasts[:, train_end:val_end]\n",
    "    val_targets = targets[:, train_end:val_end]\n",
    "    \n",
    "    # 테스트 데이터: val_end+1 ~ 끝\n",
    "    test_forecasts = forecasts[:, val_end:]\n",
    "    \n",
    "    return train_forecasts, train_targets, val_forecasts, val_targets, test_forecasts\n",
    "\n",
    "def prepare_gnn_data(train_forecasts, train_targets, val_forecasts, val_targets, \n",
    "                     test_forecasts, edges, edge_types, device):\n",
    "    \"\"\"데이터를 PyTorch Tensor로 변환\"\"\"\n",
    "    # 학습/검증 데이터\n",
    "    train_forecasts_tensor = torch.FloatTensor(train_forecasts).to(device)\n",
    "    train_targets_tensor = torch.FloatTensor(train_targets).to(device)\n",
    "    \n",
    "    val_forecasts_tensor = torch.FloatTensor(val_forecasts).to(device)\n",
    "    val_targets_tensor = torch.FloatTensor(val_targets).to(device)\n",
    "    \n",
    "    # 테스트 데이터\n",
    "    test_forecasts_tensor = torch.FloatTensor(test_forecasts).to(device)\n",
    "    \n",
    "    # 그래프 구조\n",
    "    edges_tensor = torch.LongTensor(edges).to(device)\n",
    "    edge_types_tensor = torch.LongTensor(edge_types).to(device)\n",
    "    \n",
    "    return (train_forecasts_tensor, train_targets_tensor, \n",
    "            val_forecasts_tensor, val_targets_tensor, \n",
    "            test_forecasts_tensor, edges_tensor, edge_types_tensor)\n",
    "\n",
    "class GATModel(torch.nn.Module):\n",
    "    # 기존에 정의한 GATModel 클래스 코드\n",
    "    pass  # 실제 코드에서는 GATModel 전체 정의\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        learning_rate=1e-3,\n",
    "        weight_decay=1e-5,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.optimizer = Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', patience=5, factor=0.5)\n",
    "        \n",
    "    def train_epoch(self, forecasts, edges, edge_types, targets):\n",
    "        self.model.train()\n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = self.model(forecasts, edges, edge_types)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = torch.nn.functional.mse_loss(predictions, targets)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        return loss.item()\n",
    "    \n",
    "    def validate(self, forecasts, edges, edge_types, targets):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = self.model(forecasts, edges, edge_types)\n",
    "            val_loss = torch.nn.functional.mse_loss(predictions, targets).item()\n",
    "            \n",
    "            # 추가 메트릭\n",
    "            mae = mean_absolute_error(targets.cpu().numpy(), predictions.cpu().numpy())\n",
    "            rmse = np.sqrt(mean_squared_error(targets.cpu().numpy(), predictions.cpu().numpy()))\n",
    "            \n",
    "        return val_loss, mae, rmse\n",
    "    \n",
    "    def train(\n",
    "        self,\n",
    "        train_forecasts,\n",
    "        train_targets,\n",
    "        val_forecasts,\n",
    "        val_targets,\n",
    "        edges,\n",
    "        edge_types,\n",
    "        epochs=100,\n",
    "        patience=10\n",
    "    ):\n",
    "        best_val_loss = float('inf')\n",
    "        patience_counter = 0\n",
    "        history = {'train_loss': [], 'val_loss': [], 'val_mae': [], 'val_rmse': []}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # 학습\n",
    "            train_loss = self.train_epoch(train_forecasts, edges, edge_types, train_targets)\n",
    "            \n",
    "            # 검증\n",
    "            val_loss, val_mae, val_rmse = self.validate(val_forecasts, edges, edge_types, val_targets)\n",
    "            \n",
    "            # 학습률 조정\n",
    "            self.scheduler.step(val_loss)\n",
    "            \n",
    "            # 메트릭 기록\n",
    "            history['train_loss'].append(train_loss)\n",
    "            history['val_loss'].append(val_loss)\n",
    "            history['val_mae'].append(val_mae)\n",
    "            history['val_rmse'].append(val_rmse)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                patience_counter = 0\n",
    "                # 최고 모델 저장\n",
    "                torch.save(self.model.state_dict(), 'best_gnn_model.pt')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                if patience_counter >= patience:\n",
    "                    print(f\"Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_loss={val_loss:.4f}, \"\n",
    "                      f\"val_mae={val_mae:.4f}, val_rmse={val_rmse:.4f}\")\n",
    "        \n",
    "        return history\n",
    "\n",
    "def run_gnn_experiment(time_splits=[1885, 1913, 1941]):\n",
    "    \"\"\"GNN 실험 실행\"\"\"\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # 1. 데이터 로드\n",
    "    try:\n",
    "        forecasts = np.load('all_forecasts.npy')  # 모든 노드의 전체 기간 예측값\n",
    "        targets = np.load('all_targets.npy')     # 모든 노드의 전체 기간 실제값\n",
    "        \n",
    "        # 그래프 구조 로드\n",
    "        edges = np.load('edges.npy')\n",
    "        edge_types = np.load('edge_types.npy')\n",
    "        \n",
    "        print(f\"Loaded data - Forecasts: {forecasts.shape}, Targets: {targets.shape}\")\n",
    "        print(f\"Graph structure - Edges: {edges.shape}, Edge types: {edge_types.shape}\")\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(\"Required data files not found!\")\n",
    "        return\n",
    "    \n",
    "    # 2. 시간 기반 데이터 분할\n",
    "    print(f\"Splitting data based on time points: {time_splits}\")\n",
    "    train_forecasts, train_targets, val_forecasts, val_targets, test_forecasts = prepare_time_based_data(\n",
    "        forecasts, targets, time_splits\n",
    "    )\n",
    "    \n",
    "    print(f\"Split shapes - Train: {train_forecasts.shape}, Val: {val_forecasts.shape}, Test: {test_forecasts.shape}\")\n",
    "    \n",
    "    # 3. PyTorch Tensor 변환\n",
    "    train_forecasts_tensor, train_targets_tensor, \\\n",
    "    val_forecasts_tensor, val_targets_tensor, \\\n",
    "    test_forecasts_tensor, edges_tensor, edge_types_tensor = prepare_gnn_data(\n",
    "        train_forecasts, train_targets, val_forecasts, val_targets, \n",
    "        test_forecasts, edges, edge_types, device\n",
    "    )\n",
    "    \n",
    "    # 4. GNN 모델 초기화\n",
    "    # 각 시계열의 길이를 입력 차원으로 설정\n",
    "    input_dim = train_forecasts.shape[1]  # 학습 기간의 길이\n",
    "    print(f\"Initializing GNN model with input_dim={input_dim}\")\n",
    "    \n",
    "    model = GATModel(input_dim=input_dim, hidden_dim=256, num_layers=3)\n",
    "    trainer = Trainer(model, device=device)\n",
    "    \n",
    "    # 5. 모델 학습\n",
    "    print(\"Training GNN model...\")\n",
    "    history = trainer.train(\n",
    "        train_forecasts_tensor, train_targets_tensor,\n",
    "        val_forecasts_tensor, val_targets_tensor,\n",
    "        edges_tensor, edge_types_tensor,\n",
    "        epochs=100,\n",
    "        patience=10\n",
    "    )\n",
    "    \n",
    "    # 6. 검증 데이터에서의 성능 평가\n",
    "    print(\"Evaluating on validation data...\")\n",
    "    model.load_state_dict(torch.load('best_gnn_model.pt'))\n",
    "    _, val_mae, val_rmse = trainer.validate(\n",
    "        val_forecasts_tensor, edges_tensor, edge_types_tensor, val_targets_tensor\n",
    "    )\n",
    "    \n",
    "    print(f\"Validation performance - MAE: {val_mae:.4f}, RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # 7. 테스트 데이터(1914~1941일)에 적용하여 예측값 보정\n",
    "    print(\"Applying GNN to adjust test forecasts...\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        adjusted_forecasts = model(test_forecasts_tensor, edges_tensor, edge_types_tensor)\n",
    "    \n",
    "    # 8. 결과 저장\n",
    "    adjusted_forecasts_np = adjusted_forecasts.cpu().numpy()\n",
    "    np.save('adjusted_test_forecasts.npy', adjusted_forecasts_np)\n",
    "    \n",
    "    # 9. 학습 히스토리 저장\n",
    "    with open('train_history.pkl', 'wb') as f:\n",
    "        pickle.dump(history, f)\n",
    "    \n",
    "    print(\"Experiment completed successfully!\")\n",
    "    return history, adjusted_forecasts_np\n",
    "\n",
    "def analyze_results(original_forecasts, adjusted_forecasts, targets=None, num_samples=5):\n",
    "    \"\"\"결과 분석 및 시각화\"\"\"\n",
    "    # 랜덤 샘플 선택\n",
    "    np.random.seed(42)\n",
    "    sample_indices = np.random.choice(original_forecasts.shape[0], num_samples, replace=False)\n",
    "    \n",
    "    # 원본 예측값과 조정된 예측값 비교\n",
    "    plt.figure(figsize=(15, num_samples*4))\n",
    "    \n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(num_samples, 1, i+1)\n",
    "        plt.plot(original_forecasts[idx], 'b-', label='Original Forecast')\n",
    "        plt.plot(adjusted_forecasts[idx], 'r-', label='GNN Adjusted')\n",
    "        \n",
    "        if targets is not None:\n",
    "            plt.plot(targets[idx], 'g--', label='Actual')\n",
    "            \n",
    "            # 오차 계산\n",
    "            orig_rmse = np.sqrt(mean_squared_error(targets[idx], original_forecasts[idx]))\n",
    "            adj_rmse = np.sqrt(mean_squared_error(targets[idx], adjusted_forecasts[idx]))\n",
    "            improvement = (orig_rmse - adj_rmse) / orig_rmse * 100\n",
    "            \n",
    "            plt.title(f'Node {idx} - Improvement: {improvement:.2f}% (RMSE: {orig_rmse:.4f} → {adj_rmse:.4f})')\n",
    "        else:\n",
    "            plt.title(f'Node {idx} - Original vs GNN Adjusted')\n",
    "            \n",
    "        plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('forecast_comparison.png', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "    # 전체 성능 개선 분석 (타겟 데이터가 있는 경우)\n",
    "    if targets is not None:\n",
    "        orig_rmse = np.sqrt(mean_squared_error(targets.flatten(), original_forecasts.flatten()))\n",
    "        adj_rmse = np.sqrt(mean_squared_error(targets.flatten(), adjusted_forecasts.flatten()))\n",
    "        improvement = (orig_rmse - adj_rmse) / orig_rmse * 100\n",
    "        \n",
    "        print(f\"Overall performance:\")\n",
    "        print(f\"Original RMSE: {orig_rmse:.4f}\")\n",
    "        print(f\"Adjusted RMSE: {adj_rmse:.4f}\")\n",
    "        print(f\"Improvement: {improvement:.2f}%\")\n",
    "        \n",
    "        return orig_rmse, adj_rmse, improvement\n",
    "\n",
    "# 메인 실행 함수\n",
    "def main():\n",
    "    # 시간 분할 지점 설정\n",
    "    time_splits = [1885, 1913, 1941]  # 학습 종료, 검증 종료, 테스트 종료\n",
    "    \n",
    "    # GNN 실험 실행\n",
    "    history, adjusted_forecasts = run_gnn_experiment(time_splits)\n",
    "    \n",
    "    # 테스트 결과 로드\n",
    "    test_forecasts = np.load('all_forecasts.npy')[:, time_splits[1]:]  # 원본 예측값\n",
    "    test_targets = np.load('all_targets.npy')[:, time_splits[1]:]      # 실제값\n",
    "    \n",
    "    # 결과 분석\n",
    "    analyze_results(test_forecasts, adjusted_forecasts, test_targets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "levels = [1, 3, 10, 3, 7, 9, 21, 30, 70, 3049, 9147, 30490]\n",
    "\n",
    "start_idx = 0\n",
    "for i, level in enumerate(levels, start=1):\n",
    "    end_idx = start_idx + level\n",
    "    \n",
    "    test_id_level = test_ids[start_idx:end_idx]\n",
    "    df = pd.DataFrame(test_id_level, columns=['test_id'])\n",
    "    df.to_csv(f'test_id_level_{i}.csv', index=False)\n",
    "    \n",
    "    start_idx = end_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wrmsse(y_true, y_pred):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv('../data/original/sell_prices.csv')\n",
    "\n",
    "    sales = sales.iloc[:, 6:].values\n",
    "\n",
    "    sell_prices['id'] = sell_prices['store_id'] + '_' + sell_prices['item_id']\n",
    "    sell_prices = sell_prices[sell_prices['wm_yr_wk'] <= 11613]\n",
    "    sell_prices = sell_prices.pivot(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "    sell_prices = sell_prices.values\n",
    "\n",
    "    N, h = y_true.shape \n",
    "    w = sell_prices.shape[1]  \n",
    "\n",
    "    daily_prices = np.repeat(sell_prices, repeats=7, axis=1)[:, -sales.shape[1]:]\n",
    "    daily_prices = np.where(np.isnan(daily_prices), np.nan, daily_prices)\n",
    "    \n",
    "    squared_errors = np.mean((y_true - y_pred) ** 2, axis=1)\n",
    "    scale = np.mean(np.diff(sales, axis=1) ** 2, axis=1)\n",
    "    rmsse = np.sqrt(squared_errors / (scale + 1e-10))\n",
    "\n",
    "    total_revenue = np.nansum(sales[:, -28:] * daily_prices[:, -28:], axis=1) \n",
    "    weight = total_revenue / np.nansum(total_revenue) \n",
    "\n",
    "    wrmsse = np.nansum(weight * rmsse)\n",
    "    \n",
    "    return wrmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_wrmsse(test_labels, test_forecasts) # 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from scipy.linalg import pinv\n",
    "\n",
    "def create_S(y_id):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sales['id'] = sales['id'].str.replace('_validation', '') # 30490\n",
    "\n",
    "    states = sales['state_id'].unique()  \n",
    "    stores = sales['store_id'].unique()\n",
    "    cats = sales['cat_id'].unique()\n",
    "    depts = sales['dept_id'].unique()\n",
    "    states_cats = [f\"{state}_{cat}\" for state in states for cat in cats]\n",
    "    states_depts = [f\"{state}_{dept}\" for state in states for dept in depts]\n",
    "    stores_cats = [f\"{store}_{cat}\" for store in stores for cat in cats]\n",
    "    stores_depts = [f\"{store}_{dept}\" for store in stores for dept in depts]\n",
    "    items = sales['item_id'].unique()\n",
    "    items_states = [f\"{item}_{state}\" for item in items for state in states]\n",
    "    items_stores = [f\"{item}_{store}\" for item in items for store in stores]\n",
    "\n",
    "    S = np.zeros((42840, 30490))\n",
    "\n",
    "    for i, id in tqdm(enumerate(y_id), total=len(y_id)):\n",
    "        if id == 'total':\n",
    "            S[0, :] = 1            \n",
    "        elif id in states:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['state_id'] == id]['id']).astype(int).values\n",
    "        elif id in stores:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['store_id'] == id]['id']).astype(int).values\n",
    "        elif id in cats:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['cat_id'] == id]['id']).astype(int).values\n",
    "        elif id in depts:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['dept_id'] == id]['id']).astype(int).values\n",
    "        elif id in states_cats:\n",
    "            state, cat = id.split('_')\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['state_id'] == state) & (sales['cat_id'] == cat)]['id']).astype(int).values\n",
    "        elif id in states_depts:\n",
    "            splitted_id = id.split('_')\n",
    "            state, dept = splitted_id[0], '_'.join(splitted_id[1:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['state_id'] == state) & (sales['dept_id'] == dept)]['id']).astype(int).values\n",
    "        elif id in stores_cats:\n",
    "            splitted_id = id.split('_')\n",
    "            store, cat = '_'.join(splitted_id[:2]), splitted_id[2]\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['store_id'] == store) & (sales['cat_id'] == cat)]['id']).astype(int).values\n",
    "        elif id in stores_depts:\n",
    "            splitted_id = id.split('_')\n",
    "            store, dept = '_'.join(splitted_id[:2]), '_'.join(splitted_id[2:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['store_id'] == store) & (sales['dept_id'] == dept)]['id']).astype(int).values\n",
    "        elif id in items:\n",
    "            S[i, :] = sales['id'].isin(sales[sales['item_id'] == id]['id']).astype(int).values\n",
    "        elif id in items_states:\n",
    "            splitted_id = id.split('_')\n",
    "            item, state = '_'.join(splitted_id[:3]), '_'.join(splitted_id[3:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['item_id'] == item) & (sales['state_id'] == state)]['id']).astype(int).values\n",
    "        elif id in items_stores:\n",
    "            splitted_id = id.split('_')\n",
    "            item, store = '_'.join(splitted_id[:3]), '_'.join(splitted_id[3:])\n",
    "            S[i, :] = sales['id'].isin(sales[(sales['item_id'] == item) & (sales['store_id'] == store)]['id']).astype(int).values\n",
    "        else:\n",
    "            print(f\"Error: {id} not found\")\n",
    "\n",
    "    return S\n",
    "\n",
    "def compute_W(y_actual, y_pred):\n",
    "    E = y_actual - y_pred\n",
    "    W = (1 / (E.shape[1] - 1)) * (E @ E.T)\n",
    "    return W\n",
    "\n",
    "S = create_S(test_ids)\n",
    "W = compute_W(test_labels, test_forecasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import inv\n",
    "\n",
    "def convert_to_appropriate_dtype(matrix):\n",
    "    c_min = matrix.min()\n",
    "    c_max = matrix.max()\n",
    "    \n",
    "    if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "        return matrix.astype(np.float16)\n",
    "    elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "        return matrix.astype(np.float32)\n",
    "    else:\n",
    "        return matrix.astype(np.float64)\n",
    "\n",
    "S = convert_to_appropriate_dtype(S)\n",
    "W = convert_to_appropriate_dtype(W)\n",
    "test_forecasts = convert_to_appropriate_dtype(test_forecasts)\n",
    "\n",
    "W_inv = inv(W)\n",
    "ST_Winv = S.T @ W_inv\n",
    "ST_Winv_S = ST_Winv @ S\n",
    "ST_Winv_S_inv = inv(ST_Winv_S)\n",
    "S_ST_Winv_S_inv = S @ ST_Winv_S_inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def chunked_dot_memmap(A, B, chunk_size, n_jobs=-1, temp_dir='C:/temp'):\n",
    "    def process_chunk_memmap(start, end, A, B, result_memmap):\n",
    "        result_memmap[start:end] = A[start:end] @ B\n",
    "\n",
    "    n_chunks = (A.shape[0] + chunk_size - 1) // chunk_size\n",
    "    result_shape = (A.shape[0], B.shape[1])\n",
    "    \n",
    "    # Ensure the temporary directory exists\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Use an absolute path for the memory-mapped file\n",
    "    result_memmap_path = os.path.join(temp_dir, 'result_memmap.dat')\n",
    "    result_memmap = np.memmap(result_memmap_path, dtype=A.dtype, mode='w+', shape=result_shape)\n",
    "\n",
    "    Parallel(n_jobs=n_jobs)(\n",
    "        delayed(process_chunk_memmap)(i * chunk_size, min((i + 1) * chunk_size, A.shape[0]), A, B, result_memmap)\n",
    "        for i in range(n_chunks)\n",
    "    )\n",
    "\n",
    "    return result_memmap\n",
    "\n",
    "# Example usage\n",
    "# Assuming S_ST_Winv_S_inv and ST_Winv are already defined\n",
    "chunk_size = 1000  # Adjust chunk size based on available memory\n",
    "P = chunked_dot_memmap(S_ST_Winv_S_inv, ST_Winv, chunk_size=chunk_size)\n",
    "\n",
    "# Use the result\n",
    "print(\"Shape of P:\", P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_forecasts_reconciled = P @ test_forecasts\n",
    "\n",
    "print(\"Before reconciliation:\", test_forecasts[:10])\n",
    "print(\"After reconciliation:\", test_forecasts_reconciled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mint_reconciliation(S, W, y_pred):\n",
    "    W_inv = inv(W)\n",
    "    P = S @ inv(S.T @ W_inv @ S) @ S @ W_inv\n",
    "    return P @ y_pred\n",
    "\n",
    "test_forecasts_reconciled = mint_reconciliation(S, W, test_forecasts)\n",
    "\n",
    "print(\"Before reconcilation:\", test_forecasts[:10])\n",
    "print(\"After reconcilation:\", test_forecasts_reconciled[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_wrmsse(y_true, y_pred):\n",
    "    sales = pd.read_csv('../data/original/sales_train_validation.csv')\n",
    "    sell_prices = pd.read_csv('../data/original/sell_prices.csv')\n",
    "\n",
    "    sales = sales.iloc[:, 6:].values\n",
    "\n",
    "    sell_prices['id'] = sell_prices['store_id'] + '_' + sell_prices['item_id']\n",
    "    sell_prices = sell_prices[sell_prices['wm_yr_wk'] <= 11613]\n",
    "    sell_prices = sell_prices.pivot(index='id', columns='wm_yr_wk', values='sell_price')\n",
    "    sell_prices = sell_prices.values\n",
    "\n",
    "    N, h = y_true.shape \n",
    "    w = sell_prices.shape[1]  \n",
    "\n",
    "    daily_prices = np.repeat(sell_prices, repeats=7, axis=1)[:, -sales.shape[1]:]\n",
    "    daily_prices = np.where(np.isnan(daily_prices), np.nan, daily_prices)\n",
    "    \n",
    "    squared_errors = np.mean((y_true - y_pred) ** 2, axis=1)\n",
    "    scale = np.mean(np.diff(sales, axis=1) ** 2, axis=1)\n",
    "    rmsse = np.sqrt(squared_errors / (scale + 1e-10))\n",
    "\n",
    "    total_revenue = np.nansum(sales[:, -28:] * daily_prices[:, -28:], axis=1) \n",
    "    weight = total_revenue / np.nansum(total_revenue) \n",
    "\n",
    "    wrmsse = np.nansum(weight * rmsse)\n",
    "    \n",
    "    return wrmsse\n",
    "\n",
    "calculate_wrmsse(test_labels, test_forecasts) # 12"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
